---
title: 'Benford''s Law with RStudio'
author: Gabriel Voelcker
date: '2018-08-10'
slug: benford-s-law-using-r
categories: []
tags:
  - r
  - benford
  - distribution
  - accounting
  - fraud
Description: ''
Tags: []
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

```{r packages, echo=FALSE, message=FALSE, include=FALSE}
library(GetDFPData)
library(tidyverse)
library(benford.analysis)
library(BenfordTests)
library(dplyr)
library(dygraphs)
library(readxl)
library(blogdown)
library(writexl)
library(rstudioapi)
library(magrittr)
library(ggplot2)
library(ggthemes)
library(scales)
library(dslabs)
library(lubridate)
require(zoo)
library(BatchGetSymbols)
require(xfun)
xfun::pkg_load2(c('base64enc', 'htmltools', 'mime', 'readxl'))

library(knitr)
library(kableExtra)

```
In 2016 I had the opportunity to take a class in my masters entitled Econophysics. Econophysics is a field of study that applies methods and theories developed by physicists to economic problems. Around that time I came across a very interesting phenomenon called Benford's Law. This post is an introduction to Benford's Law using RStudio. Let's start by understanding what Benford's Law is.

I am thinking of a number from 1 to 99. You have 2 seconds to guess which is the first **digit** of that number. 

1.

2.

Which number did you choose? Actually, that's irrelevant! My intention here is to make you reflect on this question: what were the odds of you correctly guessing the first digit? 

1/9, right? There were 9 possibilities(1,2,3,4,5,6,7,8,9), since 0 is not a first digit among the possibilities I've described. So, if you chose 4, in 11(4,40,41,42,43,44,45,46,47,48,49) of 99 possible outcomes you would've pick correctly. So far, simple math. 

But what if we use another set of numbers? For example: the first 1000 occurrences of the Fibonacci sequence(Any number of the Fibonacci sequence is the sum of the previous two numbers, so it works like this: 1,1,2,3,5,8,13... and so forth). Would you say that your odds of guessing correctly would remain 1/9 were you to choose 4 again? Let me illustrate the distribution of the first digit of this sequence:

```{r Digit, echo=FALSE, message=FALSE, include=FALSE}
Base <- read_excel("Fibonacci.xlsx", sheet=1, col_names = TRUE)
```

```{r Beta, tidy=FALSE, echo=FALSE, message=FALSE, include=TRUE, warning=FALSE}
Graf1 <- ggplot(data = Base, mapping = aes(Digit, as.numeric(Distribution))) +
  geom_col() +
  theme_economist(base_size = 12) +
    scale_y_continuous(name="Distribution", limits=c(0, 0.4), breaks=c(0,0.1,0.2,0.3,0.4), labels = percent) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(name = "First Digit", breaks=c(Base$Digit)) + 
  ggtitle("1st Digit Distribution")
Graf1
```

Clearly, we have a different distribution! And the reason is because what we have here is a very peculiar mathematical phenomenon called Benford's Law. Thanks to two gentlemen, Simon Newcomb and Frank Benford, there's a good story behind this distribution. In [**1881**](http://gerry.lamost.org/upload/newcomb.pdf), Simon Newcomb, an astronomer, noticed that the first pages of a logarithm book were usually more worned out than the last ones. From this, he derived the intuition that, given a list of arbitrary numbers, more numbers will tend to begin with an "1" than any other digit. So, why is it called Benford's Law, and not Newcomb's law? (in case you're wondering, that's a classic case of [**Stigler's Law**](https://io9.gizmodo.com/5820736/stiglers-law-why-nothing-in-science-is-ever-named-after-its-actual-discoverer)[this law is named after this [**Stigler**](https://galton.uchicago.edu/faculty/stigler.shtml), not his [**father**](https://research.chicagobooth.edu/stigler/about/george-stigler), the economist and Nobel Laureate]).

Well, it wasn't until [**1938**](https://www.jstor.org/stable/984802?seq=1#page_scan_tab_contents) that physicist(remember when I talked about Econophysics?) Frank Benford formalized this discovery, which he regarded as the Law of Anomalous Numbers. He tested the distribution of the first digit of 20 different samples, and concluded that in populations of exponential growth, the distribution of the first digit followed a certain pattern. What is this pattern? If **d** is the first digit, with d(d E{1,2,...,9}), then: 



```{r}
xfun::embed_file("Fibonacci.xlsx")
```

$$P(d) = \log_{10}(d+1) - \log_{10}(d) = \log_{10}(\frac{d+1}{d}) = \log_{10}(\frac{d+1}{d})$$


Substituting d for the nine digits, we have the following results:

```{r tables-mtcars, tidy=FALSE, echo=FALSE, message=FALSE, include=TRUE, warning=FALSE}
Digits <- c(1,2,3,4,5,6,7,8,9)
Benford <- c("30.1%", "17.6%", "12.5%", "9.7%", "7.9%", "6.7%", "5.8%", "5.1%","4.6%")
Benford <- as.data.frame(cbind(Digits, Benford))
colnames(Benford) <- c("1st Digit", "Distribution")
knitr::kable(Benford[], caption = 'Benford Distribution') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F))
```

If we plot this distribution, we get:

```{r, Benforddist, tidy=FALSE, echo=FALSE, message=FALSE, include=TRUE, warning=FALSE}
Base <- read_excel("Base.xlsx", sheet=1, col_names = TRUE)
Benford <- as.data.frame(cbind(Base$Digit, as.numeric(Base$Benford)))
ggplot(data = Base, mapping = aes(Digit, Benford)) +
  geom_col() +
  theme_economist()+
   scale_y_continuous(name="Distribution", limits=c(0, 0.35), breaks=c(0,0.05,0.1,0.15,0.2,0.25,0.3,0.35), labels = percent) +
  scale_x_continuous(breaks=c(Base$Digit)) + 
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("Benford Distribution")

  
```

(Very similar to the Fibonacci 1st digit distribution, right?)

Give yourself a moment to let that sink in. If you love math as much as I do, there's a lot of reflection to be done here. There you have it: the Law of Anomalous Numbers. In a variety of data sets, the frequency distribution of the first digit obeys this law, conforming to the distribution plotted above. But, as one of my academics advisors used to always ask me, what is the purpose of this information? 

The beauty of Benford's Law is how this distribution is applied in all sorts of data sets. In 1972, [**Hal Varian**](http://people.ischool.berkeley.edu/~hal/) suggested that the Benford distribution could be used to detect accounting frauds. Some [**w**](https://search.proquest.com/openview/7d019c616546c4c5da54cba2c85d136d/1?pq-origsite=gscholar&cbl=31718)[**o**](https://www.db-thueringen.de/servlets/MCRFileNodeServlet/dbt_derivate_00028759/ilm1-2013200266.pdf)[**r**](http://jhr.uwpress.org/content/44/1/1.short)[**k**](https://amstat.tandfonline.com/doi/abs/10.1198/000313007X223496)[**s**](https://www.ams.org/journals/proc/1995-123-03/S0002-9939-1995-1233974-8/) applied Benford's distribution to investigate a variety of datasets. Statistically speaking, when data should conform to Benford's Law, a null-hypothesis rejection suggests that some form of data manipulation has taken place. In other words, if a data set should conform to Benford's Law and it doesn't, something is wrong. Let me introduce you to two RStudio packages that can test that:

1)  [**BenfordTests**](https://cran.r-project.org/web/packages/BenfordTests/BenfordTests.pdf);  
2) [**benford.analysis**](https://cran.r-project.org/web/packages/benford.analysis/benford.analysis.pdf). 

Let's go through to the basics together. 

Let's use the first 1000 terms of the Fibonacci sequence(feel free to download them in xlsx format [**here**](https://github.com/gvoelcker/Site-Gabriel/blob/master/content/post/Fibonacci.xlsx)) with the BenfordTests package. It provides a series of tests to see if your database conforms to Benford's Law. 

For example, let's perform a Kolmogorov-Smirnov test. Quoting the package manual: "ks.benftest takes any numerical vector reduces the sample to the specified number of significant digits and performs the Kolmogorov-Smirnov goodness-of-fit test to assert if the data conforms to Benfordâ€™s law.". Here's the code:

```{r}
library(readxl)
library(BenfordTests)
Fibonacci <- read_excel("Fibonacci.xlsx", sheet=2, col_names = TRUE)
Test1 <- as.numeric(Fibonacci$FirstDigit)
ks.benftest(Test1, digits = 1, pvalmethod = "simulate")
```

The most important part is how to interpret them. If H0 is rejected, then we have indications that the distribution doesn't conform to Benford's Law. Since the result we got was D = 0.068029, we can say that with a level of significance of 93.2% that the set of numbers conforms to Benford's Law. I'll leave it up to you if that's rigorous enough for your analysis.

The BenfordTests package has other tests of conformity using other methodologies, feel free to test them by slightly adjusting the code and using other tests, such as chisq.benftest or edist.benftest.

But the packages contain more than just tests. It is possible to perform a graphical analysis of the first significant digits using signifd.analysis: 

```{r}
signifd.analysis(Test1)
```

Suppose our data set should conform to Benford's Law but it doesn't. Wouldn't you agree that zooming in in each number is a good way of trying to find out where the non-conformity is? There are lots of ways you can go deep in a Benford's Law analysis, but I believe the basics have been introduced here. 
If you are craving for more, Benford's Law is also applicable, and useful, for the second digit or a combination of the first two digits. Here we have it:

```{r tables-1stand2nddigit, tidy=FALSE, echo=FALSE, message=FALSE, include=TRUE, warning=FALSE}
Second_Digit <- c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9,"1st Digit Distribution")
First <- c(4.14, 3.78, 3.48, 3.22, 3.00, 2.80, 2.63, 2.48, 2.35, 2.23, 30.10)
Second <- c(2.12, 2.02, 1.93, 1.85, 1.77, 1.70, 1.64, 1.58, 1.52, 1.47, 17.61)
Third <- c(1.42, 1.38, 1.34, 1.30, 1.26, 1.22, 1.19, 1.16, 1.13, 1.10, 12.49)
Fourth <- c(1.07, 1.05, 1.02, 1.00, .98, .95, .93, .91, .90, .88, 9.69)
Fifth <- c(.86, .84, .83, .81, .80, .78, .77, .76, .74, .73, 7.92)
Sixth <- c(.72, .71, .69, .68, .67, .66, .65, .64, .63, .62, 6.69)
Seventh <- c(.62, .61, .60, .59, .58, .58, .57, .56, .55, .55, 5.80)
Eighth <- c(.54, .53, .53, .52, .51, .51, .50, .50, .49, .49, 5.12)
Ninth <- c(.48, .47, .47, .46, .46, .45, .45, .45, .44, .44, 4.58)
SecondDigit <- c(11.97, 11.39, 10.88, 10.43, 10.03, 9.67, 9.34, 9.04, 8.76, 8.50, "Total")
Table <- as.data.frame(cbind(Second_Digit, First, Second, Third, Fourth, Fifth, Sixth, Seventh, Eighth, Ninth, SecondDigit))
colnames(Table) <- c("2nd Digit \ 1st Digit", "1", "2", "3", "4", "5", "6", "7", "8", "9", "2nd Digit Distribution")
knitr::kable(Table[], caption = '1st and 2nd Digit Distribution') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", full_width = F)) 
```

If you want to learn even more about Benford's Law, I advise you to test all the features in both packages, read their documentation and the papers that originated every test. I hope this introduction has been useful.  

Bonus: If you are a fan of math, there's still a lot you can learn about Benford's law [**here**](http://mathworld.wolfram.com/BenfordsLaw.html).
