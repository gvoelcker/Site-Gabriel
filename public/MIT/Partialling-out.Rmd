---
title: "Empirical discussion: Gender Wage Gap"
author: "Gabriel Voelcker"
date: "3/19/2020"
output: html_document
---

## Partialling Out

The idea of this document is to replicate a certain type of regression, explaining each step. This is based on the first lecture note of 14.382, more specifically on question 1.

An example of how Partialling-out works is illustrated by the following analysis of the Gender Wage Gap using the 2015 US March Supplement of the Current Population Survey data. 

The objetive is to assess the impact of gender on wage. The sample contains 32,523 workers, obtained after applying the following filters: a) no self-employed workers; b) individual was 25-64 years old; c) individual worked at least 35 hours a week; d) individual worked at least 50 weeks a year; e) individual is white non-hispanic; f) individual is not living in group quarters; g) individual was not in the military/agricultural/private household sectors; h) no inconsistent earnings reports; i) no inconsistent report of employment status; j) no allocated/missing data; and k) no hourly wage below $3. Descriptive Statistics are presented in Table (1), for the full sample and for
men only and women only subsamples. 

To assess the Gender Wage Gap, the regression of interest is the following:

\[ Y = \beta_0 + D'\beta_1 + W'\beta_2 + \epsilon \text{, } E\epsilon (D'W') = 0 \]

Where Y represents the log of the hourly wage rate, D is a dummy variable that equals 1 when the individual is a woman and W controls for characteristics that could influence the hourly wage(for ex: marital status and industry, etc). To perform the partialling out procedure, we partial W out of Y and D and compare the regressions with and without partialling-out the controls. It is not in the interest of the analysis to deal with RCT or causation, but simply that the models obey the aforementioned theoretical discussion of the partialling-out procedure. For illustrative purposes, we also run a regression with no controls. 

Onto the empirics of it. Required packages:

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Loading packages:
library(foreign)
library(xtable)
library(stargazer)
library(sandwich)
```

Setting up the working directory and downloading the data:

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Setting up working directory -  update with yours
setwd("C:/Users/gmv/Dropbox/MIT/2020 1/14.382 - Econometrics/PS2")
load("CPS2015.RData")
data <- CPS2015
```

Now we have to tidy the data for the regressions. We will create education, region and marital status factors:

```{r}
# create education factor:
data$educ <- factor(data$lhs+2*data$hsg+3*data$sc+4*data$cg+5*data$ad)

# create marital status factor:
data$ms <- factor(data$married+2*data$widowed+3*data$divorced+4*data$nevermarried)

# create region:
data$reg <- factor(data$ne+2*data$mw+3*data$so+4*data$we)

# Let's attach the data we're going to work with from now on:
attach(data)
```

Now we want to generate the first table, which has the descriptive statistics. 

```{r}
# Table 1: Descriptive Statistics:
  # Variables of interest:
vars <- c("lnw","female","married","widowed","separated","divorced","nevermarried","lhs","hsg","sc","cg","ad","ne","mw","so","we",
          "exp1") 
  
  # Let's get the means:
dstats <- cbind(sapply(data[,vars], mean), # gets mean for all sample
                apply(data[female==0,vars], 2, mean), # gets mean for men
                apply(data[female==1,vars], 2, mean)) # gets mean for women

colnames(dstats) <- c("All", "Male", "Female") # names columns of descriptive statistics table
```

If you wish to export this to LaTeX code, please:

```{r}
# Generates Table1 LaTeX code, export to Overleaf
xtable(dstats, type = "latex", file = "Table1.tex", digits = 2) 
```

Now, onto the regressions. The first one is a simple one, simply regressing wages on the dummy indicator for gender. I'll store the results for now and present them later. This first regression will be simply referred to as **reg**. 

```{r}
  # 1) reg: Y = intercept + female coefficient(D) + errors
reg <- lnw ~ female # this is the basic model
```

Let's store the coefficients, the covariance matrix and the standard errors.

```{r}
# Storing useful info!
reg.coeff<- lm(reg) # a linear model obtains the coefficients 
reg.HCV <- vcovHC(reg.coeff, type = 'HC') # obtains covariance matrix
reg.se <- sqrt(diag(reg.HCV))  # obtains standard errors
```

That's it for the basic regression. Let's proceed to a more complete regression. We'll now include the controls, and call it **regw**. As we did before for **reg**, we'll store the coefficients, covariance matrix and standard errors for **regw**.

```{r}
  # 2) reg with W: Y = intercept + female coefficient(D) + controls(W) + errors
regw <- lnw ~ female
regw.coeff<- lm(regw) # obtains the coefficients 
regw.HCV <- vcovHC(regw.coeff, type = 'HC') # obtains covariance matrix
regw.se <- sqrt(diag(regw.HCV))  # obtains standard errors for regw
```

For the third regression, we'll now proceed to the **partialling out** part. This means we're gonna remove the linear effect of all the controls from *lnw* and *female*. 

```{r}
# 3) reg partial out W: Y = intercept + female coefficient(D) + P-out: controls(W) + errors
pout.Y <- lnw ~ (exp1+exp2+exp3+exp4)*(educ+occ2+ind2+ms+reg) + (educ+occ2+ind2+ms+reg)^2 
resY <- lm(pout.Y)$res
```




