---
output: html_document
---

# 14.381 - Applied Econometrics{.tabset}

## **Questions and Concepts** {.tabset}

### Questions

- How to obtain log likelihood for probit and logit? (PS1 ex 2a)

- What is the **exclusion restriction**?

- What are the **endogenous variables**?

- And the **exogenous variables**?

- WNLS for probit/logit?

- Inverse Milles Ratio?

## **Useful Stuff** {.tabset}

### **Basic Info**

#### **Documents with basic information about how the course works**

[**2019 Syllabus**](https://gabrielvoelcker.netlify.com/mit/syl381fa19.pdf)

[**2019 Agenda**](https://gabrielvoelcker.netlify.com/mit/agenda381fa19+(new).pdf)

[**2019 Schedule**](https://gabrielvoelcker.netlify.com/mit/sched381fa19+(new).pdf)

### **Class Materials**

#### **Class Materials**

[**1) Mastering Mostly Harmless Metrics**](https://gabrielvoelcker.netlify.com/mit/Intro381fall2019flat.pdf)

[**2) Mastering Regression**](https://gabrielvoelcker.netlify.com/mit/Intro381fall2019flat.pdf)

[**3) Labor Economists Know The Score**](https://gabrielvoelcker.netlify.com/mit/Pscore381fall2019handout.pdf)


### **PSets**

#### **The PSets for the semester are:**

[**Qualifying PS**](https://gabrielvoelcker.netlify.com/mit/Review+problem+set.pdf)

[**PS 1**](https://gabrielvoelcker.netlify.com/mit/psetI_381fa19.pdf)

## **Mostly Harmless Econometrics** {.tabset}

### **1) Questions About Questions**

4 Frequently Asked Questions about the relationship of interest, the ideal experiment, the identification strategy and the mode of inference.

  *i) What is the causal relationship of interest?* Useful for making predictions about the consequences of changing circumstances/policies. 
  
  *ii) what experiment that could ideally be used to capture the causal effect of interest?* What should the treatment be?
  
Fundamentally unindentified questions: FUQs. Questions that cannot be answererd by any experiment. 

  *iii) What is your identification strategy?* Identification is the way the researcher uses his data. 
  
  *iv) What is the mode of statistical inference?* it describes tge population, the sample and the assumptions made when constructing standard errors. How straightforward is your inference?

### **2) The Experimental Ideal**

- Importance of Random assignment: eliminate selection bias. Has the randomization successfully balanced the characteristics across groups?

- Regression: constant treatment effects: the impact of the treatment is the same for everyone. Regressions are important, let's review some facts and properties about them:

### **3) Making Regression Make Sense**

#### 3.1) Regression Fundamentals

- Without randomness there is no causality.

- Conditional Expectation Function: 

- Law of iterated expectations: 

##### 3.1.2) Linear Regression and The CEF

The link between the CEF and regression functions can be explained in three ways. To start, let the K x 1 regression coefficient vector $\beta$ be defined by solving

\[ \beta = arg min_{(b)} E[(Y_i - X_i'b)^2] \]

Using FOC:

\[ E[X_i (Y_i - X_i'b)] = 0 \]

Where the solution can be written as $\beta$ = E$[X_iX_i']^{-1}E[X_iY_i]$. The population residual, which we define as $Y_i - X_i'\beta = e_i$ is uncorrelated with the regressors $X_i$. For the multivariate case, the regression anatomy is:

\[ \beta_k = \frac{Cov(Y_i, \tilde{x}_{ki})}{V(\tilde{x}_{ki})} \]

Where $\tilde{x}_{ki}$ is the residual from a regression of $x_{ki}$ on all the other covariates. Into the math: E[$X_iX_i'$]$^{-1}$E[$X_iY_i$] is the $K \times 1$ vector with *k*th element $\frac{Cov(Y_i,\tilde{x}_{ki})}{V(\tilde{x}_{ki})}$. This formula describes the anatomy of a multivariate regression coefficient and it reveals much more than the matrix formula $\beta$ = $E[X_iX_i']^{-1}E[X_iY_i]$. It is an important formula because it shows that **each coefficient in a multivariate regression is the bivariate slope coefficient for the corresponding regressor after partialing out all the other covariates**. To verify this, substitute: 

\[ Y_i = \alpha + \beta_1x_{1i} + ... + \beta_kx_{ki} + ... + \beta_k x_{ki} + e_i\]

into the numerator of the Regression Anatomy Formula. Since $\tilde{x}_{ki}$ is uncorrelated with $e_i$, it is a **linear combination of the regressors**. Also, since $\tilde{x}_{ki}$ is a residual from a regression on all other covariates in the model, it must be uncorrelated with these covariates. For the same reason, the Cov($\tilde{x}_{ki}$,$x_{ki}$) is just the V($\tilde{x}_{ki}$). Hence: Cov($Y_i, \tilde{x}_{ki}$) = $\beta_k V(\tilde{x}_{ki})$

- **Linear CEF Theorem (3.1.4.)(Regression Justification 1)**: suppose the CEF is linear $\implies$ the population regression function is it. So what makes the CEF linear? Classic scenario: joint normality $\implies$ vector ($Y_i, X_i'$) has a multivariate normal distribution(few empirical validity given the regressors and variables are often discrete). 
**Saturated regression model**: has a separate parameter for every possible combination of values that the set of regressors can take on. 

- **Best Linear Predictor Theorem(3.1.5.)(Regression Justification 2)**: the function $X_i'\beta$ is the best linear predictor of $Y_i$ given $X_i$ in a MMSE sense. $E[Y_i|X_i]$ is the best (MMSE) predictor of $Y_i$ given $X_i$ in the class of *all* functions of $X_i$, the population regression function is the best we can do in the class of *linear* functions. 

-  **The Regression CEF Theorem(3.1.6)(Regression Justification 3)**: the function $X_i'\beta$ provides the MMSE linear approximation to E[$Y_i|X_i$], that is: 

\[ \beta = argmin_{b} E\{(E[Y_i|X_i] - X_i'b)^2\} \]


#### 3.2) Regression and Causality

##### 3.2.1) The Conditional Independence Assumption (CIA)

A regression is causal when the Conditional Expectation Function it approximates is causal. The CEF is causal when it describes differences in average potential outcomes for a fixed reference population. As discussed in chapter 2: **experiments ensure that the causal variable of interest is independent of potential outcomes so that the groups being compared are truly comparable**. We want to generalize this to causal variables that take on more than two values and to more complicated situations where we must hold a variety of control variables fixed for causal inferences to be valid. This leads to the **Conditional Independence Assumption**: the assumption that provides justification for the causal interpretation of regression estimates. Also called **Selection on Observables** because the *covariates are held fixed since they are assumed to be known and observed*. The question is: *what these control variables are?* We call them *covariates* $X_i$ (in the schooling case, $X_i$ is a vector of abilities and family background). 

Suppose we want to know the difference in earnings ($Y_{1i} - Y_{0i}$) if Angrist goes to college($C_i = 1$) versus not going to college ($C_i = 0$). The observed outcome, $Y_{i}$ can be written in terms of potential outcomes as:

\[ Y_i = Y_{0i} + (Y_{1i} - Y_{0i})C_i   \]

We get to see one of $Y_{1i}$ or $Y_{0i}$, but never both. We therefore hope to measure the average of $Y_{1i} - Y_{0i}$, or the average for some group, such as those who went to college. This is E[$Y_{1i} - Y_{0i}|C_i = 1$]. But in general, comparisons of those who attend college vs those who don't are a poor measure of the causal effect of college attendance. We have to take more into consideration, given what we talked about on chapter 2:

\[  \underbrace{E[Y_i|C_i = 1] - E[Y_i|C_i = 0]}_\text{Observed difference in earnings} = \underbrace{E[Y_{1i} - Y_{0i}|C_i = 1]}_\text{ATE on the treated} + \underbrace{E[Y_{0i}|C_i = 1] - E[Y_{0i}|C_i = 0]}_\text{Selection Bias}  \]

It seems likely that those who go to college would have earned more anyway, right? So, the Selection Bias is positive and the naive comparison, $E[Y_i|C_i = 1] - E[Y_i | C_i = 0]$ exaggerates the benefits of college attendance(given the value of the Selection Bias, the observed difference in outcome may be misinterpreted).

The Conditional Independence Assumption asserts that **conditional on observed characteristics**, $X_i$, the Selection Bias disappears. Formally:

\[\{Y_{0i},Y_{1i} \} \perp \perp C_i|X_i  \]

Where the symbol $\perp \perp$ denotes the independence relation; the random variables to the right of $|$ are the **conditioning set**. Given the **Conditional Independence Assumption (CIA)**, conditional on $X_i$, comparisons of average earnings across schooling levels have a causal interpretation. In other words:

\[  E[Y_i|X_i,C_i = 1]  - E[Y_i|X_i,C_i = 0] = E[Y_{1i} - Y_{0i}|X_i]\]

This was established for two possible outcomes: $C_i = 1$ and $C_i = 0$(going to college vs not). Let's expand that to variables that can take on **more than two values**. 

The causal relationship between schooling and earnings is likely to be different for each person, so let's use the individual-specific notation: $Y_{si} \equiv f_i(s)$. This denotes the potential earnings of a person after receiving $s$ years of education. $f_i(s)$ answers causal "what if" questions. In the context of theoretical models of the relationship between human capital and earnings, the form of $f_i(s)$ may be determined by aspects of individual behavior, by market forces, or both. The CIA in this general setup becomes:

\[ Y_{si} \perp \perp s_i|X_i\text{, }\forall s \]

In an observational study, the CIA means that $s_i$ can be said to be "as good as randomly assigned", conditional on $X_i$. Conditional on $X_i$, the **average causal effect** of a one-year increase in schooling is E[$f_i(s) - f_i(s-1)|X_i$], while the ACE of a four-year increase in schooling is E[$f_i(s) - f_i(s-4)|X_i$]. The data reveal only $Y_i$ = $f_i(s_i)$, that is, $f_i(s)$ for $s = s_i$. But given the CIA, conditional on $X_i$ comparisons of average earnings across schooling levels have a causal interpretation:

\[ E[Y_i | X_i,s_i = s] - E[Y_i | X_i,s_i = s - 1] =  E[f_i(s) - f_i(s-1)|X_i], \forall s\]

If we were to compare the difference between 12 and 11 years, we would learn about the average causal effect of high school graduation. The selection bias comes from differences in the potential dropout earnings of high school graduates and non-graduates. However, given the CIA, high school graduation is independent of potential earnings conditional on $X_i$, so the selection bias vanishes. 

So far, we've constructed separate causal effects for each value taken on by the conditioning variables. This leads to as many causal effects as there are values of $X_i$, given that every observation could lead to a unique combination of characteristics. So, Empiricists almost always find it useful to boil a set of estimates down to a single summary measure, for ex: the **unconditional** or **overall average causal effect**. By the law of iterated expectations, the unconditional average causal effect of high school graduation is: 

\[  E\{E[Y_i|X_i,s_i = 12] - E[Y_i|X_i, s_i = 11] \}  \]

\[  = E\{E[f_i(12)-f_i(11)|X_i] \}\]

\[ = E[f_i(12) - f_i(11)] \]

Just as well, how much have high school graduates gained by virtue of having graduated? The average causal effect of high school graduation on high school graduates is:

\[ E\{E[Y_i|X_i,s_i = 12] - E[Y_i|X_i,s_i = 11]|s_i = 12\}   \]

\[ = E \{ E[f_i(12) - f_i (11)|X_i]|s_i = 12\}   \]

\[ E[f_i(12) - f_i(11)| s_i = 12]  \]

The **unconditional average effect** can be computed by averaging all the X-specific effects weighted by the marginal distribution of $X_i$. So the empirical **counterpart** is a **matching estimator**: we make comparisons across different schooling groups for individuals with the same covariate values, compute the difference in their average earnings and then average these difference in some way. 

**Regression** provides an easy-to-use empirical strategy that automatically turns the CIA into causal effects. Two routes can be traced from the CIA to regression: **1)** $f_i(s)$ is both linear in $s$ and the same for everyone except for an additive error term, so we can use a linear regression to estimate the features of $f_i(s)$; **2)** recognizing that $f_i(s)$ almost certainly differs for different people and does not need to be linear in $s$. Even so, allowing for random variation in $f_i(s)$ across people and for nonlinearity for a given person, regression can be thought of as a strategy for the estimation of a weighted average of the individual-specific difference $f_i(s) - f_i(s-1)$. 

Now we want to focus on the **conditions required for the regression to have a causal interpretation**. Let's consider a linear constant effects causal model:

\[ f_i(s) = \alpha + \rho s + \eta_i   \]

This equation is linear and says that the functional relationship of interest is the same for everyone. If we were to compute the realized value of s: $s_i$, we would have:

\[ Y_i = \alpha + \rho s_i + \eta_i \]

The penultimate equation explicitly associates the coefficients above with a causal relationship. Since this penultimate equation is a causal model, $s_i$ may be correlated with the potential outcomes $f_i(s)$, or, in this case, the residual term above, $\eta_i$. 

We can decompose $\eta_i$ into a linear function of observable characteristics $X_i$ and an error term $v_i$: $\eta_i = X_i'\gamma + v_i$, where $\gamma$ is a vector of population regression coefficients that is assumed to satisfy E[$\eta_i | X_i$] = $X_i' \gamma$. By virtue of the CIA, we have:

\[ E[f_i(s)|X_i,s_i] = E[f_i(s)|X_i] = \alpha + \rho s + E[\eta_i | X]   \]

\[ = \alpha + \rho s + X_i' \gamma  \]

The residual in the linear causal model: $Y_i = \alpha + \rho s_i + X_i' \gamma + v_i$ is therefore uncorrelated with the regressors $s_i$ and $X_i$, and the regression coefficient $rho$ is **the causal effect of interest**. 


##### 3.2.2) The Omitted Variables Bias Formula

In addition to the variable of interest, we now have introduced a set of control variables, $X_i$, into our regression. The **Omitted Variable Bias(OVB) Formula describes the relationship between regression estimates in models with different sets of control variables**. This is motivated by the notion that *a longer regression has a causal interpretation* while a shorter one does not. This means that the coefficients on the variables included in the shoter regression are **biased**. The difference between coefficients in a short and in a long regression are determined by the OVB formula. 

Imagine we are regressing wages on school years: $Y_i = \alpha + \rho s_i + A_i' \gamma + e_i$. What are the consequences of leaving ability ($A_i$) out of the regression? The resulting short regression is related to the long regression as follows:

Ommited Variables Bias Formula:

\[ \frac{Cov(Y_i,S_i)}{V(S_i)} = \rho + \gamma' \delta_{As} \]

Where $\delta_{As}$ is the vector of coefficients from regressions of the elements of $A_i$ on $s_i$. The OVB formula states: *short equals long plus the effect of omitted times the regression of omitted on included*. Both the OVB formula *and* the regression anatomy formula tell us that short and long regression coefficients are the same whenever the omitted and included variables are uncorrelated. 

We use the OVB formula to get a sense of what would happen if we omitted ability for schooling coefficients. Ability variables have positive effects on wages, and these variables are also likely to be positively correlated with schooling $\implies$ the short regression may be "too big" relative to what we want. But some omitted variables may be negatively correlated with schooling, in which case the short regression coefficient may be too small. 

##### 3.2.3) Bad Controls

Not always more control is better. **Bad Controls** are variables that themselves are outcome variables in the notional experiment at hand.  

#### 3.3) Heterogeneity and Nonlinearity

The assumption of a linear **Conditional Expectation Function(CEF)** is not really necessary for a causal interpretation of regression. Section 3.1.2. showed how the regression of $Y_i$ on $X_i$ and $s_i$ can be thought as as providing the best linear approximation to the underlying CEF, regardless of its shape. If the CEF is causal, the fact that a regression approximates it gives regression coefficients a causal flavor. To explore the link between regression and CEF we dive into the understanding of regression as a computationally attractive **matching estimator**.

##### 3.3.1) Regression Meets Matching

**Matching** is a useful strategy to control for *covariates*, motivated by the Conditional Independence Assumption. Military ex: conditional on the individual characteristics the military uses to select soldiers, veteran status is **independent** of potential earnings. The **appeal** of matching is: matching amounts to covariate-specific treatment-control comparisons, weighed together to produce a **single overall average treatment effect**.

An attractive feature of matching strategies is that they are typically accompanied by an explicit statement of the conditional independence assumption required to give matching estimates a causal interpretation. 

**Matching requires two-steps: matching and then averaging**

That is: CIA assumes that once you have certain characteristics sorted out, a certain outcome does not depend on them.

**REREAD PG 74 FORWARD MHE**

##### 3.3.2) Control for Covariates using Propensity Score

What happens if the covariates $X_i$ are many/multi/continuous? Matching now requires grouping or parametric assumptions, a fact that induces bias. The **Propensity Score Theorem** extends the regression OVB idea to nonparametric conditioning.

**Theorem**: if potential outcomes are independent of treatment status conditional on a multivariate covariate vector $X_i$, then potential outcomes are independent of treatment status conditional on a scalar function of covariates $\rightarrow$ **the propensity score**. It is defined as $p(X_i) = E[D_i|X_i] = P[D_i = 1|X_i]$. Like the OVB formula for regression, the propensity score theorem says that you only need to control for covariates that affect the probability of treatment itself. Also: the only covariate you really need to control for is the probability treatment itself. What it is done: **1)** $p(X_i)$ is estimated using some kind of parametric model (logit/probit); **2)** estimates of the effect of treatment are computed either by matching on the estimated score from this first step or using a weighting scheme. 

- **PSM definition from** [**Dehejia and Wahba(1999)**](https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10473858): 
The propen-sity score is defined as the probability of assignment to treatment, conditional on covariates.

- From Angrist's class *Labor Economists Know The Score*: - Propensity Score Theorem? Suppose the CIA holds for $Y_{ij}$ (j=0,1) $\implies Y_{ji} \perp \perp D_i|p(X_i)$.

**Proof for the Propensity-Score Theorem**: the claim is true if $P[D_i = 1| Y_{ji}, p(X_i)]$ does not depend on $Y_{ji}$:

<center>
```{r, echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/R/Site-Gabriel/public/MIT")
img <- readPNG("34.png")
grid.raster(img)
```
</center>

We use the score to match, similarly as Covariate matching, except that we match on the score-as-covariate. By the propensity score theorem and the CIA, $E[Y_{1i} - Y_{0i}|D_i = 1]$ is equal to:

\[ E\{E[Y_i|p(X_i),D_i = 1] - E[Y_i|p(X_i),D_i = 0]|D_i=1 \} \]

- Back to MHE pg 81: 

Example of weighting scheme: **we match on score** instead of the covariates directly. By the PST and the CIA:

\[ E[Y_{1i} - Y_{0i}|D_i = 1]  \]

\[ E\{E[Y_i|\rho (X_i), D_i = 1] - E[Y_i|\rho(X_i),D_i = 0]|D_i = 1\} \]

Estimates of the effect of treatment on the treated can therefore be obtained by: **1)**  stratifying on an estimate of p($X_i$); and **2)** substituting conditional sample averages for expectations or **2)** by matching each treated observation to controls with similar values of the propensity score(used by the paper we review in class: [**Dehejia and Wahba(1999)**](https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10473858). 

Another approach one could apply a model-based or nonparametric estimate of E$[Y_i|\rho(X_i),D_i]$ can be substituted for these conditional mean functions and the outer expectation replaced wih a sum(this was done [**here**](https://watermark.silverchair.com/65-2-261.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAnAwggJsBgkqhkiG9w0BBwagggJdMIICWQIBADCCAlIGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMdjtaBMX_bvz4Op8tAgEQgIICI2z4QA0M_9j0_jXxcnqGUgvt6nl8eYAt1Yupnrf7ET_YcJCbCDV_3WJeLP0I35F-Z-2ZIcJupppgb9FTd7V65b4YT5eWkuQ1SQTBt1DrCyz1qUbW_-DaqgvACuHP2w991HULxqzE8YTLqNp_WVLL2HfmlZJdHewnmP3DYusW_zrhv8i0VepUaTdgYsERy8P6atL422JcMC4XewBtMut-Tu-9uB-Kb4FLZIjt0YwqCmkEZKAB-r2sJZd6T5BzMoPU9quKbcsplhsC3tJHoeo_jGjN9yJA1FZD9PtiIk0ZLcwAdlFKsx4Yak0f4S_7NWKnwIUzqG6jhAA26GNIMNFakSZJhZfT_W7EE-MzpiwMfTOEmQLmhgwKJntdSukNBkQDWNCl3567r-RjtWs7WBUGJFJClTaoQibY-h6VgfolMCQCrhtNvthVZOVHhc3V9o3O-n01pczD3X5Zhs6Bgl7YB_u7eZ49y4LtPca56YUZZ03T_gdabLlYlX2QLhGhL-tXhb_6ffs_TRtgs-xEFnK29D8sAXcvFcxc244BgT7atdeQqNns-NlcxCr6hT9MM3cW4SV6wSZX09gZ_dhMHBX2-Xn0MbBlYZZbNdnJFuLb9OfeSkGssi7RIpvSdacIJWfrVJO-HQHY2cXz1PAEoXs6LPncPukY185gPr5jADYw3klGuhW5Ej7XoPncbu0g2oiThm0PrF40zdK6yxJ49itP8k3QWmA)).

We can calculate the effect of treatment on the treated from the sample analog of:

\[ E[Y_{1i} - Y_{0i}|D_i = 1] = E[\frac{(D_i - \rho(X_i))Y_i}{(1 - \rho(X_i))P(D_i = 1)}]  \]

With a consistent estimator of $\rho(X_i)$ it is possible to **correct for nonrandom sampling** via weighting by the reciprocal of the probability of selection. The first idea for this came from [**Horvitz-Thompson**](http://www.stat.cmu.edu/~brian/905-2008/papers/Horvitz-Thompson-1952-jasa.pdf): in their version of the propensity score approach the estimator is essentially automated, no need for matching. This approach highlights the link between PSM and regression. Consider again(as previously seen in 3.3.1) the regression estimand, $\delta_R$, for the population regression of $Y_i$ on $D_i$, controlling for a saturated model for covariates. This estimand can be written:

\[ \delta_R = \frac{E[(D_i - \rho(X_i))Y_i]}{E[\rho(X_i)(1 - \rho(X_i))]} \]

The two estimators obtained from the last two equations are in the class of weighted average estimands considered by [**Hirano, Imbens and Ridder(2003)**](https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00442):

\[ E \{g(X_i) [\frac{Y_iD_i}{\rho(X_i)} - \frac{Y_i(1 - D_i)}{(1 - \rho(X_i))}] \}  \]

Above, $g(X_i)$ is a known weighting function. **So you calculte the effect of the treatment on the treated with an easier less cumbersome approach using weights**. 

Thought-provoking theorems on the efficient use of the propensity score:

i) From the point of view of asymptotic efficiency, **there is usually a cost to matching on the propensity score instead of full covariate matching**. If we investigate the maximal precision of estimates of treatment effects under the CIA, with and without knowledge of the propensity score. A regression analog for this point is the result that even in a scenario with no OVB, the long regression generates more precise estimates of the coefficients on the variables included in a short regression whenever the omitted variables have some predictive power for outcomes. So **why should we use PSM?** It helps the scientist to focus on models for treatment assignment(something he is well-informed of) but not on the typically more complex and mysterious process that determines outcomes. 

ii) Even though there is no **asymptotic efficiency gain** for using the propensity score, there will often be a gain in precision in **finite samples**. Tradeoff: if the covariates omitted from the propensity score explain little of the variation in outcomes, it may be better to ignore them than to bear the statistical burden imposed by the need to estimate their effects. 

iii) [**Hirano, Imbens and Ridder(2003)**](https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00442): show that for models with continuous covariates, a Horvitz-Thompson-type weighting estimator is efficient when the weighting scheme uses a *nonparametric* estimate of the score(even though estimates of treatment effects based on a known propensity score are inefficient). 

##### 3.3.3) Propensity Score Methods vs Regression

Propensity score methods shift attention from the estimation of $E[Y_i | X_i, D_i]$ to the estimation of the propensity score: $\rho(X_i) \equiv E[D_i |X_i]$. 

Conclusion(Pg91): the experiments made in this subschapter boost the authors faith in regression. Regression controls for the right covariates does a reasonably good job of eliminating selection bias in the CPS-1 sample despite a huge baseline gap. Restricting the sample using our knowledge of program admissions yields even better results with CPS-3. Systematic **prescreening** to enforce common support seems like a useful adjunct to regression estimation with CPS-1, a large and coarsely selected initial sample. 











### **4) Instrumental Variables in Action**

From [**Wikipedia**](https://en.wikipedia.org/wiki/Instrumental_variables_estimation): the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment. IVs are used when an explanatory variable of interest is correlated with the error term, in which case ordinary least squares and ANOVA give biased results. A valid instrument **induces changes in the explanatory variable but has no independent effect on the dependent variable**, allowing a researcher to uncover the causal effect of the explanatory variable on the dependent variable.

Instrumental variable methods allow for consistent estimation when the explanatory variables (covariates) are correlated with the error terms in a regression model. Such correlation may occur **1)** when changes in the dependent variable change the value of at least one of the covariates ("reverse" causation), **2)** when there are omitted variables that affect both the dependent and independent variables, or **3)** when the covariates are subject to non-random measurement error. Explanatory variables which suffer from one or more of these issues in the context of a regression are sometimes referred to as endogenous. In this situation, ordinary least squares produces biased and inconsistent estimates.
