---
output: html_document
---

# 14.381 - Applied Econometrics{.tabset}

## **Questions and Concepts** {.tabset}

### Questions

- How to obtain log likelihood for probit and logit? (PS1 ex 2a)

- What is the **exclusion restriction**? It's a requirement for the Instrumental Variables to work. Assuming that the variable of interest is not entirely random, by the exclusion restriction we assume that the **only impact of the instrumental variable is on the variable of interest**. In the MM example: the only impact of offered seats is on whether they will enroll or not. 

- What are the **endogenous variables**? The dependent variable and the independent variable(s) to be instrumented. In a simultaneous equations model, endogenous variables are determined by solving a system of stochastic linear equations(MHE pg 126). To treat an independent variable as endogenous is to instrument it, i.e. replace it with fitted values in the second stage of a 2SLS procedure.

- And the **exogenous variables**? The exogenous variables include the exogenous covariates that are not instrumented and the instruments themselves. In a simultaneous equations model, exogenous variables are determined outside the system. Ex: in Angrist&Krueger(1991) study are dummies for year of birth and state of birth. Exogenous covariates are controls. 

- What is within-cell variance? ?The values the variable of interest can assume?

- Why is every Wald an IV estimator?

- WNLS for probit/logit?

- [**Heckman Correction**](https://en.wikipedia.org/wiki/Heckman_correction): imagine the is bias from non-randomly selected samples or incidentally truncated dependent variables(ex: wage). To correct for this: we explicitly model the individual sampling probability of each observation (the so-called selection equation) together with the conditional expectation of the dependent variable(so-called outcome equation). So we have two moments in correcting our problem: selection and outcome. The resulting likelihood function is mathematically similar to a Tobit model for censored dependent variables, a connection first drawn by Heckman in 1976. He estimated a two-step control function to estimate this model. In the first step, a regression for observing the positive outcome of the dependent variable is modeled with a probit model. The Inverse Mill's Ratio must be generated from the estimation of a probit model(logit can't be used). The probit model assumes that the error term follows a standard normal distribution. The estimated parameters are used to calculate the Inverse Mills Ratio, which is then used as an additional explanatory variable in the OLS estimation. 

- Inverse Milles Ratio? [**Mills Ratio**](https://en.wikipedia.org/wiki/Mills_ratio) is the following ratio: $m(x) = \frac{\overline{F}(x)}{f(x)}$, where: f(x) is the *probability density function*; $\overline{F}(x): = Pr [X> x] = \int_x^{+\infty} f(u)du$ is the *complimentary cumulative distribution function (aka survival function)*. Example: Imagine we have the following model that predicts wage on the observed characteristics of individuals *i*: $y_i^\ast = x_i'\beta + \varepsilon_i$ for $i = 1,...n$. But there are some people that do not work, generating the truncated value of 0 for wages. Let's model the decision to work the following way: $d_i^\ast = z_i' \gamma + v_i$ for $i = 1,...,n$. If $d_i^\ast$ is > 0, we observe $y_i = y_i^\ast$, otherwise we simply don't observe the wage. Heckman(?) tried to account for the *endogeneity* in this selection bias situation: he suggested that first we estimate $\gamma$(coefficient that influences the decision to work) via MLE probit, usually via an exclusion restriction. Then, we estimate an **Inverse Mill's Ratio** that indicates the probability that an agent decides to work over the cumulative probability of an agent's decision: $\lambda_i = \frac{\phi(z_i' \gamma)}{z_i' \gamma}$. Since we're using probit, we're actually estimating $\frac{\gamma}{\sigma_v}$. The estimated value $\hat{\lambda}_i$ is a means for controlling the endogeneity, i.e., the part of the error term for which the decision to work influences the wage earned. So, the second step is actually: $y_i^\ast = x_i'\beta + \mu \hat{\lambda}_i + \varepsilon_i$. How do we interpret the Inverse Mill's Ratio coefficient: $\mu$? It is: $\frac{\sigma_{ev}}{\sigma_v^2}$, which *represents the fraction of the covariance between the decision to work and the wage earned relative to the variation in decision to work*. To test for selection bias: t-test on whether $\mu$ = 0 (sigma of error of the two equations = 0) or cov(e,v) = 0.

- What is **partialling out**? The process of removing a variable (by giving it a fixed value) to identify any correlation between others. 

## **Useful Stuff** {.tabset}

### **Basic Info**

#### **Documents with basic information about how the course works**

[**2019 Syllabus**](https://gabrielvoelcker.netlify.com/mit/syl381fa19.pdf)

[**2019 Agenda**](https://gabrielvoelcker.netlify.com/mit/agenda381fa19+(new).pdf)

[**2019 Schedule**](https://gabrielvoelcker.netlify.com/mit/sched381fa19+(new).pdf)

### **Class Materials**

#### **Class Materials**

[**1) Mastering Mostly Harmless Metrics**](https://gabrielvoelcker.netlify.com/mit/Intro381fall2019flat.pdf)

[**2) Mastering Regression**](https://gabrielvoelcker.netlify.com/mit/Intro381fall2019flat.pdf)

[**3) Labor Economists Know The Score**](https://gabrielvoelcker.netlify.com/mit/Pscore381fall2019handout.pdf)


### **PSets**

#### **The PSets for the semester are:**

[**Qualifying PS**](https://gabrielvoelcker.netlify.com/mit/Review+problem+set.pdf)

[**PS 1**](https://gabrielvoelcker.netlify.com/mit/psetI_381fa19.pdf)

## **Mostly Harmless Econometrics** {.tabset}

### **1) Questions About Questions**

4 Frequently Asked Questions about the relationship of interest, the ideal experiment, the identification strategy and the mode of inference.

  *i) What is the causal relationship of interest?* Useful for making predictions about the consequences of changing circumstances/policies. 
  
  *ii) what experiment that could ideally be used to capture the causal effect of interest?* What should the treatment be?
  
Fundamentally unindentified questions: FUQs. Questions that cannot be answererd by any experiment. 

  *iii) What is your identification strategy?* Identification is the way the researcher uses his data. 
  
  *iv) What is the mode of statistical inference?* it describes tge population, the sample and the assumptions made when constructing standard errors. How straightforward is your inference?

### **2) The Experimental Ideal**

- Importance of Random assignment: eliminate selection bias. Has the randomization successfully balanced the characteristics across groups?

- Regression: constant treatment effects: the impact of the treatment is the same for everyone. Regressions are important, let's review some facts and properties about them:

### **3) Making Regression Make Sense**

#### 3.1) Regression Fundamentals

- Without randomness there is no causality.

- Conditional Expectation Function: 

- Law of iterated expectations: 

##### 3.1.2) Linear Regression and The CEF

The link between the CEF and regression functions can be explained in three ways. To start, let the K x 1 regression coefficient vector $\beta$ be defined by solving

\[ \beta = arg min_{(b)} E[(Y_i - X_i'b)^2] \]

Using FOC:

\[ E[X_i (Y_i - X_i'b)] = 0 \]

Where the solution can be written as $\beta$ = E$[X_iX_i']^{-1}E[X_iY_i]$. The population residual, which we define as $Y_i - X_i'\beta = e_i$ is uncorrelated with the regressors $X_i$. For the multivariate case, the regression anatomy is:

\[ \beta_k = \frac{Cov(Y_i, \tilde{x}_{ki})}{V(\tilde{x}_{ki})} \]

Where $\tilde{x}_{ki}$ is the residual from a regression of $x_{ki}$ on all the other covariates. Into the math: E[$X_iX_i'$]$^{-1}$E[$X_iY_i$] is the $K \times 1$ vector with *k*th element $\frac{Cov(Y_i,\tilde{x}_{ki})}{V(\tilde{x}_{ki})}$. This formula describes the anatomy of a multivariate regression coefficient and it reveals much more than the matrix formula $\beta$ = $E[X_iX_i']^{-1}E[X_iY_i]$. It is an important formula because it shows that **each coefficient in a multivariate regression is the bivariate slope coefficient for the corresponding regressor after partialing out all the other covariates**. To verify this, substitute: 

\[ Y_i = \alpha + \beta_1x_{1i} + ... + \beta_kx_{ki} + ... + \beta_k x_{ki} + e_i\]

into the numerator of the Regression Anatomy Formula. Since $\tilde{x}_{ki}$ is uncorrelated with $e_i$, it is a **linear combination of the regressors**. Also, since $\tilde{x}_{ki}$ is a residual from a regression on all other covariates in the model, it must be uncorrelated with these covariates. For the same reason, the Cov($\tilde{x}_{ki}$,$x_{ki}$) is just the V($\tilde{x}_{ki}$). Hence: Cov($Y_i, \tilde{x}_{ki}$) = $\beta_k V(\tilde{x}_{ki})$

- **Linear CEF Theorem (3.1.4.)(Regression Justification 1)**: suppose the CEF is linear $\implies$ the population regression function is it. So what makes the CEF linear? Classic scenario: joint normality $\implies$ vector ($Y_i, X_i'$) has a multivariate normal distribution(few empirical validity given the regressors and variables are often discrete). 
**Saturated regression model**: has a separate parameter for every possible combination of values that the set of regressors can take on. 

- **Best Linear Predictor Theorem(3.1.5.)(Regression Justification 2)**: the function $X_i'\beta$ is the best linear predictor of $Y_i$ given $X_i$ in a MMSE sense. $E[Y_i|X_i]$ is the best (MMSE) predictor of $Y_i$ given $X_i$ in the class of *all* functions of $X_i$, the population regression function is the best we can do in the class of *linear* functions. 

-  **The Regression CEF Theorem(3.1.6)(Regression Justification 3)**: the function $X_i'\beta$ provides the MMSE linear approximation to E[$Y_i|X_i$], that is: 

\[ \beta = argmin_{b} E\{(E[Y_i|X_i] - X_i'b)^2\} \]


#### 3.2) Regression and Causality

##### 3.2.1) The Conditional Independence Assumption (CIA)

A regression is causal when the Conditional Expectation Function it approximates is causal. The CEF is causal when it describes differences in average potential outcomes for a fixed reference population. As discussed in chapter 2: **experiments ensure that the causal variable of interest is independent of potential outcomes so that the groups being compared are truly comparable**. We want to generalize this to causal variables that take on more than two values and to more complicated situations where we must hold a variety of control variables fixed for causal inferences to be valid. This leads to the **Conditional Independence Assumption**: the assumption that provides justification for the causal interpretation of regression estimates. Also called **Selection on Observables** because the *covariates are held fixed since they are assumed to be known and observed*. The question is: *what these control variables are?* We call them *covariates* $X_i$ (in the schooling case, $X_i$ is a vector of abilities and family background). 

Suppose we want to know the difference in earnings ($Y_{1i} - Y_{0i}$) if Angrist goes to college($C_i = 1$) versus not going to college ($C_i = 0$). The observed outcome, $Y_{i}$ can be written in terms of potential outcomes as:

\[ Y_i = Y_{0i} + (Y_{1i} - Y_{0i})C_i   \]

We get to see one of $Y_{1i}$ or $Y_{0i}$, but never both. We therefore hope to measure the average of $Y_{1i} - Y_{0i}$, or the average for some group, such as those who went to college. This is E[$Y_{1i} - Y_{0i}|C_i = 1$]. But in general, comparisons of those who attend college vs those who don't are a poor measure of the causal effect of college attendance. We have to take more into consideration, given what we talked about on chapter 2:

\[  \underbrace{E[Y_i|C_i = 1] - E[Y_i|C_i = 0]}_\text{Observed difference in earnings} = \underbrace{E[Y_{1i} - Y_{0i}|C_i = 1]}_\text{ATE on the treated} + \underbrace{E[Y_{0i}|C_i = 1] - E[Y_{0i}|C_i = 0]}_\text{Selection Bias}  \]

It seems likely that those who go to college would have earned more anyway, right? So, the Selection Bias is positive and the naive comparison, $E[Y_i|C_i = 1] - E[Y_i | C_i = 0]$ exaggerates the benefits of college attendance(given the value of the Selection Bias, the observed difference in outcome may be misinterpreted).

The Conditional Independence Assumption asserts that **conditional on observed characteristics**, $X_i$, the Selection Bias disappears. Formally:

\[\{Y_{0i},Y_{1i} \} \perp \perp C_i|X_i  \]

Where the symbol $\perp \perp$ denotes the independence relation; the random variables to the right of $|$ are the **conditioning set**. Given the **Conditional Independence Assumption (CIA)**, conditional on $X_i$, comparisons of average earnings across schooling levels have a causal interpretation. In other words:

\[  E[Y_i|X_i,C_i = 1]  - E[Y_i|X_i,C_i = 0] = E[Y_{1i} - Y_{0i}|X_i]\]

This was established for two possible outcomes: $C_i = 1$ and $C_i = 0$(going to college vs not). Let's expand that to variables that can take on **more than two values**. 

The causal relationship between schooling and earnings is likely to be different for each person, so let's use the individual-specific notation: $Y_{si} \equiv f_i(s)$. This denotes the potential earnings of a person after receiving $s$ years of education. $f_i(s)$ answers causal "what if" questions. In the context of theoretical models of the relationship between human capital and earnings, the form of $f_i(s)$ may be determined by aspects of individual behavior, by market forces, or both. The CIA in this general setup becomes:

\[ Y_{si} \perp \perp s_i|X_i\text{, }\forall s \]

In an observational study, the CIA means that $s_i$ can be said to be "as good as randomly assigned", conditional on $X_i$. Conditional on $X_i$, the **average causal effect** of a one-year increase in schooling is E[$f_i(s) - f_i(s-1)|X_i$], while the ACE of a four-year increase in schooling is E[$f_i(s) - f_i(s-4)|X_i$]. The data reveal only $Y_i$ = $f_i(s_i)$, that is, $f_i(s)$ for $s = s_i$. But given the CIA, conditional on $X_i$ comparisons of average earnings across schooling levels have a causal interpretation:

\[ E[Y_i | X_i,s_i = s] - E[Y_i | X_i,s_i = s - 1] =  E[f_i(s) - f_i(s-1)|X_i], \forall s\]

If we were to compare the difference between 12 and 11 years, we would learn about the average causal effect of high school graduation. The selection bias comes from differences in the potential dropout earnings of high school graduates and non-graduates. However, given the CIA, high school graduation is independent of potential earnings conditional on $X_i$, so the selection bias vanishes. 

So far, we've constructed separate causal effects for each value taken on by the conditioning variables. This leads to as many causal effects as there are values of $X_i$, given that every observation could lead to a unique combination of characteristics. So, Empiricists almost always find it useful to boil a set of estimates down to a single summary measure, for ex: the **unconditional** or **overall average causal effect**. By the law of iterated expectations, the unconditional average causal effect of high school graduation is: 

\[  E\{E[Y_i|X_i,s_i = 12] - E[Y_i|X_i, s_i = 11] \}  \]

\[  = E\{E[f_i(12)-f_i(11)|X_i] \}\]

\[ = E[f_i(12) - f_i(11)] \]

Just as well, how much have high school graduates gained by virtue of having graduated? The average causal effect of high school graduation on high school graduates is:

\[ E\{E[Y_i|X_i,s_i = 12] - E[Y_i|X_i,s_i = 11]|s_i = 12\}   \]

\[ = E \{ E[f_i(12) - f_i (11)|X_i]|s_i = 12\}   \]

\[ E[f_i(12) - f_i(11)| s_i = 12]  \]

The **unconditional average effect** can be computed by averaging all the X-specific effects weighted by the marginal distribution of $X_i$. So the empirical **counterpart** is a **matching estimator**: we make comparisons across different schooling groups for individuals with the same covariate values, compute the difference in their average earnings and then average these difference in some way. 

**Regression** provides an easy-to-use empirical strategy that automatically turns the CIA into causal effects. Two routes can be traced from the CIA to regression: **1)** $f_i(s)$ is both linear in $s$ and the same for everyone except for an additive error term, so we can use a linear regression to estimate the features of $f_i(s)$; **2)** recognizing that $f_i(s)$ almost certainly differs for different people and does not need to be linear in $s$. Even so, allowing for random variation in $f_i(s)$ across people and for nonlinearity for a given person, regression can be thought of as a strategy for the estimation of a weighted average of the individual-specific difference $f_i(s) - f_i(s-1)$. 

Now we want to focus on the **conditions required for the regression to have a causal interpretation**. Let's consider a linear constant effects causal model:

\[ f_i(s) = \alpha + \rho s + \eta_i   \]

This equation is linear and says that the functional relationship of interest is the same for everyone. If we were to compute the realized value of s: $s_i$, we would have:

\[ Y_i = \alpha + \rho s_i + \eta_i \]

The penultimate equation explicitly associates the coefficients above with a causal relationship. Since this penultimate equation is a causal model, $s_i$ may be correlated with the potential outcomes $f_i(s)$, or, in this case, the residual term above, $\eta_i$. 

We can decompose $\eta_i$ into a linear function of observable characteristics $X_i$ and an error term $v_i$: $\eta_i = X_i'\gamma + v_i$, where $\gamma$ is a vector of population regression coefficients that is assumed to satisfy E[$\eta_i | X_i$] = $X_i' \gamma$. By virtue of the CIA, we have:

\[ E[f_i(s)|X_i,s_i] = E[f_i(s)|X_i] = \alpha + \rho s + E[\eta_i | X]   \]

\[ = \alpha + \rho s + X_i' \gamma  \]

The residual in the linear causal model: $Y_i = \alpha + \rho s_i + X_i' \gamma + v_i$ is therefore uncorrelated with the regressors $s_i$ and $X_i$, and the regression coefficient $rho$ is **the causal effect of interest**. 


##### 3.2.2) The Omitted Variables Bias Formula

In addition to the variable of interest, we now have introduced a set of control variables, $X_i$, into our regression. The **Omitted Variable Bias(OVB) Formula describes the relationship between regression estimates in models with different sets of control variables**. This is motivated by the notion that *a longer regression has a causal interpretation* while a shorter one does not. This means that the coefficients on the variables included in the shoter regression are **biased**. The difference between coefficients in a short and in a long regression are determined by the OVB formula. 

Imagine we are regressing wages on school years: $Y_i = \alpha + \rho s_i + A_i' \gamma + e_i$. What are the consequences of leaving ability ($A_i$) out of the regression? The resulting short regression is related to the long regression as follows:

Ommited Variables Bias Formula:

\[ \frac{Cov(Y_i,S_i)}{V(S_i)} = \rho + \gamma' \delta_{As} \]

Where $\delta_{As}$ is the vector of coefficients from regressions of the elements of $A_i$ on $s_i$. The OVB formula states: *short equals long plus the effect of omitted times the regression of omitted on included*. Both the OVB formula *and* the regression anatomy formula tell us that short and long regression coefficients are the same whenever the omitted and included variables are uncorrelated. 

We use the OVB formula to get a sense of what would happen if we omitted ability for schooling coefficients. Ability variables have positive effects on wages, and these variables are also likely to be positively correlated with schooling $\implies$ the short regression may be "too big" relative to what we want. But some omitted variables may be negatively correlated with schooling, in which case the short regression coefficient may be too small. 

##### 3.2.3) Bad Controls

Not always more control is better. **Bad Controls** are variables that themselves are outcome variables in the notional experiment at hand.  

#### 3.3) Heterogeneity and Nonlinearity

The assumption of a linear **Conditional Expectation Function(CEF)** is not really necessary for a causal interpretation of regression. Section 3.1.2. showed how the regression of $Y_i$ on $X_i$ and $s_i$ can be thought as as providing the best linear approximation to the underlying CEF, regardless of its shape. If the CEF is causal, the fact that a regression approximates it gives regression coefficients a causal flavor. To explore the link between regression and CEF we dive into the understanding of regression as a computationally attractive **matching estimator**.

##### 3.3.1) Regression Meets Matching

**Matching** is a useful strategy to control for *covariates*, motivated by the Conditional Independence Assumption. Military ex: conditional on the individual characteristics the military uses to select soldiers, veteran status is **independent** of potential earnings. The **appeal** of matching is: matching amounts to covariate-specific treatment-control comparisons, weighed together to produce a **single overall average treatment effect**.

An attractive feature of matching strategies is that they are typically accompanied by an explicit statement of the conditional independence assumption required to give matching estimates a causal interpretation. 

**Matching requires two-steps: matching and then averaging**

That is: CIA assumes that once you have certain characteristics sorted out, a certain outcome does not depend on them.

**REREAD PG 74 FORWARD MHE**

##### 3.3.2) Control for Covariates using Propensity Score

What happens if the covariates $X_i$ are many/multi/continuous? Matching now requires grouping or parametric assumptions, a fact that induces bias. The **Propensity Score Theorem** extends the regression OVB idea to nonparametric conditioning.

**Theorem**: if potential outcomes are independent of treatment status conditional on a multivariate covariate vector $X_i$, then potential outcomes are independent of treatment status conditional on a scalar function of covariates $\rightarrow$ **the propensity score**. It is defined as $p(X_i) = E[D_i|X_i] = P[D_i = 1|X_i]$. Like the OVB formula for regression, the propensity score theorem says that you only need to control for covariates that affect the probability of treatment itself. Also: the only covariate you really need to control for is the probability treatment itself. What it is done: **1)** $p(X_i)$ is estimated using some kind of parametric model (logit/probit); **2)** estimates of the effect of treatment are computed either by matching on the estimated score from this first step or using a weighting scheme. 

- **PSM definition from** [**Dehejia and Wahba(1999)**](https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10473858): 
The propen-sity score is defined as the probability of assignment to treatment, conditional on covariates.

- From Angrist's class *Labor Economists Know The Score*: - Propensity Score Theorem? Suppose the CIA holds for $Y_{ij}$ (j=0,1) $\implies Y_{ji} \perp \perp D_i|p(X_i)$.

**Proof for the Propensity-Score Theorem**: the claim is true if $P[D_i = 1| Y_{ji}, p(X_i)]$ does not depend on $Y_{ji}$:

<center>
```{r, echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/R/Site-Gabriel/public/MIT")
img <- readPNG("34.png")
grid.raster(img)
```
</center>

We use the score to match, similarly as Covariate matching, except that we match on the score-as-covariate. By the propensity score theorem and the CIA, $E[Y_{1i} - Y_{0i}|D_i = 1]$ is equal to:

\[ E\{E[Y_i|p(X_i),D_i = 1] - E[Y_i|p(X_i),D_i = 0]|D_i=1 \} \]

- Back to MHE pg 81: 

Example of weighting scheme: **we match on score** instead of the covariates directly. By the PST and the CIA:

\[ E[Y_{1i} - Y_{0i}|D_i = 1]  \]

\[ E\{E[Y_i|\rho (X_i), D_i = 1] - E[Y_i|\rho(X_i),D_i = 0]|D_i = 1\} \]

Estimates of the effect of treatment on the treated can therefore be obtained by: **1)**  stratifying on an estimate of p($X_i$); and **2)** substituting conditional sample averages for expectations or **2)** by matching each treated observation to controls with similar values of the propensity score(used by the paper we review in class: [**Dehejia and Wahba(1999)**](https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10473858). 

Another approach one could apply a model-based or nonparametric estimate of E$[Y_i|\rho(X_i),D_i]$ can be substituted for these conditional mean functions and the outer expectation replaced wih a sum(this was done [**here**](https://watermark.silverchair.com/65-2-261.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAnAwggJsBgkqhkiG9w0BBwagggJdMIICWQIBADCCAlIGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMdjtaBMX_bvz4Op8tAgEQgIICI2z4QA0M_9j0_jXxcnqGUgvt6nl8eYAt1Yupnrf7ET_YcJCbCDV_3WJeLP0I35F-Z-2ZIcJupppgb9FTd7V65b4YT5eWkuQ1SQTBt1DrCyz1qUbW_-DaqgvACuHP2w991HULxqzE8YTLqNp_WVLL2HfmlZJdHewnmP3DYusW_zrhv8i0VepUaTdgYsERy8P6atL422JcMC4XewBtMut-Tu-9uB-Kb4FLZIjt0YwqCmkEZKAB-r2sJZd6T5BzMoPU9quKbcsplhsC3tJHoeo_jGjN9yJA1FZD9PtiIk0ZLcwAdlFKsx4Yak0f4S_7NWKnwIUzqG6jhAA26GNIMNFakSZJhZfT_W7EE-MzpiwMfTOEmQLmhgwKJntdSukNBkQDWNCl3567r-RjtWs7WBUGJFJClTaoQibY-h6VgfolMCQCrhtNvthVZOVHhc3V9o3O-n01pczD3X5Zhs6Bgl7YB_u7eZ49y4LtPca56YUZZ03T_gdabLlYlX2QLhGhL-tXhb_6ffs_TRtgs-xEFnK29D8sAXcvFcxc244BgT7atdeQqNns-NlcxCr6hT9MM3cW4SV6wSZX09gZ_dhMHBX2-Xn0MbBlYZZbNdnJFuLb9OfeSkGssi7RIpvSdacIJWfrVJO-HQHY2cXz1PAEoXs6LPncPukY185gPr5jADYw3klGuhW5Ej7XoPncbu0g2oiThm0PrF40zdK6yxJ49itP8k3QWmA)).

We can calculate the effect of treatment on the treated from the sample analog of:

\[ E[Y_{1i} - Y_{0i}|D_i = 1] = E[\frac{(D_i - \rho(X_i))Y_i}{(1 - \rho(X_i))P(D_i = 1)}]  \]

With a consistent estimator of $\rho(X_i)$ it is possible to **correct for nonrandom sampling** via weighting by the reciprocal of the probability of selection. The first idea for this came from [**Horvitz-Thompson**](http://www.stat.cmu.edu/~brian/905-2008/papers/Horvitz-Thompson-1952-jasa.pdf): in their version of the propensity score approach the estimator is essentially automated, no need for matching. This approach highlights the link between PSM and regression. Consider again(as previously seen in 3.3.1) the regression estimand, $\delta_R$, for the population regression of $Y_i$ on $D_i$, controlling for a saturated model for covariates. This estimand can be written:

\[ \delta_R = \frac{E[(D_i - \rho(X_i))Y_i]}{E[\rho(X_i)(1 - \rho(X_i))]} \]

The two estimators obtained from the last two equations are in the class of weighted average estimands considered by [**Hirano, Imbens and Ridder(2003)**](https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00442):

\[ E \{g(X_i) [\frac{Y_iD_i}{\rho(X_i)} - \frac{Y_i(1 - D_i)}{(1 - \rho(X_i))}] \}  \]

Above, $g(X_i)$ is a known weighting function. **So you calculte the effect of the treatment on the treated with an easier less cumbersome approach using weights**. 

Thought-provoking theorems on the efficient use of the propensity score:

i) From the point of view of asymptotic efficiency, **there is usually a cost to matching on the propensity score instead of full covariate matching**. If we investigate the maximal precision of estimates of treatment effects under the CIA, with and without knowledge of the propensity score. A regression analog for this point is the result that even in a scenario with no OVB, the long regression generates more precise estimates of the coefficients on the variables included in a short regression whenever the omitted variables have some predictive power for outcomes. So **why should we use PSM?** It helps the scientist to focus on models for treatment assignment(something he is well-informed of) but not on the typically more complex and mysterious process that determines outcomes. 

ii) Even though there is no **asymptotic efficiency gain** for using the propensity score, there will often be a gain in precision in **finite samples**. Tradeoff: if the covariates omitted from the propensity score explain little of the variation in outcomes, it may be better to ignore them than to bear the statistical burden imposed by the need to estimate their effects. 

iii) [**Hirano, Imbens and Ridder(2003)**](https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00442): show that for models with continuous covariates, a Horvitz-Thompson-type weighting estimator is efficient when the weighting scheme uses a *nonparametric* estimate of the score(even though estimates of treatment effects based on a known propensity score are inefficient). 

##### 3.3.3) Propensity Score Methods vs Regression

Propensity score methods shift attention from the estimation of $E[Y_i | X_i, D_i]$ to the estimation of the propensity score: $\rho(X_i) \equiv E[D_i |X_i]$. 

Conclusion(Pg91): the experiments made in this subschapter boost the authors faith in regression. Regression controls for the right covariates does a reasonably good job of eliminating selection bias in the CPS-1 sample despite a huge baseline gap. Restricting the sample using our knowledge of program admissions yields even better results with CPS-3. Systematic **prescreening** to enforce common support seems like a useful adjunct to regression estimation with CPS-1, a large and coarsely selected initial sample. 











### **4) Instrumental Variables in Action**

From [**Wikipedia**](https://en.wikipedia.org/wiki/Instrumental_variables_estimation): the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment. IVs are used when an explanatory variable of interest is correlated with the error term, in which case ordinary least squares and ANOVA give biased results. A valid instrument **induces changes in the explanatory variable but has no independent effect on the dependent variable**, allowing a researcher to uncover the causal effect of the explanatory variable on the dependent variable.

Instrumental variable methods allow for consistent estimation when the explanatory variables (covariates) are correlated with the error terms in a regression model. Such correlation may occur **1)** when changes in the dependent variable change the value of at least one of the covariates ("reverse" causation), **2)** when there are omitted variables that affect both the dependent and independent variables, or **3)** when the covariates are subject to non-random measurement error. Explanatory variables which suffer from one or more of these issues in the context of a regression are sometimes referred to as endogenous. In this situation, ordinary least squares produces biased and inconsistent estimates.

##### **4.1.1)** 2SLS

(Pg 121)

The first stage fitted values are consistently estimated by $\hat{s}_i = X_i'\hat{\pi}{10} + \hat{\pi}_{11} z_i$.

The coefficient on the $\hat{s}_i$ in the regression $Y_i$ on $X_i$ and $\hat{s}_i$ is called the 2SLS estimator of $\rho$. In other words, 2SLS estimates can be constructed by OLS estimation of the "second-stage equation": $Y_i = \alpha 'X_i + \rho \hat{s}_i + [\eta_i + \rho(s_i - \hat{s}_i)]$. This is called 2SLS because it is done in two steps: i) estimating $\hat{s}_i$ and ii) estimating $Y_i$. The resulting estimator is consistent for $\rho$ because the covariates and first-stage fitted values are uncorrelated with both $\eta_i$ and ($s_i - \hat{s}_i$). The intuition is: the 2SLS retains only the variation in $s_i$ that is generated by quasi-experimental variation - that is: generated by the instrument $z_i$. 

2SLS is splendid! It is an IV estimator: the 2SLS estimate of $\rho$ is the smple analog of $\frac{Cov(Y_i, \hat{s}_i^\ast)}{Cov(s_i, \hat{s}_i^\ast)}$, where $\hat{s}_i^\ast$ is the residual from a regression of $\hat{s}_i$ on $X_i$. This follows from the multivariate regression formula and the fact that Cov($S_i, \hat{S}_i^\ast$) = V($\hat{s}_i^\ast$). 

What is the **link** between 2SLS and IV? We might want to combine alternative IV estimates into a single more precise estimate. 

#### **Recap of IV and 2SLS

- **Endogenous variables**: the *dependent variable* + the *independent variables to be instrumented*. In a simultaneous equations model, endogenous variables are determined by solving a system of stochastic linear equations. To treat an independent variable as endogenous is to instrument it $\rightarrow$ replace it with fitted values in the second stage of a 2SLS procedure. Ex: Angrist&Krueger(1991) use schooling as endogenous variable.

- **Exogenous variable**: includes the *exogenous covariates not instrumented* and the *instruments themselves*. In a simultaneous equations model, exogenous ariables are determined outside the system. Ex: Angrist&Krueger(1991) use dummies for year of birth and state of birth. 

##### **Wald Estimator**

Simplest IV estimator: uses a single dummy instrument to estimate a model with one endogenous regressor and no covariates.

##### **4.1.3 Grouped Data and 2SLS**

Wald and 2SLS are linked by **grouped data**: 2SLS using instruments is the same thing as GLS on a set of group means. GLS is a linear combination of all the Wald estimators that can be constructed from pairs of means. Using the earlier draft-eligibility status study as an example, we understand that there is a more complex link between draft lottery numbers($R_i$) and veteran status($D_i$) than eligibility alone. There is variation in who ended up serving. The thing is, we can separate our analysis and do more comparisons in more intervals than just < or > 195 for 1950 born. The result of this expansion in the set of comparisons is a **set** of Wald estimators. These sets are complete in that the intervals partition the support of the underlying instrument, while the individual estimators are linearly independent in the sense that their numerators are linearly independent. Each of these Wald estimators consistently estimate the same causal effect (assumed here to be constant), as long as $R_i$ is independent of potential outcomes and correlated with veteran status. 

What do we do with this bunch of Wald estimators? Can we translate them into a single number? Yes! And the most efficient linear combination of a full set of linearly independent Wald estimates is produced by fitting a line through the group means used to construct these samples. The OLS estimates of $\rho$ in the grouped equation are consistent, but in practice GLS are preferred since a grouped equation is heteroskedastic with a known variance structure. 

Two reasons why the GLS(WLS) estimator of $\rho$ is important:

1)

2) The GLS estimator is 2SLS. The instruments in this case are a full set of dummies to indicate each lottery number cell. The first-stage regression of $D_i$ on $Z_i$ is saturated, the fitted values will be the sample conditional means $\hat{p}_j$, repeated $n_j$ times for each *j*. The second-stage slope estimate is therefore the same as the slope from WLS estimation of the grouped equation, weighted by the cell size $n_j$. 










## **Mastering Metrics** {.tabset}

### **3) Instrumental Variables**

#### **Instrumental Variables**

**Instrumental Variables** method harnesses partial or incomplete random assignment. 

Ex: charter schools. A charter is the right to operate a public school. But can we compare the effect of these schools to other? How can we employ ceteris paribus to isolate the effect of being at this school?

The decision to attend this school is **not entirely random**. Among applicants, you can be awarded a seat and not enroll, as well as enrolling without being awarded a seat. So we compare those who were offered a seat vs those who weren't. We assume that the only difference created by winning the lottery is the likelihood of charter enrollment(this assumption is called the **exclusion restriction**). So IV turns randomized offer effects into causal estimates of the effect of charter attendance. The estimates of IV capture causal effects on the sort of child who enrolls in KIPP when offered a seat in a lottery. This group is the set of KIPP lottery *compliers*. 

KIPP lotteries randomize the offer of a charter seat, which should balance demographic characteristics. It is important to check for balance in pretreatment outcomes (test scores of lottery applicants prior to KIPP enrollment). These scores have been standardized: subtract the mean and divide by the standard deviation of scores in a reference population. After that, scores are measured in unites defined by the standard deviation of the reference population. The average math score in Lynn is $-0.3\sigma$ compared to the whole population of Massachusetts. The math score for those who were awarded a seat at Lynn is 0.095, an impressive increase when compared to the $-0.3\sigma$ stated above. Those who were not offered a seat had a score of $-0.36\sigma$. 

What does this $-0.36\sigma$ difference tell us? The **IV estimator converts the offer effects into attendance effects**! In this case, the *instrumental variable/instrument is a dummy variable indicating the applicants who receive offers*. In general, an instrument has 3 requirements:

i) **First stage**: the instrument has a causal effect on the variable whose effects we're trying to capture (offered seat has a causal effect on KIPP enrolment). 

ii) **Independence assumption**: The instrument is randomly assigned: unrelated to omitted variables we might want to control for(for example: family background or motivation). 

iii) **Exclusion restriction**: describes a single channel through which the instrument affects outcomes. The exclusion restriction amount to the claim that the $-0.36\sigma$ score differential between winners and losers is attributable solely to the .74 win-loss difference in attendance rates.

The IV method uses these assumptions to characterize the chain reaction leading from the instrument to the outcome. The first link(or the **first stage**) connects randomly assigned offers student performance. The second link(which was the one of interest) is the one comparing treatment(KIPP attendance) with achievement. Thanks to the independence assumption and the exclusion restriction, the product of these two links generates the effect of offers on test scores:

\[ \text{Offers FX on scores = Offers FX on attendance} \times \text{ Effect of attendance on scores} \]

Rearranging:

\[ \text{Attendance FX on scores} = \frac{\text{Offers FX on scores}}{\text{Offer FX on attendance}} \]

The logic is: **KIPP offers are assumed to affect test scores via KIPP attendance alone**. The key point here is related to the third assumption: once offered a KIPP, the only effect on outcome is through KIPP attendance. Offers increase attendance rates by .74, so multiplying Offers FX on scores by about ($\approx 1 / .74$) generates the attendance effect. This adjustment corrects for the facts that roughly a quarter of those who were offered a seat at KIPP chose to go elsewhere and some not offered went anyway. 

We estimate our equation as: [(Average Score of Those Offered a Seat) - (Average score of those not offered a seat)]/[(Proportion of those enrolled at KIPP that were offered a seat) - (Proportion of those enrolled at KIPP that we're not offered a seat)]: (-0,003$\sigma$ - (-0,358$\sigma$))/(0,787 - 0,046) = 0,479$\sigma$. This is an example of an **IV chain reaction**. The direct effect of the instrument on outcomes, which runs the full length of the chain is called the **reduced form**. So we have the first link and the link between instrument and outcome: their difference has got to be the **causal effect of interest**! The second link in the chain is the ratio of reduced form to first-stage estimates. This causal effect is called a **local average treatment effect**(LATE for short). 

These links are made of differences between conditional expectations: comparisons of population averages for different groups. These are, in practice, estimated using sample means, usually with data from random samples. The necessary data are:

- **The instrument $Z_i$**: a dummy variable that equals 1 if the student was offered a KIPP seat.

- **The treatment variable $D_i$**: a dummy that equals 1 if the student attended KIPP(sometimes called the **endogenous variable**).

- **The outcome variable $Y_i$**: fifth-grade math scores. 

The links between in the IV chains are the relationships between those variables: the parameters! 

- **First Stage**: E[$D_i|Z_i = 1$] - E[$D_i|Z_i = 0$]: call this $\phi$. $\phi$ is the difference in KIPP attendance rates between those who were and were not offered a seat in the lottery (=.74).

- **Reduced Form**: E[$Y_i|Z_i = 1$] - E[$Y_i|Z_i = 0$]: call this $\rho$. $\rho$ is the difference in average test scores between applicants who were and were not offered a seat in the lottery. 

- **Local Average Treatment Effect(LATE)**: $\lambda = \frac{\rho}{\phi} = \frac{E[Y_i|Z_i = 1] - E[Y_i|Z_i = 0]}{E[D_i|Z_i = 1] - E[D_i|Z_i = 0]}$. The ratio of the reduced form to the first stage. In the KIPP study, LATE is the difference in scores between winners and losers divided by the difference in KIPP attendance rates between winners and losers. If we substitute the 4 expectation functions by the corresponding sample averages, we get **IV**. In practice, we usually opt for 2SLS(more flexibility).

We've stated how an instrument($Z_i$) changes the variable of interest ($D_i$) and in turn affect outcomes ($Y_i$). LATE can also be denoted as $\lambda = \frac{\rho}{\phi} = E[Y_{1i} - Y_{0i}|C_i = 1]$ ($Y_{1i}$ denotes the outcome for $i$ with treatment, $Y_{0i}$ is the outcome without a treatment). An instrumental variable has **little use** to learn the effects on people whose treatment status can't be changed by manipulating the instrument. 

Researchers and policy makers are sometimes interested also in the average causal effects for the entire treated population, as well as in LATE. This average causal effect is called the **treatment effect on the treated**(TOT): E[$Y_{1i} - Y_{0i} | D_i = 1$]. There are **two ways to be treated: it depends on whether the instrument was switched on, or regardless of the status of the instrument**. In the KIPP study, the threated included always-takers, who were the ones that suffered treatment without being subject of the instrument. Effects on always-takers are not necessarily the same as the effects on compliers. His causal effect may be larger(he really wants to go to KIPP and make the most out of it).

LATE and TOT are usually not the same because the treated population includes always takers. Whether a a particular causal estimate has predictive value for those beyond those represented in the study is a matter of **external validity**. 

##### **Intention-to-treat**

**Intention-to-treat(ITT)**: in randomized trials with imperfect compliance(i.e. treatment assigned ≠ treatment delivered), effects of random assignment can be calculated using intention-to-treat effects. *ITT analysis captures the causal effect of being assigned to treatment*. But an ITT analysis ignores the noncompliance, the solution is: $\frac{\text{ITT effects}}{\Delta\text{compliance in treatment vs control group}}$ = causal effect on compliers. Dividing ITT estimates from a randomized trial by the corresponding difference in compliance rates is another case of IV in action: we recognize ITT as the *reduced form* for a randomly assigned instrument. Ex: the regression of a dummy for having been coddled on a dummy for random assignment to coddling is the *first stage* that goes with this reduced form. The IV causal chain begins with random assignment to treatment, runs through treatment delivered, and ultimately affects outcomes.  


##### **One-stop Shopping with Two-Stage Least Squares**

Imagine we'd like to combine the two IV estimates they generate to increase statistical precision. **2-Stage LS** generalizes IV in two ways: 1) 2SLS estimates use multiple instruments efficiently; 2) 2SLS estimates control for covariates, mitigating OVB from imperfect instruments. 

To understand how 2SLS, we rewrite the LATE equation, but now the reduced form and the first stage are written as the coefficient of interest. So the LATE equation is the ratio between the coefficients of the residual form and of the 1st stage. 

The 2SLS offers an alternative way of computing $\frac{\rho}{\phi}$. In 2SLS first stage, we estimate the equation 3.5 and then save the fitted values $\hat{D}_i$. These first-stage fits are defined as: $hat{D}_i = \alpha_1 + \phi Z_i$. The 2SLS second stage regresses $Y_i$ on $\hat{D}_i$ as: $Y_i = \alpha_2 + \lambda_{2SLS}\hat{D}_i + e_{2i}$. The value of $\lambda_{2SLS}$ is equal to $\frac{\rho}{\phi}$. Control variables like maternal age fit neatly into this two-step regression framework. Adding maternal age ($A_i$) makes the reduced form and the first stage look like:

\[ RF: Y_i = \alpha_0 + \rho Z_i + \gamma_0 A_i + e_{0i} \]

\[ 1stS: D_i = \alpha_1 + \phi Z_i + \gamma_1 A_i + e_{1i} \]

The 1st stage fitted values come from models that include the control variable, $A_i$: 

\[ \hat{D}_i = \alpha_1 + \phi Z_i + \gamma_1 A_i  \]

2SLS estimates are again constructed by regressing $Y_i$ on both $\hat{D}_i$ and $A_i$. Hence, the 2SLS second-stage equation is:

\[ Y_i = \alpha_2 + \lambda_{2SLS} \hat{D}_i + \gamma_2 A_i + e_{2i} \]

Which also includes $A_i$. The 2SLS setup allows for **as many control variables as we would like**, provided they appear on both stages. 

2SLS provides a flexible for IV: it incorporates control variables and uses multiple instruments efficiently(and not only dummies!). In practice, we use special statistical software to do 2SLS in order to get the correct standard errors. 







