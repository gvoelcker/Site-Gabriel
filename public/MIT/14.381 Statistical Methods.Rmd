---
output: html_document
---

# 14.381 - Statistical Methods{.tabset}

## **1 - Distributions and Random Variables**

Based on [**this**](https://gabrielvoelcker.netlify.com/mit/14.381_lec1.pdf). Introduction. Short summary of probabilistic concepts. Normal distribution.

### 1 Random Variables

#### 1.1 Basic Definitions 

[**Random Variable**](https://en.wikipedia.org/wiki/Random_variable): is described informally as a variable whose values depend on outcomes of a random phenomenon. There are 3 types of r.v.: 

1) [**Discrete**](https://en.wikipedia.org/wiki/Random_variable#Discrete_random_variable): can only take on a finite number of values.

2) [**Continuous**](https://en.wikipedia.org/wiki/Random_variable#Continuous_random_variable): 

3) [**Mixed**](https://en.wikipedia.org/wiki/Random_variable#Mixed_type): neither discrete nor continuous.

[**Video**](https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/discrete-and-continuous-random-variables).

[**Cdf definition**](https://en.wikipedia.org/wiki/Cumulative_distribution_function): the cdf of $F_x$ is the probability that X will take a value less than x. $F_x(t)$ = P{X ≤ t} $\forall t \in \mathbb{R}$. P{X ≤ t} denotes the probability that X ≤ t.

#### 1.2 Functions of Random Variables

[**Definition**](https://en.wikipedia.org/wiki/Random_variable#Functions_of_random_variables)

Suppose we have random variable X and function g : $\mathbb{R}$ -> $\mathbb{R}$. Then we can defi􏰀ne another random variable Y = g(X). The cdf of Y can be calculated as follows:

$F_Y$(t) = P {Y ≤ t} = P{g(X) ≤ t} = P{X $\in$ $g^{-1}$ (-$\infty$,t]},

where $g^{-1}$ may be the set-valued inverse of *g*. The set $g^{-1}(-\infty,t]$ consists of all s $\in$ $\mathbb{R}$ such that g(x) $\in$ (-$\infty$,t] i.e. g(s) ≤ t. If g is strictly increasing and continuously differentiable then it has strictly increasing and continuously differentiable inverse $g^{-1}$ defined on set g($\mathbb{R}$). 

**Linear transformation**: if a function is a linear transformation, e.g. Y = X - a, then

<center>$F_Y$(t) = P{Y ≤ t} = P{X - a≤ t} = P{X ≤ t + a} = $F_X$(t + a)</center>

#### 1.3 **Expected Value**

The [**Expected Value**](https://en.wikipedia.org/wiki/Expected_value) of a random variable, intuitively, is the long-run average value of repetitions of the same experiment it represents. For discrete variables:

<center> E[g(X)] = $\sum_i$g($x_i$)$p_i$</center>

And for discrete random variables:

<center>E[g(X)] = $\int_{-\infty}^{+\infty}$ g(x)$f_X$(x)dx.</center>

##### 1.3.1 [**Properties of Expectation**](https://en.wikipedia.org/wiki/Expected_value#Basic_properties)

- E[$1_A$] = P(A)

- If X = Y, then E[X] = E[Y]

- Expected value of a constant.

- Linearity.

- E[X] exists and is finite if and only if E[|X|] is finite.

- If X ≥ 0 (a.s.) then E[X] ≥ 0.

- Monotonicity.

- If |X| ≤ Y (a.s.) and E[Y] is finite then so is E[X].

- If E|$X^ß$ < $\infty$ and 0 < $\alpha$ < ß then E|$X^\alpha$| < $\infty$

- Extremal Property.

- Non-degeneracy.

- If E[X] < $+\infty$ then X < $+\infty$

- Non-multiplicativity.

- Countable non-additivity.

- Countable additivity for non-negative random variables.

#### 1.4 Examples of Random Variables

**Discrete variables**:

- [**Bernoulli**](https://en.wikipedia.org/wiki/Bernoulli_distribution): if a r.v. only takes values $\mathcal{X}$ = {0,1}, P{X = 0} = 1 - p and P{X = 1} = p. Its expectation E[X] = 1 p + 0 (1 - p) = p. Its second moment E[$X^2$] = $1^2 p$ + $0^2 (1-p)$ = p. Thus, the variance is V(X) = E[$X^2$] - $(E[X])^2$ = p - $p^2$ = p(1-p). Notation: X ~ Bernoulli(p).

- [**Poisson**](https://en.wikipedia.org/wiki/Poisson_distribution): r.v. X has a Poisson ($\lambda$) distribution if it takes values from $\mathcal{X}$ = {0,1,2,...} and P(X = j) = $e^{-\lambda}\lambda^j/j!$. Notation: X ~ Poisson($\lambda$). 

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/R/Site-Gabriel/public/MIT")
img <- readPNG("6.png")
 grid.raster(img)
```

**Continuous random variables:**

- [**Uniform**](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous))(a,b): r.v. X has a Uniform(a,b) distribution if its density $f_X$(x) = 1/(b-a) for x $\in$ (a,b) and $f_X$(x) = 0 otherwise. Notation: X ~ U(a,b)

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/R/Site-Gabriel/public/MIT")
img <- readPNG("7.png")
 grid.raster(img)
```

- [**Normal**](https://en.wikipedia.org/wiki/Normal_distribution)($\mu,\sigma^2$): its distribution has density $f_X$(x) = exp($\frac{\frac{-(x-\mu)^2}{(2\sigma^2)}}{\sqrt{2 \pi}\sigma}$) $\forall$ x $\in \mathbb{R}$. E[X] = $\mu$ and variance V(X) = $\sigma^2$. Notation: X ~ N($\mu, \sigma ^2$).

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/R/Site-Gabriel/public/MIT")
img <- readPNG("8.png")
 grid.raster(img)
```

### 2 [**Bivariate (multivariate) distributions**](https://en.wikipedia.org/wiki/Joint_probability_distribution)

Given random variables X,Y, that are defined on a probability space, the joint probability distribution for X,Y, is a probability distribution that gives the probability that each of X,Y, falls in any particular range or discrete set of values specified for that variable. In the case of only two random variables, this is called a bivariate distribution, but the concept generalizes to any number of random variables, giving a multivariate distribution.

#### 2.1 [**Joint, marginal, conditional**](https://sites.nicholas.duke.edu/statsreview/jmc/)

Joint CDF: $F_{X,Y}(x,y)$P{X ≤ x, Y ≤ y}

Joint PDF: $F_{X,Y}(x,y) = \frac{\partial F_{X,Y}(x,y)}{\partial x \partial y}$.

[**Marginal probability**](https://en.wikipedia.org/wiki/Marginal_distribution): is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. 

[**Joint probability**](https://www.investopedia.com/terms/j/jointprobability.asp): is a statistical measure that calculates the likelihood of two events occurring together and at the same point in time. Joint probability is the probability of event Y occurring at the same time that event X occurs.

[**Conditional probability**](https://en.wikipedia.org/wiki/Conditional_probability): is a measure of the probability of an event occurring given that another event has (by assumption, presumption, assertion or evidence) occurred.

- E[f(X)Y| X = x] = f(x) E[Y|X = x];

- Law of iterated expectations: E[E[Y|X = x]] = E[Y]

#### 2.2 Independence

r.v.s X and Y are **independent** if $f_{Y|X}$(y|x) = $f_Y(y)$ for all x $\in \mathbb{R}$. If X and Y are independent, then so are any g(X) and f(Y).

#### 2.3 [**Covariance**](https://en.wikipedia.org/wiki/Covariance)

Covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive. Vice-versa, it's negative.

Useful **properties**:

1) cov(X,Y) = 0 whenever X and Y are independent.

2) cov (aX,bY) = *ab*cov(X,Y) for any r.v. X and Y and any constants *a* and *b*.

3) cov(X + a,Y) = cov(X,Y) for any r.v. X and Y and any constant *a*.

4) cov(X,Y) = cov(Y,X) for any r.v. X and Y.

5) |cov(X,Y)| ≤ $\sqrt{V(X)V(Y)}$ for any r.v. X and Y.

6) V(X+Y) = V(X) + V(Y) + 2cov(X,Y) for any r.v. X and Y.

7) V($\sum_{i = 1}^nX_i$) = $\sum_{i = 1}^n$ V($X_i$) whenever $X_1,...,X_n$ are independent.

### 3 [**Normal Random Variables**](https://en.wikipedia.org/wiki/Normal_distribution)

[**Multivariate normal distribution**](https://en.wikipedia.org/wiki/Multivariate_normal_distribution): is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions. One definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. 

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/R/Site-Gabriel/public/MIT")
img <- readPNG("9.png")
 grid.raster(img)
```

**Properties of the normal distribution:**

1) if X ~ N($\mu, \sum$), then $\sum_{ij}$ = cov($X_i,X_j$) for any i,j = 1,...,n where X = ($X_1,...,X_n$)$^T$.

2) if X ~ N($\mu, \sum$), then $\mu_i$ = E[$X_i$] for any i = 1,...,n.

3) if X ~ N($\mu, \sum$), then any subset of components of X is normal as well. In particular: $X_i$ ~ N($\mu_i,\sum_{ii}$).

4) if X and Y are uncorrelated normal r.v., then X and Y are independent.

5) if X ~ N($\mu_X, \sigma^2_X$), Y ~ N($\mu_Y, \sigma^2_Y$) and X and Y are independent, then X+Y ~ N($\mu_X + \mu_Y, \sigma^2_X + \sigma^2_Y$)

6) Any linear combination of normals is normal. That is, if X ~ N($\mu,\sum$) is an *n x 1* dimensional normal vector, and A is a fixed *k x n* full-rank matrix with k ≤ n, then Y = AX is a normal k x 1 vector: Y ~ N(A$\mu$,A$\sum$A$^T$).

#### 3.1 [**Conditional Distribution**](https://www.statisticshowto.datasciencecentral.com/conditional-distribution/)

A conditional distribution is a probability distribution for a sub-population. In other words, it shows the probability that a randomly selected item in a sub-population has a characteristic you’re interested in. 

## **2 - Limit Theorems**

### 1 Useful Inequalities

[**Markov's Inequality**](https://en.wikipedia.org/wiki/Markov%27s_inequality): gives an upper bound for the probability that a non-negative function of a random variable is greater than or equal to some positive constant. [**Intuition**](https://www.youtube.com/watch?v=4nHcPJsxyv8).
 Let X be any nonnegative random variable such that E[X] exists. Then for any t > 0, we have P{X ≥ t} ≤ E[X]/t.
[**Chebyshev's Inequality**](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality): guarantees that, for a wide class of probability distributions, no more than a certain fraction of values can be more than a certain distance from the mean.
 For any random variable X with mean $\mu$ and finite variance and for any t > 0, we have P{|X - $\mu$| ≥ t} ≤ Var(X)/$t^2$.
[**Hölder's Inequality**](https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality): is a fundamental inequality between integrals and an indispensable tool for the study of $L^p$ spaces.
 E|XY| ≤(E|X|$^p$)$^{1/p}$(E|Y|$^q$)$^1/q$

### 2 [**Convergence in Probability**](https://en.wikipedia.org/wiki/Convergence_of_random_variables) and [**Law of Large Numbers**](https://en.wikipedia.org/wiki/Law_of_large_numbers).

Is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.

**Definition 4**: We say that {$X_n$}$_{n=1}^\infty$ converges to X in probability if for any $\varepsilon$ > 0 P{|$X_n - X$| > $\varepsilon$} -> 0 as n -> $\infty$.

**Theorem 5**: if E($X_n - X$)$^2$ -> 0, then $X_n$ -> X.

**Theorem 6**: if $\{X_i\}_{i = 1}^\infty$ is a sequence of iid r.v. with E[$X_i$] = $\mu$ and Var($X_i$) = $\sigma^2$ < $\infty$, then $	\overline{\rm X}_n$ := $\sum_{i = 1}^n$ $X_i$/n ->$^p$ $\mu$.

**Theorem 7**: if $\{X_n\}_{n=1}^\infty$ is a sequence of iid r.v. with E$X_n$ = $\mu$ and E|$X_n$| < $\infty$, then $\overline{\rm X}_n$ ->$_p$ $\mu$.

### 3 [**Weak Convergence**](https://en.wikipedia.org/wiki/Law_of_large_numbers#Weak_law) and [**Central Limit Theorem**](https://en.wikipedia.org/wiki/Central_limit_theorem)

CLT: the central limit theorem (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a "bell curve") even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.

**Definition 8**: we say that $\{X_n\}_{n=1}^\infty$ converges to X in distribution or weakly if lim$_{n-> \infty}$ F$_{X_n}$(x) = $F_X$(x) $\forall$ x $\in$ $\mathbb{R}$ where $F_X$(x) is continuous. $X_n \implies X$.

**Theorem 9**: If $X_n$ ->$_p$ X, then $X_n \implies X$.

**Theorem 10**: Central Limit Theorem: Let $\{X_i\}$ be a sequence of iid r.v. with a mean $\mu$ and variance $\sigma^2$. Then $\sum_{i = 1}^n$ ($X_i - \mu$)/$\sqrt{n}$ $\implies$ N(0,$\sigma^2$).

**Theorem 11: Linderberg??:  **

### 4 Asymptotic statements derived from basic limit theorems

[**Slutsky's Theorem**](https://en.wikipedia.org/wiki/Slutsky%27s_theorem)

1) If $X_n$ ->$_p$ X and $Y_n$ ->$_p$ Y, then $X_n$ + $Y_n$ ->$_p$ X + Y, and $X_nY_n$ ->$_p$ XY.

2) If $X_n$ $\implies$ X and Y ->$_p$ c, then $X_n$ + $Y_n$ $\implies$ X + c and $X_nY_n$ $\implies$ cX.

[**Continuous Mapping Theorem**](https://en.wikipedia.org/wiki/Continuous_mapping_theorem)

3) If $X_n$ ->$_p$ X, then g($X_n$) ->$_p$ g(X).

4) If $X_n$ $\implies$ X, then g($X_n$) $\implies$ g(x)

#### 4.1 Asymptotic Notation: Symbols $o_p$ and $O_p$

- It is written $x_n = o(b_n)$ when $lim_{n->\infty}$ $\frac{x_n}{b_n}$ = 0. It is described often as "$x_n$ is asymptotically smaller than $b_n$". Informally. it means that $x_n$  grows much slower than $b_n$ and is insignificant in comparison.

- It is written $x_n$ = O($b_n$) when $sup_n$ |$\frac{x_n}{b_n}$| < $\infty$. Intuitively, this means that $x_n$ does not grow faster than ($b_n$).

**Definition 13:** We say that $X_n$ = $o_p$($b_n$) iff $\frac{X_n}{b_n}$ ->$_p$ 0.

### 5 Delta Method

[**Delta Method**](https://en.wikipedia.org/wiki/Delta_method): is a result concerning the approximate probability distribution for a function of an asymptotically normal statistical estimator from knowledge of the limiting variance of that estimator. 

Theorem: assume that for a sequence of random variables $X_n$, and constants $\mu$ and $\sigma$ we have $\sqrt{n}(X_n - \mu)$ $\implies$ N(0,$\sigma^2$). If g'($\sigma$) ≠ 0, then $\sqrt{n}(g(X_n) - g(\mu))$ $\implies$ N(0, $\sigma^2$(g'($\mu$))$^2$).


## **3 - Intro to Statistics**

### 1) Basic Concepts: Population, Sample, Parameter, Statistics

[**Sample**](https://en.wikipedia.org/wiki/Sample_(statistics)): a single draw of data from all potential realizations of that data. It is a realization **x** of vector **X**. 

[**Population**](https://en.wikipedia.org/wiki/Statistical_population): is the distribution $F_x$ of data vector $\mathcal{X}$.

[**Parameter**](https://en.wikipedia.org/wiki/Statistical_parameter): is a numerical characteristic of a statistical population or a statistical model. The goal of statistics is to render some judgement about a parameter (or a population $F_x$) based on a single draw from this population, which is called *inference*. There are three types of inference: 

  a) **Estimation**:
  
  b) **Confidence set construction**:
  
  c) **Testing**:

[**Statistic**](https://en.wikipedia.org/wiki/Statistic): any function of a r.v.. The distribution of a statistic is called the [**sampling distribution**](https://en.wikipedia.org/wiki/Sampling_distribution). 

Types of data:

[**Cross-section**](https://en.wikipedia.org/wiki/Cross-sectional_data): i.i.d. random vectors $X_1,...,X_n$. 

[**Time-series**](https://en.wikipedia.org/wiki/Time_series):  is a series of data points indexed/listed/graphed in time order. Usually they allow for dependency between consecutive observations. $X_t$ with t = 1,...,T.

[**Panel data**](https://en.wikipedia.org/wiki/Panel_data): are multi-dimensional data involving measurements over time. $X_{i,t}$,i = 1,...,n and t = 1,...,T.

#### 1.1) Sample mean and sample variance.

[**Sample mean**](https://en.wikipedia.org/wiki/Sample_mean_and_covariance): is an estimator of the population mean, $\overline{\rm X}_n$ = $\sum_{i=1}^n X_i/n$.

[**Sample variance**](https://en.wikipedia.org/wiki/Variance): variance is the expectation of the squared deviation of a random variable from its mean. $s^2$ = $\frac{\sum_{i = 1}^n(X_i - \overline{\rm X}_n)^2}{n-1}$. 

#### 1.2) Empirical distribution function

[**Empirical Distribution Function**](https://en.wikipedia.org/wiki/Empirical_distribution_function): is the distribution function associated with the empirical measure of a sample. This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points:

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/R/Site-Gabriel/public/MIT")
img <- readPNG("5.png")
 grid.raster(img)
```
This is represented by the formula:

<center>
$\hat{F}_n(x) = \sum_{i=1}^n I(X_i ≤ x)/n$
</center>

Where:

I(.) = indicator function: the function which equals 1 if the statement in brackets is true and 0 otherwise.

$\hat{F}_n(x)$ shows the fraction of observations with a value smaller than or equal to x. It tends to F(x) as n -> $\infty$.

### 2) Ways to find the distribution of a statistic

To make inferences we often need to know the distribution of different statistics.

#### 2.1) Exact Distribution

Rare case in which we can actually calculate the exact distribution of a statistic.

#### 2.2) Monte-Carlos Method

[**Monte-Carlo Method**](https://en.wikipedia.org/wiki/Monte_Carlo_method): are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. The important assumption is: we assume that we know the distribution of data.

#### 2.3) Asymptotic Approximation

[**Asymptotic Approximation**](https://en.wikipedia.org/wiki/Asymptotic_expansion): is a formal series of functions which has the property that truncating the series after a finite number of terms provides an approximation to a given function as the argument of the function tends towards a particular, often infinite, point. 

#### 2.4) Bootstrap

[**Bootstrapping**](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)): is any test or metric that relies on random sampling with replacement. 

### 3) Plug-in estimators

### 4) Parametric Families: Normal


## **4 - Sufficient Statistics**

### 1) Sufficient Statistics

A sufficient statistic is a sample statistic that conveys the same information about the data generating process as the entire data itself. Let f(x|$\theta$) with $\theta \in \Theta$ be some parametric family. X = ($X_1,...,X_n$) is a random sample from distribution f(x|$\theta$). A [**sufficient statistic**](https://en.wikipedia.org/wiki/Sufficient_statistic) separates all the information of X into two parts: 1) all the valuable information for parameter $\theta$; 2) pure noise.

Definition 1: Statistic T(X) is **sufficient** for $\theta$ if the conditional distribution of X given T(X) does not depend on $\theta$. 

Let X(T) be a sufficient statistic and consider the pair (X,T(X)). Obviously, T(X) contains the same information about $\theta$ as X alone. But since T(X) is a sufficient statistic, X itself has no value for us since by observing it we cannot say whether one particular value of parameter $\theta$ is more likely than another. Once we know T(X), we can discard X completely.

### 2) Factorization Theorem

Theorem 2 ([**Factorization Theorem**](https://www.sciencedirect.com/topics/computer-science/factorization-theorem)): Let f(x|$\theta$) be the pdf of X. Then T(X) is a sufficient statistic iff there exist functions g(t|$\theta$) and h(x) such that f(x|$\theta$) = g(T(x)|$\theta$)h(x).

### 3) Minimal Sufficient Statistics

Given two statistics: T(X) and T$\ast$(X). We say that T$\ast$ is not bigger than T whenever having T is sufficient for calculating T$\ast$. In this case, when T$\ast$ changes, T must change as well. 

**Definition 3**: A sufficient statistic T$\ast$(X) is called *minimal* if for any sufficient statistic T(X) there exists some function *r* such that T$\ast$(X) = r(T(X)) $\implies$ the minimal sufficient statistic gives us the greatest data reduction without a loss of information about parameters. 

**Theorem 4**: let f(x|$\theta$) be the pdf of X and T(X) be such that, for any x,y statement $\{f(x|\theta)/f(y|\theta)$ does not depend on $\theta \}$ is equivalent to statement $\{$T(x) = T(y) $\}$. Then T(X) is minimal sufficient.

### 4) Estimators and their Properties

An [**estimator**](https://en.wikipedia.org/wiki/Estimator) is a function of the data(statistic). If we have a parametric family with parameter $\theta$, then an estimator of $\theta$ is usually denoted by $\hat{\theta}$. Ex: sample variance $\hat{\sigma}^2$ is an estimator of the population variance.

#### 4.1) Unbiasness

Let X be our data and $\hat{\theta}$ = T(X) be an estimator where T is some function. We say that $\hat{\theta}$ is **unbiased** if $E_{\theta}[T(X)]$ = $\theta$ for all possible values of $\theta$ where $E_{\theta}$ denotes the expectation when $\theta$ is the true parameter value. The *bias* of $\hat{\theta}$ is defined by Bias($\hat{\theta}$)  = $E_{\theta}[\hat{\theta}] - \theta$. The concept of unbiasness means that we are on average correct. 

#### 4.2) Efficiency: MSE

[**Mean Square Error**](https://en.wikipedia.org/wiki/Mean_squared_error): evaluates the performance of estimators. MSE($\hat{\theta}$) = $E_{\theta}[(\hat{\theta} - \theta)^2]$. 

**Theorem 5**: MSE($\hat{\theta}$) = $Bias^2 (\hat{\theta})$ + V($\hat{\theta}$). The smaller the MSE, the more efficient it is. 

#### 4.3)  Connection between efficiency and sufficient statistics

For any estimator $\hat{\theta}$ = $\delta$(X), there is another estimator which depends on data X only through T(X) and is at least as efficient as $\hat{\theta}$.

[**Theorem 6(Rao-Blackwell)**](https://en.wikipedia.org/wiki/Rao%E2%80%93Blackwell_theorem): define $\phi$(T) = E[$\delta(X)|T$]. Then $\hat{\theta}_2$ = $\phi$(T(X)) is an estimator for $\theta$ and MSE($\hat{\theta}_2$) ≤ MSE($\hat{\theta}$). In addition, if $\hat{\theta}$ is unbiased, then $\hat{\theta}_2$ is unbiased as well. So, we're asked to take an estimate of whatever we want to find and plug in the original estimator and through Rao-Blackwellization find a better estimator. If you plug in biased estimators you'll get biased estimators, otherwise unbiased $\rightarrow$ unbiased estimator.

To solve it: plug in the original estimator and 

### 5) Point Estimators

#### 5.1) Estimators and their properties

An [**estimator**](https://en.wikipedia.org/wiki/Estimator) is a function of the data. If we have a parametric family with parameter $\theta$, then an estimator of $\theta$ is usually denoted by $\hat{\theta}$. Ex: sample variance $\hat{\sigma}^2$ is an estimator of the population variance.

##### 5.1.1) Unbiasness

We say that $\hat{\theta}$ is *unbiased* for $\theta$ if $E_{\theta}$[T(X)] = $\theta$ for all possible values of $\theta$ where $E_{\theta}$ denotes the expectation when $\theta$ is the true parameter value. Bias($\hat{\theta}$) = $E_{\theta}[\hat{\theta}]$ - $\theta$. $\hat{\theta}$ is only unbiased if it equals 0. 

##### 5.1.1.1) Bootstrap bias correction

5 steps to estimate the bias.

##### 5.1.1.2) Efficiency: MSE

##### 5.1.2) Asymptotic Properties

###### 5.1.2.1) Consistency

$\hat{\theta}_n$ is **consistent** for $\theta$ if $\hat{\theta}_n$ $\rightarrow_p$ $\theta$.

###### 5.1.2.2) Asymptotic Normality

We say that $\hat{\theta}_n$ is *asymptotically normal* if there are sequences $(a_n)^\infty_{n = 1}$ and $(r_n)^\infty_{n = 1}$ and constant $\sigma^2$ s.t. $r_n(\hat{\theta} - a_n)$ $\implies$ N(0,$\sigma^2$). Then $r_n$ is called the *rate of convergence*, $a_n$ is the *asymptotic mean* and $\sigma^2$ - the asymptotic variance.

## **Useful Info**{.tabset}

### Schedule

[**Schedule**](https://gabrielvoelcker.netlify.com/mit/schedule_2018.pdf)

### Material

[**Statistical Inference - Casella & Berger**](https://gabrielvoelcker.netlify.com/mit/Casella Berger - Statistical Inference.pdf)

[**All of Statistics - Wasserman**](https://gabrielvoelcker.netlify.com/mit/Wasserman - All of Statistics.pdf)

Useful website [**1**](https://towardsdatascience.com/why-sample-variance-is-divided-by-n-1-89821b83ef6d).

### Problem Sets


### Exams

[**Midterm 2018**](https://gabrielvoelcker.netlify.com/mit/midterm_exam_fall12.pdf)

[**Midterm 2018 Solutions**](https://gabrielvoelcker.netlify.com/mit/midterm_exam_fall12_solutions.pdf)


