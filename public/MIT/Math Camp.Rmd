---
title: "Math_Camp"
author: "Gabriel Voelcker"
date: "8/14/2019"
output: html_document
---

## Math Camp Key Concepts {.tabset}

All the material for the 2019 Math Camp is [**here**](https://gabrielvoelcker.netlify.com/mit/Math Camp 2019.zip).

### Symbols

- $\mathbb{N}$ = set of natural numbers.
    
- $\mathbb{Z}$ = set of integers.
    
- $\mathbb{Q}$ = set of rational numbers.
    
- $\mathbb{R}$ = set of real numbers

- **$\forall$** = "For all".

- **$\exists$** = there exists at least one.

- **$\exists!$** = there exists one and only one. 

- **$\neg$** = negation of the following. "$\neg$p" is the logical opposite of "p".

- **$\in$** = in/is contained in. Ex: a is contained in B = a $\in$ B.

- **$\notin$** = is not contained in. 

- **$\land$** = conjunction.

- **$\lor$** = disjunction. 

- **$\cup$** = union of two sets.

- **$\cap$** = intersection of two sets.

- **$\setminus$** = the set difference.

- **$\subset$**= is a proper subset of. If A $\subset$ B, then all elements of A are on B, but B has at least one element that is not on A.

- **$\subseteq$**= is a subset of. A subset of a set can include all elements of that group. For ex: A={1,3,5}, B={1,3,5}: A $\subseteq$ B. 

- **$\supseteq$**= is a superset of.

- **$A^C$**= is the complement of set A. So $A^C = X \setminus A$.

- **$\varnothing$**= empty set.

- **$\mapsto$**= maps to.

- $2^X$/P(X)= power set.

- "#A"" or "|A|"= cardinality of set A.

- **Open interval**= (0,1), does not include endpoints.

- **Closed interval**= [0,1], includes endpoints.

- $\implies$= implication. 

- $\iff$= equivalence.

- $\ni$= owns / has member(s).

- **x ~ y**= equivalence relation.

- **≥**= partial order.

- **>>**= strict partial order.

- **$\preceq$**= precedes or equals.

### Concepts {.tabset}

#### 1st Lesson{.tabset}

##### Statements

- **Statement**: is a sentence which is either true or false.

- **Quantifier**: is a language element that generates a quantification. The resulting expression is a quantified expression, it is said to be quantified over the predicate whose free variable is bound by the quantifier. The **semantics** of the language specify how the constructor is interpreted. Examples:

    - **Universal**: "for all" ($\forall$). Example: [$\forall$x $\in$ *S*, p(x)] means that p(x) holds for all x in *S*. 

    - **Existential**: "there exists" ($\exists$). Example: [$\exists$x $\in$ *S*: p(x)] means that p(x) holds for at least one x $\in$ *S*. 
    
    - **Exists one and only one**: $\exists!$. Example: [$\exists$x $\in$ *S*: p(x)] means that p(x) holds for only one x $\in$ *S*. 

    - **Negation**: if p is a statement, then $\neg$p is the negation of that statement. For example: $\neg$["She was elected president"] = ["She was not elected president"].
    
- **Conjunction**: if *p* and *q* are statements, *p $\land$ q* denotes conjunction. For example, if it's true that "it's raining"(p), and it's true that "I'm inside"(q), then it's true that "it's raining and I'm inside"(*p $\land$ q*). If at least one of the statements is false, then *p $\land$ q* is false. 

- **Disjunction**: if *p* and *q* are statements, *p $\lor$ q* denotes disjunction. At least one of the statements has to be true for the disjunction to be true. For example: Socrates is a man(p) or pigs are flying in formation over the English Channel(q). Since p is true, then *p $\lor$ q* is true.

- **Implication**: p $\implies$ q means that whenever *p* holds, *q* holds as well. By asserting the conditional, and proving the antecedent(*p*) of the conditional necessarily leads to the consequent (*q*). There are some special names for p $\implies$ q
    
    - **Necessity**: statement *q* is **necessary** for statement *p*. This means that *p* cannot be true without statement *q*.
    
    - **Sufficiency**: statement *p* **suffices** for statement *q*. This means that when *p* is true, *q* is also true.

- **Contrapositive**: is an inference that says that a conditional statement is logically equivalent to its contrapositive. So if p $\implies$ q, then $\neg$p $\implies$ $\neg$q. For example: the proposition *"All cats are mammals"* can be restated as the conditional *"If something is a cat, then it is a mammal"*. The law of contraposition says that statement is true if, and only if, its contrapositive *"If something is not a mammal, then it is not a cat"* is true.

- **Converse**: of a categorical or implicational statement is the result of reversing its two parts. So it means that q $\implies$ p. In general, the verity of S says nothing about the verity of its converse, unless the antecedent P and the consequent Q are logically equivalent. For example, consider the true statement "If I am a human, then I am mortal." The converse of that statement is "If I am mortal, then I am a human," which is not necessarily true.

- **Equivalence**: the statement “*p* if and only if (iff) *q*” is an equivalence. It is exactly the same as “[if *p* $\implies$ *q*] and [if $\neg$*q* $\implies$ $\neg$*p*]”. So, *p* $\iff$ *q* if they have the same truth value in every model. For example, 1) if Lisa is in Denmark(*p*), then she is in Europe(*q*) (*p* $\implies$ *q*) + 2) if Lisa is not in Europe, then she is not in Denmark ($\neg$*q* $\implies$ $\neg$*p*).

##### Set Theory

- **Set** is a collection of elements. Denote elements with lowercase letters and sets with uppercase. Ex: A = {x,y,z}.

- **Operators**:

    - **Union**: the union of two sets A and B is: A $\cup$ B means that {x| x $\in$ A and/or x $\in$ B}.
    
    - **Intersection**: A $\cap$ B denotes the elements that are in both A **and** B ({x|x $\in$ A and x $\in$ B}).
    
    - **Set difference**: A $\setminus$ B (or A-B), means all the elements that are on set A but not on set B ({x|x ∈ A and x ∈/ B}). 
    
    - **Subset**: A $\subset$ B is true iff $\forall$x $\in$ A: x $\in$ B.
    
    - **Equality**: A = B is true iff A $\subseteq$ B and B $\subseteq$ A.
    
    - **Complement**: let A $\subseteq$ X. The complement of A with respect to the space X is defined to be $A^{C}$ = X$\setminus$A.
    
- **Empty set**($\varnothing$ or $\emptyset$): is defined by x $\in$ $\varnothing$ being false for *any* object x. If sets A and B have no elements in common, then A $\cap$ B = $\varnothing$. Also: A $\cup$ $\varnothing$ and A $\cap$ $\varnothing$.

- The **power set** of any set S is the set of all subsets of S, including the empty set($\varnothing$) and the set S itself. So the power set of {x,y,z} is: {$\varnothing$},{x,y},{x,z},{y,z},{x},{y},{z} and {x,y,z}.

- The **Cartesian Product** of two sets **A** and **B** is A x B $\equiv$ {(a,b)|a $\in$ A and b $\in$ B}. So, the Cartesian Product A x B is the set of **all** ordered pairs (a,b) where a $\in$ A and b $\in$ B.*

- Sets: a set is **well-defined** if, given any object, either it is or it is not an element of the set.

- Russell’s paradox: the set of all sets which do not contain themselves.

- Basic Facts about sets:

    - **Distributivity**: A $\cap$ (B $\cup$ C) = (A $\cap$ B) $\cup$ (A $\cap$ C). This means the intersection of set A with the union of sets B and C is the same as the union of the intersection of set A with set B and the intersection of set A with set C.
    
    - **Associativity**: A $\cup$ (B $\cup$ C) = (A $\cup$ B) $\cup$ C, etc. This means that the union of set A with the union of sets B and C is the same as the union of set C with the union of sets A and B.
    
- **De Morgan's Laws**: the complement of the union is the intersection of the complements: (A $\cup$ B)$^C$ = $A^C$ $\cup$ $B^C$.

- The **cardinality** of a set A is denoted by #A or |A| and is the **number of objects** in the set. For example, the set A = {2,4,6} contains 3 elements, and therefore A has a cardinality of 3.

- A set is **countable** when it has the same cardinality (number of elements) as some subset of the set of natural numbers. All finite sets are countable.

- Otherwise, the set is **uncountable**. Ex: $\mathbb{R}$.

- Some commonly used sets are:

    - $\mathbb{N}$ = {1,2,3,...}: natural numbers.
    
    - $\mathbb{Z}$ = {...,-2,-1,0,1,2,...}: integers.
    
    - $\mathbb{Q}$ = {$\frac{p}{q}$| p $\in$ $\mathbb{Z}$, q $\in$ $\mathbb{N}$}: set of rational numbers.
    
    - $\mathbb{R}$ = set of real numbers ($-\infty,\infty$), $\mathbb{R}_{+}$ = [0,$\infty$), $\mathbb{R}_{++}$ = (0,$\infty$).
    
    - $\mathbb{R} \setminus \mathbb{Q}$ = set of irrational numbers.
    
- Remark: $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{Q} \subset \mathbb{R}$

##### Relations

- A [**binary relation**](https://en.wikipedia.org/wiki/Binary_relation)(R) over two sets *A* and *B* is a collection of ordered pairs, *R* $\subseteq$ A x B. It encodes the information of relation: an element a is related to an element b if and only if the pair (a, b) belongs to the set. For ex: Let A be a set of students and B be a set of courses. Define R = {(a, b) $\in$ A × B : student a took course b}.
    
- For (x,y) $\in$ R, write *xRy*.

- For R $\subseteq$ A x A, say [**binary relation on A**](https://en.wikipedia.org/wiki/Binary_relation#Homogeneous_relation).

**There are three important types of relations:**

**1) Equivalence relations**

- An [**equivalence relation**](https://en.wikipedia.org/wiki/Equivalence_relation) on a set A is a binary relation R on A having the following three properties:

    a) **Reflexivity**: *xRx* for every *x* in A.
    
    b) **Symmetry**: if *xRy*, the *yRx*.
    
    c) **Transitivity**: if *xRy* and *yRz*, the *xRz*.
    
Example: "is equal to" in the set of numbers. $\frac{1}{2}$ is equal to $\frac{4}{8}$. 

Examples when each property [**doesn't hold**](https://en.wikipedia.org/wiki/Equivalence_relation#Relations_that_are_not_equivalences):

- *Reflexivity*: the empty relation R on a non-empty set X (i.e. aRb is never true) is vacuously symmetric and transitive, but not reflexive. (If X is also empty then R is reflexive.)

- *Symmetry*: the relation "≥" is not an equivalence relation. For example, 7 ≥ 5 is reflexive (5 ≥ 5) and transitive (9 ≥ 7 ≥ 5 $\implies$ 9 ≥ 5), but it is not symmetric (5 ≥ 7 does not hold).

- *Transitivity*: The relation "has a common factor greater than 1 with" between natural numbers greater than 1, is reflexive and symmetric, but not transitive. Example: The natural numbers 2 and 6 have a common factor greater than 1, and 6 and 3 have a common factor greater than 1, but 2 and 3 do not have a common factor greater than 1).

Upshot from equivalence relations: it is possible to define an [**equivalence class**](https://en.wikipedia.org/wiki/Equivalence_class) of x. So when elements of a set have an equivalence relation defined on them, then it is possible to naturally split them into **equivalence classes**. So, given a set *S* and an equivalence relation *~* on *S*, the equivalence class of an element *a* in S is the set {x $\in$ *S* | x ~ a} of elements which are equivalent to a. For example:

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("2.png")
 grid.raster(img)
```

We can use the set of equivalence classes to define a **partition** of a set A: a collection of disjoint, nonempty subsets of A whose union is all of A: A = $\cup_{x \in A}$ $E_x$, where $E_x$ is the equivalence class of x.

**2) Orders**

- [**Partial Order**](https://en.wikipedia.org/wiki/Partially_ordered_set#Formal_definition): a binary relation R on A that is:

    a) **Reflexive**: xRx, for every x $\in$ A.
    
    b) **Antisymmetric**: if xRy and yRx, then x = y.
    
    c) **Transitive**: If xRy and yRz, then xRz.
    
- [**Strict Partial Order**](https://en.wikipedia.org/wiki/Partially_ordered_set): the difference to the partial order is that *xRx* is **false**.

- [**Complete(Total) Order**](https://en.wikipedia.org/wiki/Total_order): is a binary relation on some set X which is [**antisymmetric**](https://en.wikipedia.org/wiki/Antisymmetric_relation), [**transitive**](https://en.wikipedia.org/wiki/Transitive_relation), and a [**connex relation**](https://en.wikipedia.org/wiki/Connex_relation). For example, the letters of the alphabet ordered by the standard dictionary order: A < B < C.

- A partially ordered set is called a **poset**. For example: the real numbers ordered by the standard less-than-or-equal relation ≤. 5 ≤ 6 ≤ 7, etc.

- A' $\subseteq$ A is **bounded above** if $\exists$b $\in$ A s.t. $\forall$x $\in$ A', x ≤ b. This means that any *b* in set A that is ≥ than any x in subset A' is called the **upper bound** of A'. 

- The smallest such *b*(if it exists) is called the **least upper bound**/[**supremum**](https://en.wikipedia.org/wiki/Infimum_and_supremum). 

- Similarly, there is the **lower bound** and **greatest lower bound/infimum**. 

- For instance, the positive real numbers ℝ+ (not including 0) does not have a minimum, because any given element of $\mathbb{R}^{+}$ could simply be divided in half resulting in a smaller number that is still in $\mathbb{R}^{+}$. There is, however, exactly one infimum of the positive real numbers: 0, which is smaller than all the positive real numbers and greater than any other real number which could be used as a lower bound.

- Infima and suprema do not necessarily exist. Existence of an infimum of a subset S of P can fail if S has no lower bound at all, or if the set of lower bounds does not contain a greatest element. However, if an infimum or supremum does exist, it is unique.

- **Bounded Set**: it is a set that has both upper and lower bounds. Therefore, a set of real numbers is bounded if it is contained in a finite interval.

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("3.png")
 grid.raster(img)
```

**3) Functions**

We call a binary relation *f* over *X* and *Y* a **function** when: 

- $\forall$x $\in$ *X*, $\exists!$y $\in$ Y s.t. (x,y) $\in$ *f*. 

This means that for every x there is only one y.

- X is called the **domain**. 

- Y is called the **codomain**.

- f is called the **rule** (often function).

- f(X) is the **image**.

- Y $\supseteq$ f(X).

- **Injective**: no two points on X map to the **same** Y. $\forall$x, x' $\in$ X : f(x) = f(x) $\implies$ x = x.

- **Surjective**: ("onto") if every element of Y is the image of some element of X under f: $\forall$ y $\in$ Y: $\exists$x $\in$ X: f(x) = y. Any function can be made surjective by **restricting** its range. 

- **Bijective** is both injective and surjective.

- Let *f*: X $\implies$ Y and *g*: Y $\implies$ Z. The **composition** of *g* with *f* is: g $\circ$ f : X $\implies$ Z, x $\mapsto$ g(f(x)). For example: f(x) = 5x, g(x) = $x^2$, a = 2. Then g $\circ$ f(a) = (5 $\bullet$ 2)$^2$ = 100.

- **Mapping with restriction**: f: X -> Y **restricted** to $X_0$ $\subseteq$ X is defined by {(x, f (x))|x $\in$ $X_0$}. Denote by f|$x_0$. For example: suppose agent i attends MIT, and attains utility u : $A_i$ → $\mathbb{R}$; then we may desire to restrict its action set $A_i$ to some subset $A_{i0}$ $\subseteq$ $A_i$ of actions which are available within Cambridge, MA.

- If a function f: X -> Y is bijective, then it is **invertible**, where the **inverse** $f^{-1}$(y) is defined as the unique element x $\in$ s.t f(x) = y. $f^{-1}$(y) is also bijective.

- If Y' $\subseteq$ Y, then the [**preimage**](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/v/preimage-of-a-set) of Y' under *f* is $f^{-1}$(Y') = {x|f(x) $\in$ Y'}. So we go from a subset of our codomain(Y') to find what subset of our domain(X') maps into that subset Y'.

- **Well-defined functions/expressions** are unambiguously determined expressions that have a unique interpretation or value. For ex: is *f*: $\mathbb{Q}$ -> $\mathbb{Z}$, f(a/b) = a a well-defined function? No! f (1/2) = 1, f (2/4) = 2, and 1/2 = 2/4 but 1 ≠ 2.

##### Monotone Comparative Statics

[**Monotone Comparative Statistics**](https://en.wikipedia.org/wiki/Monotone_comparative_statics)

- The [**join**](https://en.wikipedia.org/wiki/Join_and_meet) is the elementwise maximum x $\lor$ x' = (max{$x_i$, $x'_i$})$^n_{i=1}$.

- The [**meet**](https://en.wikipedia.org/wiki/Join_and_meet) is the elementwise minimum x $\land$ x' = (min{$x_i$, $x'_i$})$^n_{i=1}$.

For these operations to be well-defined, a [**lattice**](https://en.wikipedia.org/wiki/Lattice_(order)) is any set X such that x $\lor$ x' $\in$ X and x $\land$ x' $\in$ X for all x, x' $\in$ X.  It consists of a partially ordered set in which every two elements have a unique supremum (also called a least upper bound or join) and a unique infimum (also called a greatest lower bound or meet). An example is given by the natural numbers, partially ordered by divisibility, for which the unique supremum is the least common multiple and the unique infimum is the greatest common divisor. So for example, (4,6) have the LCM(supremum) of 12 and the GCD(infimum) of 2.

- **Strong Set Order**: let Y and Y' be subsets of $\mathbb{R}$. Set Y' dominates Y in the *strong set order* (Y' ≥$_{SSO}$ Y) if for any x' in Y' and x in Y, we have max{x',x} in Y' and min{x',x} in Y.

- [**Supermodularity**](https://en.wikipedia.org/wiki/Supermodular_function): a function *f* : Z -> $\mathbb{R}$ is **supermodular** if

f(z $\lor$ z') + f(z $\land$ z') ≥ f(z) + f(z')

for all z, z' $\in$ Z. "f(z $\lor$ z')" denotes the elementwise maximum and "f(z $\land$ z')" the elementwise minimum of z and z'.

##### Proof Techniques

Given the conjecture p $\implies$ q, there are many ways to go:

1) [**Direct proof**](https://en.wikipedia.org/wiki/Direct_proof): in mathematics and logic, a direct proof is a way of showing the truth or falsehood of a given statement by a straightforward combination of established facts, usually axioms, existing lemmas and theorems, without making any further assumptions. In order to directly prove a conditional statement of the form "If p, then q", it suffices to consider the situations in which the statement p is true. Logical deduction is employed to reason from assumptions to conclusion. The type of logic employed is almost invariably first-order logic, employing the quantifiers for all and there exists. Common proof rules used are modus ponens and universal instantiation.

2) [**Proof by decomposition into cases**](https://en.wikipedia.org/wiki/Proof_by_exhaustion): proof by exhaustion, also known as proof by cases, proof by case analysis, complete induction, or the brute force method, is a method of mathematical proof in which the statement to be proved is split into a finite number of cases or sets of equivalent cases and each type of case is checked to see if the proposition in question holds.

3) [**Proof by induction**](https://en.wikipedia.org/wiki/Mathematical_induction): mathematical induction is a mathematical proof technique. It is essentially used to prove that a property P(n) holds for every natural number n, i.e. for n = 0, 1, 2, 3, and so on.

4) [**Proof by contrapositive**](https://en.wikipedia.org/wiki/Proof_by_contrapositive): In mathematics, proof by contraposition is a rule of inference used in proofs. This rule infers a conditional statement from its contrapositive. In other words, the conclusion "if A, then B" is drawn from the single premise "if not B, then not A.

5) [**Proof by contradiction**](https://en.wikipedia.org/wiki/Proof_by_contradiction): in logic and mathematics proof by contradiction is a form of proof that establishes the truth or validity of a proposition by showing that assuming the proposition to be false leads to a contradiction.

#### 2nd Lesson{.tabset}

##### Metric Topology

- [**Topological Space**](https://en.wikipedia.org/wiki/Topological_space): is a set of points, along with a set of neighbourhoods for each point, satisfying a set of axioms relating points and neighbourhoods. The definition of a topological space relies only upon set theory and is the most general notion of a mathematical space that allows for the definition of concepts such as continuity, connectedness, and convergence. Other spaces, such as manifolds and metric spaces, are specializations of topological spaces with extra structures or constraints. A topology is a set of sets satisfying 3 rules. So considering we have a set X, so T is a topology on X if the following 3 things hold:

    1) Both the empty set($\varnothing$, aka the set with no elements) and the set X have to $\in$ T.
    
    2) If you have two elements in a topology, A,B, then their intersection is also in the topology(the topology is closed under finite intersection). A,B $\in$ T $\implies$ A $\cap$ B $\in$ T.
    
    3) If you have any number of elements in T, their union is also in T(T is closed under arbitrary unions). $A_i$ $\in$ T $\implies$ $U_{i \in I}$ A$_i$ $\in$ T.
  
- Examples:
  
```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("10.png")
 grid.raster(img)
```

    1) Given X = {1, 2, 3, 4}, the collection τ = {{}, {1, 2, 3, 4}} of only the two subsets of X required by the axioms forms a topology of X, the trivial topology (**indiscrete topology**).
    
    2) Given X = {1, 2, 3, 4}, the collection τ = {{}, {2}, {1, 2}, {2, 3}, {1, 2, 3}, {1, 2, 3, 4}} of six subsets of X forms another topology of X.
    
    3) Given X = {1, 2, 3, 4} and the collection τ = P(X) (the power set of X), (X, τ) is a topological space. τ is called the **discrete topology**.
    
    4) Given X = Z, the set of integers, the collection τ of all finite subsets of the integers plus Z itself is not a topology, because (for example) the union of all finite sets not containing zero is infinite but is not all of Z, and so is not in τ.

- [**Metric topology**](https://en.wikipedia.org/wiki/Metric_space): in math, a metric space is a set together with a metric on the set. The metric is a function that defines a concept of distance between any two members of the set, which are usually called points

- [**Metric(Distance)**](http://www-history.mcs.st-and.ac.uk/~john/analysis/Lectures/L15.html): a metric on X is an assignment of a distance d(x,y) $\in$ $\mathbb{R}$ to every pair of points x,y in X. The natural measure of distance: |x - y|. On a set X is a function d: X x X -> $\mathbb{R}$ satisfying:

      1) **Nonnegativity**: $\forall$ *p*,*q* $\in$ X, d(p,q) > 0 if p ≠ q.
      
      2) **Symmetry**: d(p,q) = d(q,p) $\forall$ *p*,*q* $\in$ X.
      
      3) **Triangle inequality**: d(p,r) + d(r,q) ≥ d(p,q) $\forall$ *p*,*q*,*r* $\in$ X.
      
- **Metric space**(X,d): any set *X* with a metric d(.,.) defined as above.

- **Space of functions**: d(f,g) = $sup_{x\in \mathbb{R}}$ |f(x) - g(x)|

- Any **metric subspace** (Y, d|$_{Y x Y}$), where Y $\subset$ X for a known metric space (X,d) and d|$_{Y x Y}$ is the restriction of *d* to *Y x Y*.

- [**Open ball**](http://mathworld.wolfram.com/OpenBall.html): an n-dimensional open ball of radius *r* is the collection of points of distance less than r from a fixed point in Euclidean n-space. Explicitly, the open ball with center x and radius r is defined by:

$B_r$ (x) = {y: |y - x| < r}

- [**Bounded Set**](https://en.wikipedia.org/wiki/Bounded_set): a set is called **bounded** if it is of a finite size. $sup_{x,y \in A}$ d(x,y) < $\infty$.

- **Interior Point**: a point is called the **interior point** of the subset A $\subseteq$ X if there exists and r > 0 such that $B_r$(x) $\subseteq$ A. The set of **all interior points** of A is denoted by **Å** and called the **interior** of A.

- **Proper limit point**: Limit points that themselves are not included on the set.

- For example: in order to construct a bounded set of real numbers with exactly three limit points which themselves are not included in the set, take {k + $\frac{1}{n}:k$ $\in$ {0,1,2}}, n $\in$ {2,3,4,5,...}}. We would then have

```{r Limitpoints, tidy=FALSE, echo=FALSE, message=FALSE, include=TRUE, warning=FALSE}
library(kableExtra)
library(tidyverse)
Coluna1 <- c(2,3,4,5)
Coluna2 <- c(1/2,1/3,1/4,1/5)
Coluna3 <- c(3/2,4/3,5/4,6/5)
Coluna4 <- c(5/2,7/3,9/4,11/5)

Limitpoints <- as.data.frame(cbind(Coluna1,Coluna2,Coluna3,Coluna4))
colnames(Limitpoints) <- c("Values", "0", "1", "2")
knitr::kable(Limitpoints[], caption = 'Limit Points Table') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

- The table values are the numbers included on the set. 0,1 and 2 are not included on the set, and they are limit points.

- If **Å** = A, then A $\subseteq$ X is an [**open subset**](https://en.wikipedia.org/wiki/Open_set).

- [**Closed Set**](https://en.wikipedia.org/wiki/Closed_set): is a set whose complement is an open set. In a topological space, a closed set can be defined as a set which contains all its limit points. Alternatively, A point x $\in$ X is a **limit point** of the subset A $\subseteq$ X, if for all r > 0, it holds that $B_r$(x)$\cap$(A$\setminus${x} ≠ $\varnothing$).

- **Closure of subset A**: the set A unified with all its limit points is denoted by $\overline{A}$. A $\subset$ X is closed if and only if it contains all of its limit points, i.e. A = $\overline{A}$.

- **Boundary of a set**: $\partial$A $\equiv$ $\overline{A} \setminus Å$ is called the boundary of set A. It comprises all limit points of A.

- **Dense**: A is **dense** in X if $\overline{A}$ = X.

##### Sequences

- A [**sequence**](https://en.m.wikipedia.org/wiki/Sequence) is an enumerated collection of objects in which repetitions are allowed. The number of elements (possibly infinite) is called the **length** of the sequence. A **sequence** in X is a function p : $\mathbb{N}$ -> X. Often p(n) is denoted as $p_n$ and the sequence is written as $(p_n)^\infty_{n=1}$ or $(p_n)_{n≥0}$.

- **Convergence**: if a sequence converges, it converges to a particular value known as the **limit**. So, ($p_n$) converges if $\forall$ $\varepsilon$ > 0, $\exists$ N > 0 s.t. d($p_n$, p) < $\varepsilon$ $\forall$ n ≥ N.

- *p* is the **limit** of $p_n$. So, $p_n$ -> *p*.

- Lemma:

    1) $p_n$ -> p iff d($p_n$, *p*) -> 0.
    
    2) If $p_n$ -> p and $p_n$ -> p', then p = p' (at most 1 limit)
    
    3) If $p_n$ -> p, then {$p_n$|n $\in$ $\mathbb{N}$} is a bounded set.

- [**Monotone Convergence Theorem**](https://en.wikipedia.org/wiki/Monotone_convergence_theorem): if a sequence is increasing and bounded above by a supremum, then the sequence will converge to the supremum;  in the same way, if a sequence is decreasing and is bounded below by an infimum, it will converge to the infimum. So, if ($x_n$) $\subset$ $\mathbb{R}$ bounded and monotone, $x_{n+1}$ ≥ $x_n$, then $x_n$ converges.

- [**Subsequence**](https://en.wikipedia.org/wiki/Subsequence): is a sequence that can be derived from another sequence by deleting some or no elements without changing the order of the remaining elements. So, a subsequence of ($p_n$) is any sequence ($p_{n_k}$)$_k$ such that ($n_k$) is an increasing sequence in $\mathbb{N}$. For example, $\frac{1}{2n}$ is a subsequence of $\frac{1}{n}$.

- **Bolzano-Weierstrass Theorem**: every bounded sequence ($x_n$) in $\mathbb{R}$ has a convergent subsequence.

- [**Cauchy Sequence**](https://en.wikipedia.org/wiki/Cauchy_sequence): is a sequence whose elements become arbitrarily close to each other as the sequence progresses. More precisely, given any small positive distance, all but a finite number of elements of the sequence are less than that given distance from each other. So, $\forall$ $\varepsilon$ > 0 $\exists$ N > 0 s.t. $\forall$ m,n ≥ N : d($x_n,x_m$) < $\varepsilon$. **Every** convergence sequence is a Cauchy Sequence.

- The utility of the Cauchy sequence lies in the fact that in a **complete** metric space (one where all such sequences are known to converge to a limit), the criterion for convergence depends only on the terms of the sequence itself, as opposed to the definition of convergence, which uses the limit value as well as the terms.

- [**Contraction Mapping**](https://en.wikipedia.org/wiki/Contraction_mapping): is a function *f* from M to itself, with the property that there is some nonnegative real number 0 ≤ k < 1 such that $\forall$ x and y in M, d(f(x),f(y)) < k d(x,y).

- **Contraction Mapping Theorem**: every contraction *f* has a unique fixed point x $\in$ X s.t. f(x) = x.

##### Series

- **Series**($s_n$) is a particular subset of sequences in $\mathbb{R}$, that can be written as

$s_n$ = $\sum_{k = 1}^{n} a_k$,

for some other real sequence ($a_k$).

- There are many types of series:

    1) Geometric series: $\sum_{k = 0}^{n} q^k$
    
    2) Harmonic series: $\sum_{k = 1}^{n} \frac{1}{k}$
    
    3) Alternating harmonic series: $sn = \sum_{k = 1}^{n} (-1)^{k+1\frac{1}{k}}$
  
    4) Sample mean: $\overline{x}_n = \frac{1}{n} \sum_{i = 1}^{n} x_i$
    
    5) Discounted stream of lifetime utility up to time: T: $\sum_{t = 1}^{T} ß^tu(c_t)$
    
- **Limit of a Sequence**: is the value that the terms of a sequence "tend to". This makes the sequence convergent. This happens iff $\forall$ $\varepsilon$ > 0, $\exists$ N < $\infty$ s.t. $\forall$ m,n < N, |$\sum_{k = m}^{n}a_k$ < $\varepsilon$.

##### Continuity

- **Continuity**: indicates that the map has no gaps. Under some conditions, continuity will guarantee:

    1) Existence of solutions to various equations involving *f*.
    
    2) Maxima and minima of *f*.

- Defining if a **function is continuous**: Let (X,dx) and (Y,dy) each be metric spaces. If $\forall$ x $\in$ X, $\forall \varepsilon$ > 0, $\exists \delta$ > 0: $\forall$ x' $\in$ X, dx(x,x') < $\delta$ $\implies$ $d_y$(f(x),f(x')) < $\varepsilon$. The intuition behind this is that as x,x' get closer, so do f(x) and f(x').

- Informally, we could say that [**close points**](http://www-history.mcs.st-and.ac.uk/~john/analysis/Lectures/L13.html) ($\delta$ apart) are mapped to close points ($\varepsilon$ apart).

- [**Connectedness**](https://en.wikipedia.org/wiki/Connectedness): states that X is connected if it **can't** be written as the union of two nonempty subsets, each disjoint from the closure of the other("separated by them"). Continuity **preserves** connectedness.

- [**Intermediate Value Theorem**](https://en.wikipedia.org/wiki/Intermediate_value_theorem):  if a continuous function, f, with an interval, [a, b], as its domain, takes values f(a) and f(b) at each end of the interval, then it also takes any value between f(a) and f(b) at some point within the interval.

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("4.png")
 grid.raster(img)
```

##### Compactness

- [**Compactness**](https://en.wikipedia.org/wiki/Compact_space) is a property that generalizes the notion of a subset of Euclidean space being closed (that is, containing all its limit points) and bounded (that is, having all its points lie within some fixed distance of each other).

- **Open Cover**: is a collection {$G_\alpha$} of open subsets of X such that A $\subset$ $\cup_\alpha$ $G_\alpha$.

- A is **compact** if every open cover of A has a finite subcover, i.e. $\exists \alpha_1,...,\alpha_n$ s.t. A $\subset$ $\cup^{n}_{i=1}G_{\alpha_i}$.

- **Heine-Borel Theorem**: A $\subset \mathbb{R}^n$ is compact $\iff$ A is closed and bounded.

- [**Extreme Value Theorem**](https://en.wikipedia.org/wiki/Extreme_value_theorem): a real-valued function f is continuous on the closed interval [a,b], then f must attain a maximum and a minimum, each at least once. 

##### Function Spaces

- [**Function Spaces**](https://en.wikipedia.org/wiki/Function_space): are sets of functions between two fixed sets.

- A function f: X -> Y is bounded if its image f(X) is bounded.

- [**Convergence**](https://en.wikipedia.org/wiki/Convergent_series): A series is **convergent** if the sequence of its partial  sums {$S_1, S_2,...,$} tends to a limit. That means that the partial sums become closer and closer to a given number when their terms increases. More precisely, a series converges if there exists a number $\ell$ such that for every arbitrarily small positive number $\varepsilon$, there is a (sufficiently large) integer N such that for all *n* ≥ *N*, |$S_n - \ell$| ≤ $\varepsilon$. If the series is convergent, the number $\ell$ (necessarily unique) is called the **sum of the series**. Any series that is not convergent is called **divergent**. So, $f_n$ converges (uniformly) if $\forall \varepsilon$ > 0, there exists an integer N such that d($f_n$(x),f(x)) < $\varepsilon$ for all x $\in$ X and n > N. 

- Examples of convergent series:

    1) $\frac{1}{1}$ - $\frac{1}{2}$ + $\frac{1}{3}$ - $\frac{1}{4}$ + $\frac{1}{5}$ - $\frac{1}{6}$ + ...  = ln(2).

    2) $\frac{1}{1}$ + $\frac{1}{1}$ + $\frac{1}{2}$ + $\frac{1}{6}$ + $\frac{1}{24}$ + $\frac{1}{120}$ + ...  = e.
    
    3) $\frac{1}{1}$ + $\frac{1}{4}$ + $\frac{1}{9}$ + $\frac{1}{16}$ + $\frac{1}{25}$ + $\frac{1}{36}$ + ...  = $\frac{\pi^2}{6}$.

- Some facts about convergence in $\mathbb{R}$:

    1) $x_n$ + $y_n$ -> x + y.

    2) $x_ny_n$ -> xy.

    3) c $\circ x_n$ -> $cx$.
    
    4) $x_n$/$y_n$ -> x/y (if y ≠ 0).
    
    

- [**Arzelà-Ascoli Theorem**](https://en.wikipedia.org/wiki/Arzel%C3%A0%E2%80%93Ascoli_theorem): is a fundamental result of mathematical analysis giving necessary and sufficient conditions to decide whether every sequence of a given family of real-valued continuous functions defined on a closed and bounded interval has a uniformly convergent subsequence. 



#### 3rd Lesson{.tabset}

##### Univariate Differential Calculus

- **Derivative**: it's the **measure** of the **slope** of a function at a specific point. For example, the slope between $x_0$ and $x_0 + \Delta$ should be close to: $\frac{f(x_0 + \Delta)-f(x_0)}{\Delta}$. We call *f* **differentiable** if that approximation improves enough.

- We say a **limit exists** if $lim_{n->\infty}$g($\Delta_n$) exists and is equal for any sequence $\Delta_n$ -> $\Delta_0$ for which g($\Delta_n$) is defined. In that case, the common limit is denoted by $lim_{\Delta -> \Delta_0}g(\Delta)$.

- **Differentiable**: let f: I -> R be a continuous function defined on some interval I and let $x_0$ $\in$ I. So, **f is differentiable** at $x_0$ if $lim_{\Delta->0}\frac{f(x_0+\Delta)-f(x_0)}{\Delta}$ exists for ($x_0 + \Delta \in I$). In that case, we denote it by f'($x_0$). f is **differentiable** on I if it is differentiable for **any $x_0$** $\in$ I.

- There are some general rules for derivatives:

    1) **Sum rule**: (f + g)'(x) = f'(x) + g'(x).
    
    2) **Product rule**: (f $\circ$ g)'(x) = f'(x)g(x) + f(x)+g'(x)
    
    3) **Quotient rule**: If g(x) ≠ 0, then (f/g)'(x) = (f'(x)g(x)-f(x)g'(x))/g(x)$^2$
    
    4) **Chain rule**: (g $\circ$ f)'(x) = g'(f(x))f'(x)

- A function *f* is in the class $C^k$ if its 1$^{st}$, 2$^{nd}$,..., $k^{th}$ derivatives exist and are continuous. We can also say that f is k-times continuously differentiable. A **smooth** function is one that $f \in C^{\infty}$.

- [**Leibniz Theorem**](https://en.wikipedia.org/wiki/Leibniz_integral_rule): suppose f: X $\subset$ $\mathbb{R} -> \mathbb{R}$ is differentiable and has a maximum at x $\in$ $\mathring{X}$. Then f'(x) = 0.

- [**Rolle's Theorem**](https://en.wikipedia.org/wiki/Rolle%27s_theorem): states that any real-valued differentiable function that attains equal values at two distinct points must have at least one stationary point somewhere between them—that is, a point where the first derivative (the slope of the tangent line to the graph of the function) is zero. So, if f(a) = f(b), then $\exists$x $\in$(a,b) with f'(x)=0.

- [**Mean Value Theorem**](https://en.wikipedia.org/wiki/Mean_value_theorem): is a generalization of Rolle's Theorem, stating, roughly, that for a given planar arc between two endpoints, there is at least one point at which the tangent to the arc is parallel to the secant through its endpoints. So, $\exists$ x $\in$ (a,b) with f'(x) = $\frac{f(b) - f(a)}{b - a}$.

- [**Inverse Function Theorem**](https://en.wikipedia.org/wiki/Inverse_function_theorem): gives a sufficient condition for a function to be invertible in a neighborhood of a point in its domain: namely, that its derivative is continuous and non-zero at the point. The theorem also gives a formula for the derivative of the inverse function. 

- Suppose f:(a,b) -> $\mathbb{R}$ continuously differentiable and fix $x_0 \in (a,b)$. Suppose f'($x_0$) ≠ 0. Then $\exists$ open sets U $\subset$(a,b), V $\subset$ $\mathbb{R}$ s.t.: **1)** $x_0 \in U, f(x_0) \in V, f(U) = V$, and f is injective on U. **2)** the inverse g: V -> U of f is $C^1$ and g'($y_0$) = 1/f'($x_0$) with $y_0$ = f($x_0$).

- For a continuous function to have an **inverse**: **1)** in one dimension: strict monotonicity, i.e. $\forall$x, x', x > x' -> either f(x) > f(x') or f(x) < f(x') consistently. We can find the inverse function g(y ) by solving y − f (x) = 0 for x. **2)** in multiple dimensions: Invertible derivative.

- [**Taylor Expansion Theorem**](https://en.wikipedia.org/wiki/Taylor%27s_theorem): gives an approximation of a k-times differentiable function around a given point by a k-th order Taylor polynomial. For analytic functions the Taylor polynomials at a given point are finite-order truncations of its Taylor series, which completely determines the function in some neighborhood of the point. It can be thought of as the extension of linear approximation to higher order polynomials, and in the case of k equals 2 is often referred to as a quadratic approximation. So, $\exists$ z $\in$ [x,y] s.t. f(y) = $\sum_{k = 0}^{n - 1} \frac{f^{(k)}(x)}{k!}(y-x)^k$ + $\frac{f^{(n)}(z)}{n!}(y - x)^n$. The first term being the approximation and the second the error term.

- This will be used to show a few results in econometrics about, e.g., GMM, so that utility can be usefully written as the sum of marginal utility u'($\circ$) and its concavity u''($\circ$).
      
- [**L'Hôpital Rule**](https://en.wikipedia.org/wiki/L%27Hôpital%27s_rule): uses derivatives to help evaluate limits involving indeterminate forms. Application (or repeated application) of the rule often converts an indeterminate form to an expression that can be evaluated by substitution, allowing easier evaluation of the limit. So, if f,g are differentiable and $\frac{f'(x)}{g'(x)}$ -> A > 0 as x -> $x_0$, and either f(x), g(x) -> 0 or f(x),g(x) -> $\infty$ as x -> $x_0$, then $\frac{f(x)}{g(x)}$ -> A.

##### Multivariate differential calculus

- [**Partial Derivative**](https://en.wikipedia.org/wiki/Partial_derivative): is derivative with respect to one variable in a function that has several variables. So, the **partial derivative** of f at $x_0$ $\in$U in direction i is given by: $\frac{\partial f}{\partial x_i}(x_0) \equiv f_{x_i}(x_0) \equiv lim_{\Delta -> 0} \frac{f(x_0 + \Delta e_i) - f(x_0)}{\Delta}$, if the limit exists. If it exists for all i = 1,...,n, and each $f_{x_i}$ is continuous at $x_0$, then we say *f* is (totally) **differentiable** at $x_0 \in U$.

- [**Jacobian Matrix**](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant): is the matrix of the first-order partial derivatives of a vector-valued function in several variables. $J_f(x_0) = f_x(x_0) = (f_{x1}(x_0),...,f_{x_n}(x_0)) \in \mathbb[R]^{m x n}$. Alternative notation: $f_x, \triangledown f(x), [Df](x)$.

- [**Total Derivative**](https://en.wikipedia.org/wiki/Total_derivative): is the best linear approximation near this point of the function with respect to its arguments.

- [**Hessian Matrix**](https://en.wikipedia.org/wiki/Hessian_matrix): is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. Since it determines local concavity/convexity, it is useful for optimization.

- [**Implicit Function Theorem**](https://en.wikipedia.org/wiki/Implicit_function_theorem): is a tool that allows relations to be converted to functions of several real variables. It does so by representing the relation as the graph of a function. There may not be a single function whose graph can represent the entire relation, but there may be such a function on a restriction of the domain of the relation. The implicit function theorem gives a sufficient condition to ensure that there is such a function. So, F($x_0 + dx, \theta_0 + d\theta) \approx F(x_0,\theta_0) + F_xdx + F_{\theta}d\theta = F_xdx + F_\theta d\theta$ = 0

- The IFT states that if $F_$ is invertible, then there exist: 1) (small) open sets U and V around $\theta_0$ and $\x_0$ and 2) a differentiable function $x^*$ : U -> V s.t. $\frac{\partial x^*(\theta_0)}{\partial \theta}$ = - [$F_x(x_0, \theta_0)]^{-1}F_\theta(x_0,\theta_0)$.

##### Integral Calculus

- [**Integral**](https://en.wikipedia.org/wiki/Integral): assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. There are two usual ways to construct an integral: **Riemann** and **Lebesgue**.

- [**Riemann Integral**](https://en.wikipedia.org/wiki/Riemann_integral): the definition is to use very simple approximations for the area of S. By taking better and better approximations, we can say that "in the limit" we get exactly the area of S under the curve. So, if $U_n$ and $L_n$ converge to the same limit, then that limit is called the Riemann integral of *f* over [a,b], denoted by $\int_{a}^{b}$ f(x)dx. We say *f is Riemann integrable*. What functions are Riemann integrable? If f: [a,b] -> $\mathbb{R}$ is continuous, it is Riemann integrable.

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("5.png")
 grid.raster(img)
```

- [**First Fundamental Theorem of Calculus**](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus): states that one of the antiderivatives (also called indefinite integral), say F, of some function f may be obtained as the integral of f with a variable bound of integration. This implies the existence of antiderivatives for continuous functions. So, define F: [a,b] -> $\mathbb{R}$ by F(t) $\int_{a}^{t}$ f(x)dx. F is differentiable, with F'(t) = f(t).

- **Second Fundamental Theorem of Calculus**: states that the integral of a function f over some interval can be computed by using any one, say F, of its infinitely many antiderivatives. This part of the theorem has key practical applications, because explicitly finding the antiderivative of a function by symbolic integration avoids numerical integration to compute integrals. This provides generally a better numerical accuracy. So, if there exists some differentiable F: [a,b] -> $\mathbb{R}$, with F' = f, then $\int_{a}^{b}$f(x)dx = F(b)-F(a).

- [**Stoke's Theorem**](https://en.wikipedia.org/wiki/Stokes%27_theorem): the integral of a differential form $\omega$ over the boundary of some orientable manifold $\Omega$ is equal to the integral of its exterior derivative d$\omega$ over the whole of $\Omega$, i.e., $\int_{\partial \Omega}\omega = \int_{ \Omega}d \omega$. For example: $\int_{a}^{b}$ $(\frac{1}{x})dx$ = ln(b) - ln(a).

- [**Integration by parts**](https://en.wikipedia.org/wiki/Integration_by_parts):  is a process that finds the integral of a product of functions in terms of the integral of their derivative and antiderivative. It is frequently used to transform the antiderivative of a product of functions into an antiderivative for which a solution can be more easily found. The rule can be readily derived by integrating the product rule of differentiation. So, if F,G : [a,b] -> $\mathbb{R}$ are $C^1$, then $\int_{a}^{b}F(x)G'(x)dx = F(b)G(b)-F(a)G(a)-\int_{a}^{b}F'(x)G(x)dx$.

- [**Fubini's Theorem**](https://en.wikipedia.org/wiki/Fubini%27s_theorem): is a result that gives conditions under which it is possible to compute a double integral by using iterated integral. One may switch the order of integration if the double integral yields a finite answer when the integrand is replaced by its absolute value.

#### 4th Lesson{.tabset}

##### Vector Spaces

- [**Vector Spaces**](https://en.wikipedia.org/wiki/Vector_space): is a collection of objects called vectors, which may be added together and multiplied ("scaled") by numbers, called scalars. So, by fixing some set V, a triple (V,+,$\circ$) consisting of a set V, addition
+: V x V -> V, (x,y) $\mapsto$ x + y; and scalar multiplication ("scaling"), $\circ$: $\mathbb{R}$ x V -> V, ($\alpha$, x) $\mapsto$ $\alpha$ $\circ$ x, is called a **(real) vector space** if the following six conditions hold:

    1) Commutativity: x + y = y + x $\forall$ x,y $\in$ V.
    
    2) Associativity: (x + y) + z = x + (y + z) and ($\alpha \circ \mu$) $\circ$ v = $\alpha \circ (\mu \circ v)$ $\forall$ x,y,z $\in$ V and $\alpha,\mu \in \mathbb{R}$.
    
    3) Origin/additive identity: $\exists$ 0 $\in$ V (the zero vector) s.t. x + 0 = $\forall$ x $\in$ V.
    
    4) Additive inverse: for each element x $\in$ V, there is an element y $\in$ V s.t. x + y = 0.
    
    5) Multiplicative identity: 1 $\circ$ x = x $\forall$ x $\in$ V.

    6) Distributive properties: $\alpha \circ$ (x + y) = $\alpha \circ$ x + $\alpha \circ$ y and ($\alpha$ + $\mu$) $\circ$ x = $\alpha$ $\circ$ x + $\mu$ $\circ$ x $\forall$ $\alpha$,$\mu$ $\in$ $\mathbb{R}$, x,y $\in$ V.
    
  - The **+** and **$\circ$** operations allow us to "fill up" the vector space. We can construct many new vectors in V, for example:
  
    
      
    1) $v_1 + v_2$

    2) 3$v_1$
    
    3) v_1 - 3v$_2$

- [**Subspace**](http://linear.ups.edu/html/section-S.html): a subspace is a vector space that is contained within another vector space. So every subspace is a vector space in its own right, but it is also defined relative to some other (larger) vector space. So, a nonempty subset U $\subseteq$ V is called a **subspace** of V if $\forall$ x,y $\in$ U and $\alpha \in \mathbb{R}$, x + y $\in$ U and $\alpha \circ x \in$ U. A subspace is **closed** under addition and multiplication. This means that you can add/multiply any two members of the subspace and you'll get another member of the subspace.

- If M and N are subspaces of some vector space V, then so is M $\cap$ N.

- [**Convex Subsets of Vector Spaces**](http://mathonline.wikidot.com/convex-subsets-of-vector-spaces): a set K $\subseteq$ V is **convex** if $\forall$ $v_1,v_2$ $\in$ K, all points of the form $\alpha v_1$ + (1 - $\alpha$)$v_2$ with $\alpha$ $\in$ [0,1], are in K.

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("6.png")
 grid.raster(img)
```

- [**Convex Hull**](https://en.wikipedia.org/wiki/Convex_hull): is the smallest convex set of S that contains S.

- [**Linear Combination**](https://en.wikipedia.org/wiki/Linear_combination): is an expression constructed from a set of terms by multiplying each term by a constant and adding the results. For ex: a linear combination of x and y would be any expression of the form ax + by, where a and b are constants. So, a vector v $\in$ V is a **linear combination** of a finite number of vectors $v_1,...,v_n$ if there exist numbers $\alpha_1,...,\alpha_n \in \mathbb{R} s.t.$ v = $\alpha_1 v_1 + ... + \alpha_n v_n$.

- [**Span**](https://en.wikipedia.org/wiki/Linear_span): of a set S of vectors in a vector space is the smallest linear subspace that contains the set. It can be characterized either as the intersection of all linear subspaces that contain S, or as the set of all linear combinations of elements of S. The linear span of a set of vectors is therefore a vector space. So, let S $\subseteq$ V be a nonempty subset. The **span** of S(span(S) or [S]) is the set of all finite linear combinations of elements in S. If span(S) $\supseteq$ V, then we say that S spans V.

- [**Linear (In)dependence**](https://en.wikipedia.org/wiki/Linear_independence): a set of vectors is said to be linearly dependent if at least one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent. So, we say that a list of vectors ($v_1,...,v_n$) in V are **linearly dependent** if the zero element can be expressed as a non-trivial linear combination of them. That is, there exist $\alpha_1,...,\alpha_n$ $\in \mathbb{R}$ not all equal to 0, such that $\sum_{i = 0}^{n} \alpha_i v_i$ = 0. Otherwise, ($v_1,...,v_n$) are **linearly independent**.

- If $v_1,...,v_n$ are linearly independent, then the representation of v $\in$ span($v_1,...,v_n$) as a linear combination v = $\alpha_1v_1$ + ... + $\alpha_n v_n$ is **unique** (i.e., $\forall v \in V, \exists! \alpha \in \mathbb{R}^n : \sum_{k = 1}^{n} v_k = v$).

- [**Basis**](https://en.wikipedia.org/wiki/Basis_(linear_algebra)): a set of B elements(vectors) in a vector space V is called a **basis** if they are a linearly independent set of vectors. If every element of V may be written in a unique way as a (finite) linear combination of elements of B. The coefficients of this linear combination are referred to as components or coordinates on B of the vector. The elements of a basis are called basis vectors. So, vectors ($v_\alpha$)$_{\alpha \in A}$ form a **basis** of V if and only if they are linearly independent *and* span V. This means that B is a subset of V if it satisfies the two following conditions: 

    1) The linear independence property: for every finite subset {$b_1,...,b_n$} of *B* and every $a_1,...,a_n$ in F, if $a_1b_1 + ... + a_nb_n$ = 0, then necessarily $a_1 = ... = a_n = 0$. 
    
    2) The spanning property: for every vector *v* in *V*, it is possible to choose $v_1,...,v_n$ in *F* and $b_1,...,b_n$ in *B* s.t. v = $v_1b_1 + ... + v_nb_n$. 

- [**Standard Basis**](https://en.wikipedia.org/wiki/Standard_basis): in mathematics, the **standard basis** (also called natural basis) for a Euclidean space is the set of unit vectors pointing in the direction of the axes of a Cartesian coordinate system. For example, the standard basis for the Euclidean plane is formed by vectors $e_x = (1,0)$ and $e_x = (0,1)$. And the standard basis for three-dimensional space is formed by vectors $e_x$ = (1,0,0), $e_y$ = (0,1,0) and $e_z$ = (0,0,1). So, for $\mathbb{R}^n$, where $ê_i$ $\in$ $\mathbb{R}^n$ is 0 for every entry except i, which is 1. 

- [**Dimensionality Theorem**](https://en.wikipedia.org/wiki/Dimension_theorem_for_vector_spaces): states that all bases of a vector space have equally many elements. This number of elements may be finite or infinite (in the latter case, it is a cardinal number), and defines the dimension of the vector space. So, suppose some finite list of vectors A = ($a_1,...,a_n$) in V span V and some finite list of vectors B = ($b_1,...,b_m$) are linearly independent in V. Then m ≤ n. Some corollaries:

    1) All (finite) bases have the same length.
    
    2) For a subspace U $\subset$ V: dim U ≤ dim V.
    
    3) If dim V = n and $v_1,...,v_n$ are linearly independent, then ($v_1,...,v_n$) is a basis.

    4) If dim V = n and $v_1,...,v_n$ span V, then ($v_1,...,v_n$) is a basis.
    
    5) Suppose V has a finite basis. Then any list LI list of vectors can also be extended to a basis.
    
- **Dimension of a vector space**: the unique cardinality of any basis.

##### Combining vector spaces

- How to get a new subspace? $U_1 \cup U_2$? No! The sum $U_1$ + $U_2$ is defined as the subspace with elements $u_1$ + $u_2$, where $u_1$ $\in$ $U_1$, $u_2$ $\in$ U$_2$.

- If dim($U_1+U_2)$ < $\infty$, then dim($U_1+U_2)$ = dim $U_1$ + dim $U_2$ - dim($U_1 \cap U_2$).

- [**Cartesian product of vector spaces**](https://planetmath.org/cartesianproductofvectorspaces): if $V_1$ and $V_2$ are vector spaces, then their **(Cartesian) Product**, denoted $V_1 x V_2$, is the collection of ordered pairs ($v_1,v_2$) with $v_1$ $\in$ V$_1$, $v_2$ in $V_2$ with entry-by-entry addition and scalar multiplication, i.e. ($v_1,v_2$) + ($v_1 + v'_1$, $v_2 + v'_2$) and $\alpha(v_1,v_2$) = ($\alpha v_1, \alpha v_2$). Sometimes this is called **direct sum** and denoted with $\oplus$. For ex: $\mathbb{R}$ $\oplus$ $\mathbb{R}$ = $\mathbb{R}^2$.

##### Linear Maps

- [**Linear Map**](https://en.wikipedia.org/wiki/Linear_map): is a mapping V -> W between two modules (including vector spaces) that preserves the operations of addition and scalar multiplication. A map T: V -> W is linear if it is:

    1) Additive: T(u + v) = T(u) + T(v) $\forall$ u,v $\in$ V.
    
    2) Homogeneous of degree 1: T($\alpha v$) = $\alpha \circ$ T(v) $\forall$ v $\in$ V, $\alpha \in \mathbb{R}$.
    
- There are many examples of linear maps:

    1) **Identity map**: $id_v$ : V -> V, defined by $id_v$ : v $\mapsto$ v, is linear.
    
    2) **Addition map** +: $\mathbb{R}^2$ -> $\mathbb{R}$, ($x_1,x_2$) $\mapsto$ $x_1 + x_2$ is linear.
    
    3) **Multiplication map** $\mathbb{R}^2$ -> $\mathbb{R}$, ($x_1,x_2) \mapsto x_1$ $\circ$ $x_2$ is *not* linear.
    
- Fix a linear map T: V -> W. **Theorem**: **1)** for any subspace U $\subseteq$ V, the image T(U) $\subseteq$ W is a linear sequence. **2)** for any subspace Z $\subseteq$ W, the preimage $T^{-1}$(Z) $\subseteq$ V is a linear subspace.

- **Null Space** of T: null(T) = $T^{-1}$({0}). It is the set of all vectors that when the matrix T times any of these vectors equal 0. Also called the **kernel**, denoted ker(T). If T is a linear transformation of $\mathbb{R}^n$, then the null space/kernel Ker(T) is the set of all vectors X such that T(X) = 0.

- **Image** of T: T(V) is the **image** of T.

- **Rank** of T: is dim(im(T)).

- **Nullity** of T: is dim(null(T)).

- [**Rank-Nullity Theorem**](https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem): it holds that dim(V) = dim(im(T)) + dim(ker(T)).

- A **bijective linear map** is in fact an *isomorphism* ("structure-preserving", where here the structure is linearity). From that follows that T: V -> V is bijective, then

    1) T is injective.
    
    2) ker(T) = {0}.
    
    3) dim(im(T)) = dim(V).
    
    4) T is surjective.
    
    5) T(V) = V.
    
- **Theorem**: fix $v_0$ $\in$ V such that T($v_0$) = w. Any solution to the equation $T_v$ = w is of the form v = $v_0$ + u with u $\in$ ker(T). We call $v_0$ a **particular** solution and then say that all other solutions v to T(v) = w must be the sum of the particular solution $v_0$ and some other homogenous solution u.

##### Coordinate Representations

- **Choice of Basis**: a vector can be **uniquely** represented on a given basis $\implies$ unique choice of basis. For ex: for $\mathbb{R}^2$, {(1,0),(0,1)} vs {$(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2}),(-\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$}. 

- **Proposition**: for any finite-dimensional vector space V and basis ($v_i$)$_{i≤n}$, there exists a linear bijection ("isomorphism") $\Phi_{(v)}$:$\mathbb{R}^n$ -> V given by ($a_1,...,a_n$) $\mapsto$ $\sum_{i ≤ n} a_i v_i$.

- Given two bases (v) and (v'), we can map between coordinates in the two bases; i.e., do a "change of" by composing these isomorphisms:

    1) Fix u $\in$ V, and let $a_B$ (u) be u's coordinate representation in basis B.
    
    2) $a_{(v')}(u) = \Phi^{-1}_{v}\circ \Phi_{(v)}(a_{(v)}(u))$
    
- **Proposition**: the set L(V,W) of linear maps from V to W is also a vector space, with addition and scalar multiplication defined by ($\alpha \circ T + S)(v) = \alpha T(v) + S(v)$.

- If two linear maps **coincide on the basis**, i.e. T$(v_i$) = T'($v_i$) $\forall$ $v_i$ $\in$ (v). Then, T = T'. 

##### Matrices

- [**Matrix**](https://en.wikipedia.org/wiki/Matrix_(mathematics)): is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. For ex:

$\mathbf{X} = \left[\begin{array}{rrr} 1 & 2 & 3 \\ 3 & 1 & 2 \\ 2 & 3 & 1 \end{array}\right]$

- **Diagonal entries**: are the entries $a_{ij} \in \mathbb{R}$.

- **Column vector**: is an m x 1 matrix.

- **Row vector**: is an n x 1 matrix.

- Matrix operations are defined so as to coincide with operations between linear maps
    
    1) Matrix addition $\iff$ linear map addition.

    2) Matrix multiplication by vectors ↔ linear map application.

    3) Matrix multiplication by matrices ↔ linear map composition.

    4) Matrix inversion ↔ linear map inversion.

- [**Matrix addition**](https://en.wikipedia.org/wiki/Matrix_addition): of A and B is defined when A is m × n and B is m × n. The sum is then defined by (A + B)$_{ij}$ = $a_{ij} + b_{ij}$ and AB is an m x n matrix.

- [**Matrix multiplication**](https://en.wikipedia.org/wiki/Matrix_multiplication): is a binary operation that produces a matrix from two matrices with entries in a field, or, more generally, in a ring or even a semiring. The matrix product is designed for representing the composition of linear maps that are represented by matrices. So, the **matrix product** of A and B is defined when A is m x n and B is n x p. The product is then defined by (AB)$_{ij}$ = $\sum_{k = 1}^{n} a_{ik}b_{kj}$ and AB is an m x p matrix.

- The identity map on $\mathbb{R}^n$ is given by the identity matrix, defined as 

$\mathbf{I_n} = \left[\begin{array}{rrr} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right]$

- A matrix is **invertible** if there exists an inverse matrix such that AA$^{-1}$ = $A^{-1}A$ = $I_n$.

- **Singular/Degenerate**: matrices that are noninvertible.

- [**Upper triangular**](https://en.wikipedia.org/wiki/Triangular_matrix): all elements below the diagonal are zero. 

$\left[\begin{array}{rrr} 1 & 4 & 3 \\ 0 & 2 & 5 \\ 0 & 0 & 2 \end{array}\right]$

- **Transpose**: All elements in A' (or $A^T$) are flipped along the diagonal, e.g. $\left[\begin{array}{rrr} 1 & 2 & 5 \\ 0 & 4 & 3\end{array}\right]$' = $\mathbf{X} = \left[\begin{array}{rrr} 1 & 0 \\ 2 & 4 \\ 5 & 3 \end{array}\right]$

- **Symmetric matrix**: if A' = A.
  
#### 5th Lesson{.tabset}

##### Complex Vector Spaces

- Recall complex numbers: a + bi $\in$ i = $\sqrt{-1}$.

1) Addition: (a + bi) + (c + di) = (a + b) + (c + d)i **Is this wrong?**

2) Multiplication: (a + bi)·(c + di) = ac + adi + bci + bdi$^2$ = (ac − bd) + (ad + bc)i
  
- **Scalar Multiplication for vector spaces as a map**:

- $\bullet$ : $\mathbb{R}$ x V -> V 

- Allow for vector spaces "over $\mathbb{C}$": $\mathbb{C}$ x V -> V

- [**Fundamental Theorem of Algebra**](https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra): every polynomial p(x) = $a_0$ + $a_1 x^1$ + ... + $a_n x^n$ with coefficients $a_i$ $\in$ $\mathbb{C}$ (or in $\mathbb{R}$ $\subset$ $\mathbb{C}$) can be uniquely factored in $\mathbb{C}$ i.e. there exist unique $ß_0,...,ß_n$ $\in$ $\mathbb{C}$ s.t. p(x) = $ß_0(x - ß_1)...(x - ß_n)$.

##### Eigen-stuff

- [**Eigenvector**](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)(**characteristic vector**): of a linear transformation is a non-zero vector that changes only by a scalar factor when that linear transformation is applied to it. More formally, if *T* is a linear transformation from a vector space *V* over a field *F* into itself and **v** is a vector in *V* that is not the zero vector, then **v** is an eigenvector of *T* if *T*(**v**) is a scalar multiple of **v**. This condition can be written as the equation T(v) = $\lambda$v, where $\lambda$ is a scalar in the field *F*, known as the **eigenvalue**(**characteristic value**/**characteristic root**) associated with the eigenvector **v**.

- If $v_1,...,v_n$ are eigenvectors with distinct eigenvalues $\lambda_1,...,\lambda_n$, then $v_1,...,v_m$ are linearly independent.

- Corollaries:

    1) A linear operator T on V has at most n $\equiv$ dim(V ) distinct eigenvalues.
    
    2) If T has n distinct eigenvalues, V has a basis (v) consisting of all eigenvectors, or an **eigenbasis**.
    
- **Diagonalizability**: An n x n matrix is diagonalizable if, for some change of basis matrix U and some diagonal matrix D, A = UDU$^2$. Considering (**v**) is an eigenbasis for *V* with respect to *T* $\iff$ T's matrix representation *A* in the basis (**v**) is diagonal. Diagonal elements are the eigenvalues: $A_{ii}$ = $\lambda{i}$. So A is diagonalizable $\iff$ it corresponds to some T admitting an eigenbasis.

- **Diagonal Matrices**: they just take the different components of
any vector and multiply each one by some constant. There are two cases:

    1) Eigenvalues $\in$ $\mathbb{R}$. In the eigenbasis, T just scales up or down each component separately.
    
    2) Eigenvalues $\in$ $\mathbb{C}$. In the eigenbasis, T scales up or down each component separately **and** T rotates the components.
    
- Every operator on a complex vector space has at least one eigenvalue.

- Every operator on a complex vector space has an upper triangular matrix
representation in some basis.

- **Proposition**: an upper-triangular matrix A is invertible iff all diagonal elements $A_{ii}$ are non-zero.

- **Corollary**: all diagonal elements of an upper-triangular matrix are eigenvalues of the corresponding operator.

##### Trace and Determinant

- [**Trace**](https://en.wikipedia.org/wiki/Trace_(linear_algebra)): in an *n x n* matrix is the sum of its diagonal elements: 	tr(A) = $\sum_{i = 1}^{n} A_{ii}$. Also, if *A* and *B* are *m x n* and *n x m* matrices, then tr(AB) = tr (BA). Trace is **basis-invariant**.

- [**Determinant**](https://en.wikipedia.org/wiki/Determinant): is a scalar value that can be computed from the elements of a square matrix and encodes certain properties of the linear transformation described by the matrix. For (k + 1) × (k + 1) matrices, det(A) = $\sum_{j = 1}^{k + 1} (-1)^{i+j}A_{ij} \bullet det(A_{-i,-j})$. Some properties of determinants:

    1) det($v_1,...,v_n$) is linear in each argument.
    
    2) If $v_1,...,v_n$ and $u_1,...,u_n$ are same except for column swap - i.e. for some i ≠ j $u_i$ = $v_j$, $u_j$ = $v_i$ - then det($v_1,...,v_n$) = -det($u_1,...,u_n$).
    
    3) A and B are *n x n* matrices. Then det(AB) = det(A)det(B).
    
    4) The determinant is basis-invariant.
    
##### Algorithm for Diagonalization

1) Find eigenvalues.

    a) Compute $p_A(\lambda)$.
    
    b) Factor $p_a(\lambda)$.
    
2) Find eigenvectors.

    a) For each $\lambda_i$, solve the equation (A - $\lambda_i I$)$v_i$ = 0.
    
    b) If all $\lambda$s are distinct, you will find *n* LI $v_is$.
    
    c) If not, you may not.
    
3) (If n LI $v_is$) Diagonalize A

    a) Want to write A = UDU$^{-1}$, for D diagonal, U change of basis.
    
    b) Take $(v_i)^{n}_{i=1}$ as new basis.
    
    c) D is diagonal with entries $D_{ii} = \lambda{i}$.
    
    d) $i^{th}$ column of *U* is $v_i's$ representation in your original basis.

##### Review

- Eigenstuff

    1) What is an eigenvector?

    2) What can we say if some operator has dim(V) distinct eigenvalues?

    3) What can we say more generally?
    
    4) What is the relationship between eigenvalues and invertibility?

- Trace

    1) What is it equal to (in any basis)?

- Determinant

    1) What is it equal to (in any basis)?

    2) How can we use it to test for invertibility? Why?

- Diagonalization

    1) What’s the first step?

    2) What’s the second step?

    3) What’s the third step?


#### 6th Lesson{.tabset}

##### Inner Product Spaces

- **Relevance**: If an equation (Ax = b) doesn't have a solution, the answer is to pick the "closest" Ax to b. Classic problem in statistics: given data points $y_1,...,y_n$ and $X$ $\in$ $\mathbb{R}^{N x n}$, what is the "best" $ß$ $\in$ $\mathbb{R}^n$ such that y is close to Xß? To answer that, we need a **measure of distance** between two vectors.

- [**Normed Space**](https://en.wikipedia.org/wiki/Normed_vector_space): is a vector space on which a **norm** is defined. A norm is the formalization and the generalization to real vector spaces of the intuitive notion of distance in the real world. A normed vector space is a pair (V, ||$\circ$||) where V is a vector space and ||$\circ$|| a norm on V. The norm ||$\circ$|| induces a metric (notion of distance) and therefore a topology on V. This metric is defined the usual way (d(u,v) = ||u-v||).

- A norm is a real-valued function defined on the vector space that has the following properties:

    1) The zero vector, **0**, has zero length; every other vector has positive length. ||x|| ≥ 0, and ||x|| = 0 if and only if x = 0.
    
    2) Multiplying a vector by a positive number changes its length without changing its direction. Moreover, ||$\alpha$x|| = ||$\alpha$|| ||x|| for any scalar $\apha$.
    
    3) The triangle inequality holds. That is, taking norms as distances, the distance from point A through B to C is never shorter than going directly from A to C, or the shortest distance between any two points is a straight line. So, ||x + y|| ≤ ||x|| + ||y|| for any vectors x and y (triangle inequality).

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("7.png")
 grid.raster(img)
```

- The generalization of these three properties to more abstract vector spaces leads to the notion of norm. A vector space on which a norm is defined is then called a normed space or normed vector space. So, this generates a metric space (V,d), where d(u,v) = |u - v|. Some examples of normed spaces:

    1) In $\mathbb{R}^n$, the usual Euclidean norm: ||x|| = $\sqrt{x_1^2 + ... + x_n^2}.
    
    2) In C([a,b],$\mathbb{R}$), the $L^2$ norm: ||f||$_2$ = $\sqrt{	$\int_{a}^{b}f(x)^2dx$}.
    
    3) C([a,b],$\mathbb{R}$) endowed with the sup norm: $||f||_\infty = sup_{x \in[a,b]}|f(x)|$, which we call the L$^\infty$ norm.
    
- [**Banach Space**](https://en.wikipedia.org/wiki/Banach_space): it is a **complete** normed space. A Banach Space is a vector space with a metric that allows the computation of vector length and distance between vectors and is complete in the sense that a Cauchy sequence of vectors always converges to a well defined limit that is within the space. Then, for every Cauchy sequence{$x_n$} in X, there exists an element *x* in *X* such that $lim_{n -> \infty}x_n = x$. Propositions:

    1) In a normed linear space, any finite-dimensional subspace is complete.
    
    2) For all p ≥ 1, both $l_P$ and $L_P$ are Banach spaces.

- So far we only have the magnitude of vectors, but not their directions. So, fix a vector space V.

- **Axioms for an Inner Product**: an (real-valued) inner product is a function $\langle \circ \rangle$: V x V -> $\mathbb{R}$ such that

    1) Positive definiteness: $\langle v,v \rangle$ ≥ 0 and $\langle v,v \rangle$ = 0 iff v = 0.
    
    2) Bilinearity: $\langle v,w \rangle$ is linear in each argument separately.
    
    3) Symmetry: $\langle v,w \rangle$ = $\langle v,w \rangle$.
    
- We call V an [**inner product space**](https://en.wikipedia.org/wiki/Inner_product_space). It is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The most common inner product space is the "dot product", V = $\mathbb{R}^n$ with standard inner product: $\langle x,y \rangle$ = $\sum_*{i = 1}^{n} x_i y_i$ for x,y $\in$ $\mathbb{R}^n$.  (Expected utility interpretation: if x = $\pi$, y = u, get $\sum_{i} \pi_i u_i \equiv \mathbb{E}[u]$. Here are some [**examples**](https://en.wikipedia.org/wiki/Inner_product_space#Examples).

##### Orthogonality

- [**Orthogonality**](https://en.wikipedia.org/wiki/Orthogonality): is the generalization of the notion of perpendicularity to the linear algebra of bilinear forms. Two elements u and v of a vector space with bilinear form B are orthogonal when B(u, v) = 0. Depending on the bilinear form, the vector space may contain nonzero self-orthogonal vectors. In the case of function spaces, families of orthogonal functions are used to form a basis. So, two vectors *v,w* are orthogonal if $\langle v,w \rangle$ = 0.

- **Orthogonality Theorem**: if nonzero vectors $v_1,...,v_n$ are (mutually) orthogonal, they are linearly independent.

- **Pythagorean Theorem**: if u,v $\in$ V are orthogonal, then ||u + v||$^2$  = ||u||$^2$ + ||v||$^2$.

- Interpreting $\langle v,w \rangle$ as some kind of "projection" of v on w(or vice-versa), we might suspect that $\langle v,w \rangle$ is biggest when v and w are parallel, i.e. w = $\lambda$v. In that case, $\langle v,w \rangle$ = $\lambda$$\langle v,v \rangle$ = $\lambda$||v||$^2$ = ||w|| ||v||.

- [**Cauchy-Schwartz Inequality**](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality): states that for all vectors *u* and *v* of an inner product space it is true that |$\langle u,v \rangle$$^2$ ≤ $\langle u,u \rangle$ $\circ$ $\langle v,v \rangle$, where $\langle .,. \rangle$ is the inner product.

- **Continuous Proposition**: the inner product function $\langle \circ,\circ \rangle$ : V x V -> $\mathbb{R}$ is, in both arguments, continuous in the metric space (V,d) with metric d(u,v) = ||u - v|| = $\sqrt{u-v,u-v}$

- [**Orthonormal Basis**](https://en.wikipedia.org/wiki/Orthonormal_basis): for an inner product space V with finite dimension, an orthonormal basis  is a basis for V whose vectors are orthonormal, that is, they are all unit vectors and orthogonal to each other. So, a basis **v** is an **orthonormal basis** of V if: *1)* all vectors in the basis are mutually orthogonal: $\langle v_i,v_j \rangle$ = 0 for all i ≠ j; *2)* all vectors in the basis have norm 1: ||$v_i$|| = 1 for all *i*. Why is this kind of basis useful? Because all vectors have a simple coordinate representation! 

- [**Parseval's Identity**](https://en.wikipedia.org/wiki/Parseval%27s_identity): let $e_1,...,e_n$ be an orthonormal basis of V. Then for any v, v = $\langle v,e_1 \rangle e_1 + ... + \langle v,e_n \rangle e_n$ and ||v||$^2$ = $\sum_{i = 1}^{n} |\langle v,e_i \rangle|^2$.

- [**Gram-Schmidt Orthonormalization Theorem**](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process): shows us that we can always find an orthonormal basis: given a basis $v_1,...,v_n$ we can find an orthonormal basis $\overrightarrow{\rm e_1}$,...,$\overrightarrow{\rm e_k}$ is the same as the span of $v_1,...,v_k$ for all k = 1,...,n. 

- [**Orthogonal Matrices**](https://en.wikipedia.org/wiki/Orthogonal_matrix):  is a square matrix whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors), i.e.: Q$^T$Q = QQ$^T$ = I. This leads to: Q$^T$ = Q$^{-1}$. 

- [**Orthogonal Complement**](https://en.wikipedia.org/wiki/Orthogonal_complement): of the subspace U is the subspace $U^\perp$ defined by: $U^\perp$ = {v $\in$ V: $\langle v,u \rangle$ = 0 $\forall$ u $\in$ U}. Some examples:

    1) {0}$^{\perp}$ = V.
    
    2) V$^{\perp}$ = 0.
    
    3) In $\mathbb{R}^3$, span ((0,0,1))$^{\perp}$ is the x-y plane.
    
- [**Orthogonal Projection**](https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Orthogonal_projection): a projection is a linear transformation *P* from a vector space to itself such that $P^{2}=P$. That is, whenever *P* is applied twice to any value, it gives the same result as if it were applied once([**idempotent**](https://en.wikipedia.org/wiki/Idempotence)). It leaves its image unchanged. So, the distance ||u - v|| of v $\in$ V to u $\in$ U is (uniquely) minimized across all u by u = P$_U$(v).

- [**Hilbert's Projection Theorem**](https://en.wikipedia.org/wiki/Hilbert_projection_theorem): is a famous result of convex analysis that says that for every point *x* in a Hilbert space *H* and every nonempty closed convex C $\subset$ H, there exists a unique point *y* in *C* for which ||x-y|| is minimized over *C*. So, to solve "Ab = y if y $\notin$ image(A)", we should take the vector *b* which minimizes the distance ||Ab - y||, and the theorem says the (unique) *u* which minimizes ||u - y|| over all *u* $\in$ image(A) is u* = P$_{image(A)}$(y).

- [**Spectral Theorem**](https://en.wikipedia.org/wiki/Spectral_theorem): is a result about when a linear operator or matrix can be diagonalized (that is, represented as a diagonal matrix in some basis). This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix. So, suppose *A* is a (real) symmetric matrix i.e. s.t. A' = A. Then, A admits a **basis of orthonormal eigenvectors**, giving A = Q$\Lambda$Q, with Q being the orthogonal matrix of (real) eigenvectors, and $\Lambda$ being the diagonal matrix with (real) eigenvalues. Two remarks: **1)** This means that A is diagonalizable, because Q$^{-1}$ = Q'; **2)**  Note that A = X'X is always symmetric.

##### Definiteness

- [**Definiteness of a Matrix**](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix): a symmetric *n x n* matrix *M* is called **positive definite** if the scalar $z^TMz$ is strictly positive for every non-zero column vector z of *n* real numbers. Here $z^T$ denotes the transpose of z. When interpreting $Mz$ as the output of an operator, $M$, that is acting on an input, *z*, the property of positive definiteness implies that the output always has a positive inner product with the input, as often observed in physical processes. 

- So, a symmetric matrix *A* is called **positive definite** if $\langle x,Ax \rangle$ > 0 $\forall$ nonzero x $\in$ $\mathbb{R}^n$. We write A > 0. A matrix A is positive **semi-definite** if $\langle x,Ax \rangle$ ≥ 0 $\forall$ nonzero x $\in$ $\mathbb{R}^n$. We write A ≥ 0.

- Likewise, a symmetric matrix *A* is called **negative definite** if $\langle x,Ax \rangle$ < 0 $\forall$ nonzero x $\in$ $\mathbb{R}^n$. We write A < 0. A matrix A is negative **semi-definite** if $\langle x,Ax \rangle$ ≤ 0 $\forall$ nonzero x $\in$ $\mathbb{R}^n$. We write A ≤ 0.

- **Theorem**: suppose that A $\in$ $\mathbb{R}^{n x n}$ is symmetric. The followings are equivalent: **1)** A is positive definite; **2)** Every eigenvalue of A is positive.

- Properties of Definite Matrices:

    1) A, B > 0 $\implies$ A + B > 0

    2) A > 0 $\implies$ A invertible.
    
    3) A > 0 $\iff$ A' > 0.
    
    4) $\forall A : A'A is positive semi-definite.
    
    5) A > 0 $\iff$ $\langle x,y \rangle$ $\equiv$ x'Ay defines an inner product on $\mathbb{R}^n$. 
    
- **Theorem**: Let A be a real m × n matrix. Then there exist: **1)** an orthogonal m × m matrix R; **2)** a diagonal m × n matrix $\Lambda$; **3)** an orthogonal n × n matrix Q; such that A = R$\Lambda$Q'. Interpretation:  All linear maps (including non-square) consist only of: **1)**scaling in by different amounts in different orthogonal directions; **2)** rotation.

##### Dual Spaces

- [**Dual Spaces**](https://en.wikipedia.org/wiki/Dual_space): any vector space V has a corresponding dual vector space consisting of all linear functionals on V, together with the vector space structure of pointwise addition and scalar multiplication by constants. So, the **dual space** of V (write V$\ast$) is the space of all linear functions ("functionals") $\Phi$ : V -> $\mathbb{R}$. This V$\ast$ is a particularly useful notation. When V is a commodity space, its dual V$\ast$ corresponds to the space of
all “price functionals”: real-valued, linear functions that return the cost
of any consumption bundle x $\in$ V given a price vector p, i.e., $\sum_{i} p_i x_i$.

- [**Hyperplane**](https://en.wikipedia.org/wiki/Hyperplane): is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes. This notion can be used in any general space in which the concept of the dimension of a subspace is defined. So, a **hyperplane** H is a set H = {v $\in$ V : $\Phi$(v) = c} where some constant c and a linear function $\phi$($\circ$) $\in$ V$\ast$.

- [**Hans-Banach Theorem**](https://en.wikipedia.org/wiki/Hahn%E2%80%93Banach_theorem): is a central tool in functional analysis. It allows the extension of bounded linear functionals defined on a subspace of some vector space to the whole space, and it also shows that there are "enough" continuous linear functionals defined on every normed vector space to make the study of the dual space "interesting". So, suppose K $\subset$ $\mathbb{R}^n$ is convex. For any $v_0 \notin \overline{K}$, $\exists \lambda \in \mathbb{R}^n s.t. \langle \lambda , v_0 \rangle$ > $\langle \lambda, k \rangle$ for all k $\in \overline{K}$.

- [**Hyperplane Separation Theorem**](https://en.wikipedia.org/wiki/Hyperplane_separation_theorem): s a theorem about disjoint convex sets in n-dimensional Euclidean space. There are several rather similar versions. In one version of the theorem, if both these sets are closed and at least one of them is compact, then there is a hyperplane in between them and even two parallel hyperplanes in between them separated by a gap. In another version, if both disjoint convex sets are open, then there is a hyperplane in between them, but not necessarily any gap. An axis which is orthogonal to a separating hyperplane is a separating axis, because the orthogonal projections of the convex bodies onto the axis are disjoint. So, suppose that $K_1, K_2 \subset \mathbb{R}^n$ are convex and disjoint. Then $\exists \lambda \in \mathbb{R}^n$ s.t. $\langle \lambda,k_1 \rangle$ ≥ $\langle \lambda, k_2 \rangle$ $\forall k_1 \in K_1, k_2 \in K_2$.


#### 7th Lesson{.tabset}

##### Convexity

- [**Convex Optimization**](https://en.wikipedia.org/wiki/Convex_optimization): is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. Many classes of convex optimization problems admit polynomial-time algorithms. A convex optimization problem is an optimization problem in which the objective function is a convex function and the feasible set is a convex set. A function *f* mapping some subset of $\mathbb{R}^n$ into $\mathbb{R}$ $A \cup B$ {±$\infty$} is convex if its domain is convex for all $\theta$ $\in$ [0,1] and all *x,y* in its domain *f*($\theta$x + (1 - $\theta$)y) ≤ $\theta$*f*(x) + (1 - $\theta$)*f(y)*; a set is convex if for all members *x,y* and all $\theta$ $\in$ [0,1]., $\theta$ + (1 - $\theta$)y is also in the set. 

- [**Concave**](https://en.wikipedia.org/wiki/Concave_function): let X $\subset$ $\mathbb{R}^n$ convex and consider a function f: X -> $\mathbb{R}$. $f$ is **concave** if f($\alpha$x + (1 - $\alpha$)x') ≥ $\alpha$f(x) + (1 - $\alpha$)f(x') for all x,x' $\in$ X and all $\alpha$ $\in$ [0,1]. If inequality strict for all $x' ≠ x$ and all $\alpha$ $\in$ (0,1), then *f* is **strictly concave**. Reverse it and have the same for **convex**.

- A few properties:

    1) f concave $\iff$ -f convex
    
    2) $f_1,...,f_n$ concave $\iff$ min{$f_1,...,f_n$} concave.
    
    3) $f_1,...,f_n$ convex $\implies$ max($f_1,...,f_n$) convex.
    
    4) f,g concave(convex) $\implies$ f + g concave(convex).
    
    5) f concave $\implies$ {(x,y) $\in$ X x $\mathbb{R}$: f(x) ≥ y} is convex.

    6) f convex $\implies$ {(x,y) $\in$ X x $\mathbb{R}$: f(x) ≤ y} is concave.
    
- **FOC Characterization Theorem**:

    1) f concave $\iff$ f(x) ≤ f(x') + $f_x(x') \circ$ (x - x') $\forall x,x' \in$ X.

    2) f strictly concave $\iff$ f(x) < f(x') + $f_x(x') \circ$ (x - x') $\forall x,x' \in$ X, x ≠ x'.

    3) f convex $\iff$ f(x) ≥ f(x') + $f_x(x') \circ$ (x - x') $\forall x,x' \in$ X.

    4) f strictly convex $\iff$ f(x) > f(x') + $f_x(x') \circ$ (x - x') $\forall x,x' \in$ X, x ≠ x'.

- **Theorem**: suppose *f* is twice continuously differentiable with Hessian $H_f$ : X -> $\mathbb{R}^{n x n}$. Then f is..

    1) concave $\iff$ $H_f$(x) negative semidefinite $\forall$x $\in$ X

    2) strictly concave $\Longleftarrow$ $H_f$(x) negative definite $\forall$x $\in$ X

    3) convex $\iff$ $H_f$(x) positive semidefinite $\forall$x $\in$ X

    4) strictly convex $\Longleftarrow$ $H_f$(x) positive definite $\forall$x $\in$ X

- [**Quasiconvexity**](https://en.wikipedia.org/wiki/Quasiconvex_function): is a real-valued function defined on an interval or on a convex subset of a real vector space such that the inverse image of any set of the form ($\infty,a$) is a convex set. For a function of a single variable, along any stretch of the curve the highest point is one of the endpoints. The negative of a quasiconvex function is said to be quasiconcave. All convex functions are also quasiconvex, but not all quasiconvex functions are convex, so quasiconvexity is a generalization of convexity. Quasiconvexity and quasiconcavity extend to functions with multiple arguments the notion of unimodality of functions with a single real argument. Can think of **quasiconcavity** as single-peakedness: if there is a max, *f* needs to be monotone on both sides of it.

- **Quasiconvex**: f is **quasiconvex** if {x $\in$ X : f(x) ≤ a} is a convex set for any a $\in$ $\mathbb{R}$. So, $\forall x,x' \in X and \forall \alpha \in (0,1),$ f($\alpha$x + (1 - $\alpha$)x') ≤ max{f(x),f(x')}.

- **Quasiconcave**: f is **quasiconcave** if *-f* i.e., if{x $\in$ X : -f(x) ≤ a} is a convex set for any a $\in$ $\mathbb{R}$. So, $\forall x,x' \in X and \forall \alpha \in (0,1),$ f($\alpha$x + (1 - $\alpha$)x') ≥ min{f(x),f(x')}.

##### Unconstrained optimization

- Four distinct questions we need to answer:

    1) What conditions are **Necessary**?
    
    2) What conditions are **Sufficient**?
    
    3) What conditions are **Globl**?
    
    4) What conditions are **Local**?

- [**Interior Local Optimality**](https://en.wikipedia.org/wiki/Local_optimum): x$\ast \in$ X is a **local maximizer** of f, and f(x$\ast$) a **local maximum**, if there exists an open set around *U* around x$\ast$ such that f(x$\ast$) ≥ f(x) for all x $\in$ *U* $\cap$ X. If x$\ast$ $\in$ $ \mathring{X}$, call it an "interior local maximizer", and f(x$\ast$) an "interior local maximum".

- If f$_x$(x) = \overrightarrow{\rm 0}' then x is a **critical point of* f. 
- **Necessary condition for interior local optimality**: Suppose f is differentiable. Then any interior local maximizer x$\ast$ of *f* is also a critical point of *f*, i.e., f$_x$ (x$\ast$) = \overrightarrow{\rm 0}'.

- **Necessary condition for global max**: let *f* be defined on a closed set X. If sup$_{x \in \partial X}$ f(x) ≤ f(x$_0$) for some $x_0$ $\in$ $\mathring{X}$, then any global maximizer x$\ast$ is also an interior local maximizer, and therefore also critical point of *f*.

- **Characterizing local optima**: Often we assume *f* is twice differentiable. In that case, we can generate another necessary condition, and a sufficient condition for local maximum: 

- **Second-order conditions for local maximizers Theorem**: suppose *f* $\in$ C$^2$ with Hessian $H_f$. Then: **1)** x$\ast$ $\in$ $\mathring{X}$ $\subset \mathbb{R}^n$ local maximizer $\implies$ f$_x$(x$\ast$) = $\overrightarrow{\rm 0}'$ and $H_f$(x$\ast$) ≤ 0. **2)** $f_x(x\ast)$ = $\overrightarrow{\rm 0}'$ and $H_f(x \ast)$ < 0 $\implies$ x$\ast$ $\in$ $\mathbb{R}^n$ local maximizer.

- Strategy for global maximizers:

    1) Take a global property of *f* that implies (local maxes) $\subset$ (global maxes).
    
    2) Use local conditions to find a local max -> global max.
    
- **Sufficient Condition for Global Max Theorem**:

    1) If f is strictly quasiconcave and X is convex, then $\exists$ at most one global maximizer.

    2) If f is strictly concave and X is convex, then if x$\ast$ is a critical point, it is the unique global argmax.

Summary:

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("8.png")
 grid.raster(img)
```

- Finding the Global Maximizer: suppose you have f : X -> $\mathbb{R}$, X $\subset$ $\mathbb{R}^n$ convex.

    1) Check for concavity(by checking $H_f$ ≤ 0). If yes:
    
    1.1) Search for a critical point $x\ast$. If it exists, we **have** a global max.
    
    1.2) If no critical points: a) If X compact, a boundary max exists. Use constrained optimization. b) else, possible no max exists.
    
    2) If no concavity: Try to show $\exists$ global maximum (e.g., is X compact?)

    3) If global max exists: Check local conditions
    
      3.1) Candidate max type I: x$\ast$ with $f_x$(x$\ast$) = \overrightarrow{\rm 0}' and $H_f$(x$\ast$) neg semi-def.
      
      3.2) Candidate max type II: Boundary points x$\ast$ (rule out the boundary or do constrained optimization on the boundary)
      
      3.3) Compute all such x$\ast$ and compare their function values f (x$\ast$) by hand.
      
##### Constrained optimization

- By adding constraints, we solve $max_{x \in U}$ f(x) where U $\subset$ X has a lower dimension than X(e.g. U = $\partial$X...). To start, U consists only of [**equality**](https://en.wikipedia.org/wiki/Equality_(mathematics)) [**constraints**](https://en.wikipedia.org/wiki/Constraint_(mathematics)).

- From consumer theory: max u(x), p $\circ$ x = w. 3 ways to solve this:

    1) Classic variational intuition. If x$\ast$ is an optimum, small feasible deviations should not lead to an improvement. More generally, taking small deviation dx, as long as p $\circ$ d$_x$ = 0, we should have $u_x \circ d_x$ = 0. So $u_x$ and p should be **parallel**, there should be a $\lambda \in \mathbb{R}$ such that $u_x = \lambda p$.
    
    2) Usual lagrangian machinery. Suppose one solves: $max_x$ [u(x) + $\lambda$(w - p $\circ$ x)], where we could imagine that the Lagrange multiplier $\lambda \in \mathbb{R}$ punishes deviations from the constraint: a) $\lambda$ < 0 positive deviations are punished; b) $\lambda$ > 0 negative deviations are punished; c) $\lambda$ = 0 no punishment. The usual maxim: find the $\lambda$ so that the optimum satisfies the constraint.
    
    3) Mathematically precise application of Lagrangian sufficiency and necessity. 
    
- [**Lagrangian**](https://en.wikipedia.org/wiki/Lagrange_multiplier): the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints (i.e., subject to the condition that one or more equations have to be satisfied exactly by the chosen values of the variables). The basic idea is to convert a constrained problem into a form such that the derivative test of an unconstrained problem can still be applied. Once stationary points have been identified from the first-order necessary conditions, the definiteness of the bordered Hessian matrix determines whether those points are maxima, minima, or saddle points. The **Lagrange multiplier theorem** roughly states that at any stationary point of the function that also satisfies the equality constraints, the gradient of the function at that point can be expressed as a linear combination of the gradients of the constraints at that point, with the Lagrange multipliers acting as coefficients.
    
- **Existence of Lagrange Multiplies Theorem**: suppose that f($\circ$) is concave, G($\circ$) is convex, and the set {x: G(x) = \overrightarrow{\rm 0}	} has an interior point. If x$\ast$ is a local constrained maximizer of the problem, then there exists $\lambda$ $\in$ $\mathbb{R}^n$ such that $f_x(x\ast)$ + $\lambda ' G_x(x\ast)$ = \overrightarrow{\rm 0}'.

- Equality constraints summary:

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("9.png")
 grid.raster(img)
```



#### 8th Lesson{.tabset}

##### Inequality Constrained Optimization

- [**Inequality-constrained optimization**](https://en.wikipedia.org/wiki/Constrained_optimization): is the constrain binding? We call an inequality constraint *j* **binding** at x $\in$ X if $H_j$(x) = 0.

    1) What we’re saying above is that non-binding inequality constraints all have no Lagrange multipliers of zero.

    2) Intuition: If they’re not binding, they’re not constraining our choices on the margin.
    
    3) Does that mean we’re getting the same solution as if dropped non-binding constaints?

- As it refers to “slack constraints”, µ'H(x) = 0 is called the **complementary slackness** condition.

- [**Global sufficiency**](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions): Allowing inequality constraints, the KKT approach to nonlinear programming generalizes the method of Lagrange multipliers, which allows only equality constraints. Similar to the Lagrange approach, the constrained maximization (minimization) problem is rewritten as a Lagrange function whose optimal point is a saddle point, i.e. a global maximum (minimum) over the domain of the choice variables and a global minimum (maximum) over the multipliers, which is why the Karush–Kuhn–Tucker theorem is sometimes referred to as the saddle-point theorem. So, let x$\ast$ $\in$ U, $\lambda$ $\in$ $\mathbb{R}^m$, $\mu$ $\in$ $\mathbb{R}_{≥0}^{k}$ s.t.

    1) satisfies $\mu ' H(x \ast)$ = 0 (complementary slackness)
    
    2) is critical, i.e. $L_x$(x$\ast$; $\lambda,\mu$) = \overrightarrow{\rm 0}'.
    
    3) L(-;$\lambda,\mu$) is (strictly) concave
    
- Then x$\ast$ is a (unique) global solution to (1).x

- [**Bordered Hessian**](https://en.wikipedia.org/wiki/Hessian_matrix#Bordered_Hessian): 

#### 9th Lesson {.tabset}

##### Differential Equations

- [**Ordinary Differential Equations**](https://en.wikipedia.org/wiki/Ordinary_differential_equation): is a differential equation containing one or more functions of one independent variable and the derivatives of those functions. These equations involve only derivatives with respect to a single dependent variable. There are four main issues to be tackled:

    1) Existence of a solution.
    
    2) Uniqueness of a solution (in a neighborhood or the entire globe)
    
    3) Technologies to obtain it
    
      3.1) explicit **closed-form** solution
      
      3.2) **graphical** exploration (e.g. phase diagrams)
      
      3.3) linearization, then closed form
      
      3.4) numerical solution (e.g. Matlab’s ode45 solver)

    4) Stability of a solution
    
- The **order** of an ODE is the order of its highest order derivative. We focus on **explicit ODEs** (those which can be solved for the highest-order term): $x^{(n)} = f(t,x,...,x^{n-1})$. An important trick is to convert an explicit ODE into a first-order vector ODE: 

```{r ,echo=FALSE}
library(png)
library(grid)
setwd("~/Dropbox/MIT/2019:2/Math Camp")
img <- readPNG("11.png")
 grid.raster(img)
```

- F is **linear** if there exists numbers $\alpha_i (t)$ $\in$ $\mathbb{R}$ such that: $\sum_{}^{} \alpha_i (t) x^{(i)}(t)$ = g(t).

- It is **(linear) homogeneous** if g(t) = 0. Any nonzero scaling of a solution generates a new solution.

- F is **autonomous** if it **does not depend on t**, in the sense that F(t,x,x',...,$x^{(n)}$) = F(x,x',...,$x^{(n)}$) = 0. E.g. the linear equation above is autonomous if neither $\alpha _i \in \mathbb{R}$ nor $g \in \mathbb{R}$ depend on t. Any solution to an autonomous equation will be independent of the time at which the initial conditions are applied.

##### Existence and Uniqueness

- Do all (explicit) differential equations have solutions? Intuitively, *seems like yes*—we can construct x(t + $\Delta$) from x(t) and $\mathring{x}$(t), iterate. But maybe this doesn't converge as $\Delta$ -> 0. How about $\mathring{x}$ = $x^2$ with x(0) = 1? $\implies$ x(t) = $\frac{1}{1 - t}$. Need to avoid things "blowing up".

- [**Lipschitz Continuity**](https://en.wikipedia.org/wiki/Lipschitz_continuity): is limited in how fast it can change: there exists a real number such that, for every pair of points on the graph of this function, the absolute value of the slope of the line connecting them is not greater than this real number; the smallest such bound is called the Lipschitz constant of the function (or modulus of uniform continuity). For instance, every function that has bounded first derivatives is Lipschitz. In the theory of differential equations, Lipschitz continuity is the central condition of the Picard–Lindelöf theorem which guarantees the existence and uniqueness of the solution to an initial value problem. A special type of Lipschitz continuity, called contraction, is used in the Banach fixed point theorem. So, given two metric spaces (X,$d_x$) and (Y,$d_y$), where $d_x$ denotes the metric on the set X and $d_y$ is the metric on set Y, a function *f* X -> Y is called **Lipschitz continuous** if there exists a real constant ≥ 0 s.t., $\forall$ $x_1$ and $x_2$ in X: $d_Y(f(x_1),f(x_2)) ≤ Kd_X(x_1,x_2)$. Any such K is referred to as a Lipschitz constant for the function f. The smallest constant is sometimes called the (best) Lipschitz constant; however, in most cases, the latter notion is less relevant. If K = 1 the function is called a short map, and if 0 ≤ K < 1 and f maps a metric space to itself, the function is called a contraction. 

- [**Picard-Lindelöf**](https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem): gives a set of conditions under which an initial value problem has a unique solution. Consider the initial value problem: y'(t) = f(t,y(t)), y($t_0$) = $y_0$. Suppose *f* is uniformly Lipschitz continuous in y (meaning the Lipschitz constant can be taken independent of t) and continuous in t, then for some value $\varepsilon$ > 0, there exists a unique solution y(t) to the initial value problem on the interval [$t_0 - \varepsilon, t_0 + \varepsilon$].

- **Global Uniqueness on Compact Sets**: suppose *f* is continuous in *t* on a closed interval t $\in$ [0,a] and K-Lipschitz continuous in x. If both x and $\hat{x}$ solve the ODE $\mathring{x}$(t) = f(t,x(t)), it holds that for t $\in$ [0,a]. ||x(t) - $\hat{x}(t)$||≤||x(0) -$\hat{x}$(0)||$e^{KTt$. So the rough idea of Lipschitz is that: $\implies |x(t + dt) - \hat{x}(t + dt)| ≤ | x(t) - \hat{x}| + K|x(t) - \hat{x}(t)|$. In particular, if x(0) = $\hat{x}(0)$, the solution is globally unique.

- **Global Existence Theorem**: suppose that within the interval [a,b], f is continuous in t and K-Lipschitz continuous in x. Then there exists a solution to the ODE $\mathring{x}$(t) = f(t,x(t)) with initial condition x(0) = $x_0$ $\in$ $\mathbb{R}$.

##### Solving Linear ODEs

- [**Spectral Theory of Ordinary Differential Equations**](https://en.wikipedia.org/wiki/Spectral_theory_of_ordinary_differential_equations): is the part of spectral theory concerned with the determination of the spectrum and eigenfunction expansion associated with a linear ordinary differential equation. So, consider the simplest case: $\mathring{x}$(t) = Ax(t); where x: [0,$\infty$) -> $\mathbb{R}^n$, and A $\in$ $\mathbb{R}^{n x n}$, and we impose boundary condition x(0) = $x_0$. We arrive at the **Theorem Linear ODE: spectral solution**: If A has n distinct real eigenvalues, $\lambda_1,...,\lambda_n$, then the unique solution to ($\ast$) takes the form: x(t) $\sum_{i = 1}^{n} c_i e^{\lambda_i t}v_i$ where $v_i$ is the eigenvector corresponding to the $i^{th}$ eigenvalue and {$c_i$}$_{i = 1}^n$ are some constraints of integration. This solution generalizes to non-diagonalizable A: $\mathring{x}$(t) = Ax(t), with boundary condition x(0) = $x_0$. This has solution: x = $e^{A_t}x_0$, where $e^B$ = $\sum_{n = 0}^{\infty} \frac{B^n}{n!}$.

##### Solving Nonlinear ODEs

- There are two important classes of equations for which we may obtain closed-form representations, with cleverness:

1) Exact equations

2) Separable equations

- Suppose an ODE takes the form: $F_t + F_x \mathring{x}(t)$ = 0, for some F: $\mathbb{R}$ x U -> $\mathbb{R}^n$. Here, $F_t$ $\in$ $\mathbb{R}^n$, and $F_x$ $\in$ $\mathbb{R}^{n x n}$ is the Jacobian of F with respect to x. In that case, the LHS is a derivative: $\frac{d}{dx}$ F(t,x(t)) = $F_t$ + $F_x \mathring{x}(t)$ = 0. But then, F(t,x(t)) = const $\in \mathbb{R}^n$. This gives (at least) an implicit characterization of x(t).

- [**Separable Equations**](https://www.khanacademy.org/math/ap-calculus-ab/ab-differential-equations-new/ab-7-6/v/separable-differential-equations-introduction): 

- [**Separation of Variables**](https://en.wikipedia.org/wiki/Separation_of_variables): is any of several methods for solving ordinary and partial differential equations, in which algebra allows one to rewrite an equation so that each of two variables occurs on a different side of the equation. Suppose a differential equation can be written in the form: $\frac{d}{dx}$ f(x) = g(x)h(f(x)); can be rearranged more simply be letting f(x) = y: $\frac{dy}{dx}$ = g(x)h(y). If y ≠ 0, we can separate both variables as: $\frac{dy}{h(y)}$ = g(x)dx.

##### Stability

- [**Global Asymptotic Stability**](https://en.wikipedia.org/wiki/Lyapunov_stability): if the solutions that start out near an equilibrium point $x_e$ stay near $x_e$ forever, than $x_e$ is**Lyapunov stable**. More strongly, if $x_e$ is Lyapunov stable and all solutions that start out near $x_{e}$ converge to $x_{e}$, then $x_{e}$ is asymptotically stable. The notion of exponential stability guarantees a minimal rate of decay, i.e., an estimate of how quickly the solutions converge. So, consider x(t), b $\in \mathbb{R}^n$ and A $\in$ $\mathbb{R}^{n x n}$. If $\mathring{x}(t)$ = Ax(t) + b and all of the eigenvalues of A have negative real parts, then there exists a steady state $x^{\ast}$ i.e. Ax$^{\ast}$ + b = 0 for which, starting from any $x_0$ $\in$ $\mathbb{R}^n$, the system converges: x(t) -> $x^{\ast}$ as t -> $\infty$. We call the system **globally asymptotically stable**.

- **Saddle Path Stability**: refers to dynamical systems, (usually systems of difference or differential equations), where the system has a fixed point, and there exists a single trajectory that leads to the fixed point. It follows that from a mathematical point of view these systems are in reality unstable. So, for **linear systems** consider x(t) $\in \mathbb{R}^n$ and A $\in \mathbb{R}^{n x n}$. If $\mathring{x}$(t) = Ax(t) + b and m ≤ n of the eigenvalues of A have negative real parts (and all are non-zero), then there exists a steady state $x^{\ast}$ and a m-dimensional subspace M $\subseteq$ $\mathbb{R}^m$ s.t. starting from any $x_0$ $\in$ M, the system converges: x(t) -> $x^{\ast}$ as t -> $\infty$. For **nonlinear systems**: if $\mathring{x}$(t) = G(x(t)) for some continuously differentiable G: $\mathbb{R}^n$ -> $\mathbb{R}$, then if m ≤ n eigenvalues of the Jacobian $G_x(x^{\ast}) \in \mathbb{R}^{n x n}$ are negative and $x^{\ast}$ is a steady state i.e. G($x^{\ast}$) = 0, then there exists an open neighborhood around $x^{\ast}$ and an m-dimensional manifold M $\subset$B$_\delta$($x^{\ast}$) s.t. x(t) -> $x^{\ast}$ starting from any $x_0 \in$ M.

##### Difference Equations

- [**Difference Equations**](https://en.wikipedia.org/wiki/Linear_difference_equation): sets equal to 0 a polynomial that is linear in the various iterates of a variable—that is, in the values of the elements of a sequence. The polynomial's linearity means that each of its terms has degree 0 or 1. Usually the context is the evolution of some variable over time, with the current time period or discrete moment in time denoted as t, one period earlier denoted as t − 1, one period later as t + 1, etc. So, consider the simplest difference equation: x(t + 1) = $\alpha$ + ßx(t). Anchoring at x(0) = $x_0$ $\in$ $\mathbb{R}$, with $\alpha,ß \in \mathbb{R}$. Inducting: x(1) = $\alpha$ + $ßx_0$; x(2) = $\alpha$ + ß($\alpha$ + ß$x_0$) = $\alpha + ß\alpha + ß^2 x_0$. We see that the general solution is x(t) = 1) if ß = 1, $x_0 + \alpha t$; 2) if ß ≠ 1, $ß^t$($x_0$ - $\frac{\alpha}{1 - ß}$) + $\frac{\alpha}{1 + ß}$.

- Stability for linear difference equations: for x(t+1) = Ax(t) + b, the steady state (equilibrium) A$x^{\ast}$ + b = $x^{\ast}$ is **globally asymptotically stable** if all of the eigenvalues of A live strictly inside the unit circle in the complex plane.

- [**Lag operator**](https://en.wikipedia.org/wiki/Lag_operator): In time series analysis, the lag operator (L) or backshift operator (B) operates on an element of a time series to produce the previous element. Differences between backwards and forwards operators?

#### 10th Lesson{.tabset}

##### Core Concepts

- [**Sample Size**](https://en.wikipedia.org/wiki/Sample_size_determination): start with a set $\Omega$.  This is the ambient sample space. Interpretations:

    1) The set of all potential outcomes of a stochastic “experiment”.
    
    2) The set of “states” of the world.

- We call a subset E $\subseteq$ $\Omega$ an **event**. We can think about events as (collections of) outcomes of a experiment. A point $\omega$ $\in$ $\Omega$ is a **realization**.

- [**Events**](https://en.wikipedia.org/wiki/Event_(probability_theory)):  is a set of outcomes of an experiment (a subset of the sample space) to which a probability is assigned. So, we will say that any set $\digamma$ of subsets of $\Omega$ is a well-defined **field of events** on $\Omega$ ("$\sigma$-algebra") if

    1) $\Omega$ $\in$ $\digamma$
    
    2) If E $\in$ $\digamma$, then $\Omega$\E $\in$ $\digamma$

    3) If each set $E_n$ in a countable collection ($E_n$)$_{n ≥ 1}$ lives in $\digamma$, so does $U_{n=1}^{\infty}$ $E_n$. In words, $\digamma$ is any set of subsets of $\Omega$ that contains $\Omega$ and is closed under complements and countable unions.

- [**Probability Measure**](https://en.wikipedia.org/wiki/Probability_measure): is a real-valued function defined on a set of events in a probability space that satisfies measure properties such as countable additivity. The difference between a probability measure and the more general notion of measure (which includes concepts like area or volume) is that a probability measure must assign value 1 to the entire probability space. Intuitively, the additivity property says that the probability assigned to the union of two disjoint events by the measure should be the sum of the probabilities of the events, e.g. the value assigned to "1 or 2" in a throw of a die should be the sum of the values assigned to "1" and "2". The requirements for a function $\mu$ to be a probability measure on a *probability space* are that:

    1) $\mu$ must return results in the unit interval [0,1], returning 0 for the empty set and 1 for the entire space.
    
    2) $\mu$ must satisfy the countable additivity property that for all countable collections {E$_i$} of pairwise disjoint sets: $\mu$ ($U_{i \in I} E_i$) = $\sum_{i \in I} \mu(E_i)$.

- So, a **probability measure** $\mathbb{P}$ on some field of events $\digamma$ on $\Omega$ is a function $\digamma$ -> [0,1], E $\mapsto$ $\mathbb{P}$(E) s.t.:

    1) $\mathbb{P}$($\Omega$) = 1.
    
    2) $\mathbb{P}$ is *countably additive*, in the sense that the union of any pairwise disjoint collection {$E_n$}$_{n≥0}$ has probability $\mathbb{P}$($\cup_n$E$_n$) = $\sum_n \mathbb{P}(E_n)$
    
- [**Lebesgue Measure**](https://en.wikipedia.org/wiki/Lebesgue_measure): is the standard way of assigning a measure to subsets of n-dimensional Euclidean space. For n = 1, 2, or 3, it coincides with the standard measure of length, area, or volume. In general, it is also called n-dimensional volume, n-volume, or simply volume. Given a subset E $\subseteq$ $\mathbb{R}$ with the length of interval I = [a,b] (or I = (a,b)) given by $\ell$(I) = b - a, the Lebesgue *outer measure* $\lambda^{\ast}$(E) is defined as $\lambda^{\ast}$(E) = inf{$\sum_{k = 1}^{\infty}\ell(I_k): (I_k)_{k\in \mathbb{N}}$ is a sequence of intervals with open boundaries with E $\subseteq$ $\cup_{k = 1}^{\infty}I_k$}.

- [**Borel Set**](https://en.wikipedia.org/wiki/Borel_set): is any set in a topological space that can be formed from open sets (or, equivalently, from closed sets) through the operations of countable union, countable intersection, and relative complement. Borel sets are important in measure theory, since any measure defined on the open sets of a space, or on the closed sets of a space, must also be defined on all Borel sets of that space. Any measure defined on the Borel sets is called a Borel measure. Borel sets and the associated Borel hierarchy also play a fundamental role in descriptive set theory.

- [**Probability Space**](https://en.wikipedia.org/wiki/Probability_space): ($\Omega, \mathcal{F}, \mathcal{P}$) is a mathematical construct that models a real-world process (or “experiment”) consisting of states that occur randomly. A probability space is constructed with a specific kind of situation or experiment in mind. One proposes that each time a situation of that kind arises, the set of possible outcomes is the same and the probabilities are also the same. A probability space consists of three parts: 

    1) A sample space, $\Omega$, which is the set of all possible outcomes.
    
    2) A set of events $\mathcal{F}$, where each event is a set containing zero or more outcomes.
    
    3) The assignment of probabilities to the events; that is, a function $\mathcal{P}$ from events to probabilities.

- Given ($\Omega, \mathcal{F}, \mathcal{P}$), we can derive some properties:

    1) $\mathbb{P}($\varnothing$) = 0.
    
    2) $\mathbb{P}$(A) = 1 - $\mathbb{P}$(A$^c$)
    
    3) $\mathbb{P}$(A) ≤ 1
    
    4) $\mathbb{P}$(B$\cap$A$^c$) = $\mathbb{P}$(B) - $\mathbb{P}$(A$\cap$B)
    
    5) $\mathbb{P}$(A$\cup$B) = $\mathbb{P}$(A) + $\mathbb{P}$(B) - $\mathbb{P}$(A$\cap$B)
    
    6) If A $\subseteq$ B, then $\mathbb{P}$(A) ≤ $\mathbb{P}$(B)
    
- [**Finite Space**](https://en.wikipedia.org/wiki/Finite_topological_space): is a topological space for which the underlying point set is finite. That is, it is a topological space for which there are only finitely many points. While topology has mainly been developed for infinite spaces, finite topological spaces are often used to provide examples of interesting phenomena or counterexamples to plausible sounding conjectures.

##### Random Variables

- [**Random Variables**](https://en.wikipedia.org/wiki/Random_variable): is described informally as a variable whose values depend on outcomes of a random phenomenon. The formal mathematical treatment of random variables is a topic in probability theory. In that context, a random variable is understood as a measurable function defined on a probability space whose outcomes are typically real numbers. So, we say that X is a **random variable** if it is *measurable* with respect to $\mathcal{F}$: i.e., for all sets V $\in$ $\mathcal{R}$, the inverse image $X^{-1}(V)\in \mathcal{F}$ i.e. X$^{-1}$(V) is always an event.

- [**Cumulative Distribution Function**](https://en.wikipedia.org/wiki/Cumulative_distribution_function): is the probability that X will take a value less than or equal to x. In the case of a scalar continuous distribution, it gives the area under the probability density function from minus infinity to x. Cumulative distribution functions are also used to specify the distribution of multivariate random variables. So, for F:$\mathbb{R}$ -> [0,1] is a cdf $\iff$ all three hold:

    1) F(x) is nondecreasing in x.
    
    2) $lim_{x->-\infty}$ F(x) = 0, $lim_{x-> \infty}$F(x) = 1.
    
    3) F is continuous from the right: $lim_{|\gamma |}->0$ F($x_0 + |\gamma |)$ = F($x_0$) $\forall$ $x_0$.
    
- We denote by $f_x$(k) the derivative, when it exists, of F at k, i.e., the **density**. From the fundamental theorem of calculus: $\int_{a}^{b} f_X(x) = F_X(b) - F_X(a)$.

- [**Discrete Choice**](https://en.wikipedia.org/wiki/Discrete_choice): describe, explain, and predict choices between two or more discrete alternatives, such as entering or not entering the labor market, or choosing between modes of transport. Such choices contrast with standard consumption models in which the quantity of each good consumed is assumed to be a continuous variable. In the continuous case, calculus methods (e.g. first-order conditions) can be used to determine the optimum amount chosen, and demand can be modeled empirically using regression analysis. On the other hand, discrete choice analysis examines situations in which the potential outcomes are discrete, such that the optimum is not characterized by standard first-order conditions. Thus, instead of examining “how much” as in problems with continuous choice variables, discrete choice analysis examines “which one.” However, discrete choice analysis can also be used to examine the chosen quantity when only a few distinct quantities must be chosen from, such as the number of vehicles a household chooses to own and the number of minutes of telecommunications service a customer decides to purchase.

##### Expectation

- [**Expectation**](https://en.wikipedia.org/wiki/Expected_value): is the long-run average value of repetitions of the same experiment it represents. In other words, the law of large numbers states that the arithmetic mean of the values almost surely converges to the expected value as the number of repetitions approaches infinity. The expected value is also known as the expectation, mathematical expectation, EV, average, mean value, mean, or first moment. More practically, the expected value of a discrete random variable is the probability-weighted average of all possible values. In other words, each possible value the random variable can assume is multiplied by its probability of occurring, and the resulting products are summed to produce the expected value. The same principle applies to an absolutely continuous random variable, except that an integral of the variable with respect to its probability density replaces the sum. The formal definition subsumes both of these and also works for distributions which are neither discrete nor absolutely continuous; the expected value of a random variable is the integral of the random variable with respect to its probability measure. So, for real-valued variables with a density, can define as: $\mathbb{E}[X] = \int x f(x) dx$.

- [**Covariance**](https://en.wikipedia.org/wiki/Covariance): is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables.

- [**Variance/Standard Deviation**](https://en.wikipedia.org/wiki/Variance): is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of (random) numbers are spread out from their average value. Variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling. Variance is an important tool in the sciences, where statistical analysis of data is common. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by $\sigma^{2}$, $s^{2}$ or Var(X).

- [**Moment-generating Function**](https://en.wikipedia.org/wiki/Moment-generating_function): is an alternative specification of its probability distribution. Thus, it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions. There are particularly simple results for the moment-generating functions of distributions defined by the weighted sums of random variables. However, not all random variables have moment-generating functions. As its name implies, the moment generating function can be used to compute a distribution’s moments: the nth moment about 0 is the nth derivative of the moment-generating function, evaluated at 0. So, for X it is defined as $M_X(t) = \mathbb{E}[e^{tX}]$.

- [**Characteristic Function**]: completely defines its probability distribution. If a random variable admits a probability density function, then the characteristic function is the Fourier transform of the probability density function. Thus it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions. There are particularly simple results for the characteristic functions of distributions defined by the weighted sums of random variables. So, for X it is defined as: $\phi_X(t)$ = $\mathbb{E}[e^{itX}]$ or the collection of Fourier moments.

##### Inequalities

- [**Markov's Inequality**](https://en.wikipedia.org/wiki/Markov%27s_inequality): gives an upper bound for the probability that a non-negative function of a random variable is greater than or equal to some positive constant. So, if X is an integrable random variable, then $\mathbb{P}( |X| ≥ \lambda)$ ≤ $\frac{\mathbb{E}[|X|]}{\lambda}$ for any $\lambda$ > 0.

- [**Chebyshev's inequality**](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality): guarantees that, for a wide class of probability distributions, no more than a certain fraction of values can be more than a certain distance from the mean. Specifically, no more than 1/$k^2$ of the distribution's values can be more than k standard deviations away from the mean (or equivalently, at least 1 − 1/$k^2$ of the distribution's values are within k standard deviations of the mean). The rule is often called Chebyshev's theorem, about the range of standard deviations around the mean, in statistics. The inequality has great utility because it can be applied to any probability distribution in which the mean and variance are defined. For example, it can be used to prove the weak law of large numbers. If X has mean $\mu$, then for $\lambda$ > 0, $\mathbb{P}(|X - \mu | ≥ \lambda) ≤ \frac{Var(X)}{\lambda^2}$. As a corollary, we have the Weak Law of Large Numbers: If $X_1,...,X_n$ are iid with mean $\mu$ and finite variance $\sigma^2$, then for all $\varepsilon$ > 0, $\mathbb{P}(|\frac{1}{n}\sum_{i=1}^n X_i - \mu| ≥ \varepsilon)$ ≤ $\frac{1}{\varepsilon^2}$ Var($\frac{1}{n}\sum X_n$) -> 0; i.e. the sample average of the $X_i$'s converges to $\mu$ with probability one.

- [**Jensen's Inequality**](https://en.wikipedia.org/wiki/Jensen%27s_inequality): relates the value of a convex function of an integral to the integral of the convex function. In its simplest form the inequality states that the convex transformation of a mean is less than or equal to the mean applied after convex transformation; it is a simple corollary that the opposite is true of concave transformations. So, for a continuous concave u: $\mathbb{R} -> \mathbb{R}$, u($\mathbb{E}$[X]) ≥ $\mathbb{E}$[u(X)].

- [**Stochastic Dominance**](https://en.wikipedia.org/wiki/Stochastic_dominance): is a partial order between random variables. It is a form of stochastic ordering. The concept arises in decision theory and decision analysis in situations where one gamble (a probability distribution over possible outcomes, also known as prospects) can be ranked as superior to another gamble for a broad class of decision-makers. It is based on shared preferences regarding sets of possible outcomes and their associated probabilities. Only limited knowledge of preferences is required for determining dominance. Risk aversion is a factor only in second order stochastic dominance. 

- [**First Order Stochastic Dominance**](https://en.wikipedia.org/wiki/Stochastic_dominance#First-order): Random variable A has first-order stochastic dominance over random variable B if for any outcome x, A gives at least as high a probability of receiving at least x as does B, and for some x, A gives a higher probability of receiving at least x. In notation form: P[A ≥ x] ≥ P[B ≥ x] for all x, and for some x, P[A ≥ x] > P[B ≥ x].

- In terms of the cumulative distribution functions of the two random variables, A dominating B means that $F_A(x) ≤ F_B(x)$ for all x, with strict inequality at some x.

- [**Second Order Stochastic Dominance**](https://en.wikipedia.org/wiki/Stochastic_dominance#Second-order): Roughly speaking, for two gambles A and B, gamble A has second-order stochastic dominance over gamble B if the former is more predictable (i.e. involves less risk) and has at least as high a mean. All risk-averse expected-utility maximizers (that is, those with increasing and concave utility functions) prefer a second-order stochastically dominant gamble to a dominated one. Second-order dominance describes the shared preferences of a smaller class of decision-makers (those for whom more is better and who are averse to risk, rather than all those for whom more is better) than does first-order dominance.  


#### 11th Lesson {.tabset}

##### Probability Theory II

- [**Jointly Distributed Random Variables**](https://en.wikipedia.org/wiki/Joint_probability_distribution): the joint probability distribution for X,Y is a probability distribution that gives the probability that each of X,Y falls in any particular range or discrete set of values specified for that variable. In the case of only two random variables, this is called a bivariate distribution, but the concept generalizes to any number of random variables, giving a multivariate distribution. So, a nice way to characterize these is via a **joint** probability density of (X,Y), a map $f_{X,Y} : \mathbb{R}^2$ -> $\mathbb{R}_+$, s.t. $\mathbb{P}$({X$\in$A} $\cap${Y $\in$ B}) = $\int_{A}$ $\int_{B}$ $f_{X,Y}$(x,y)dydx. There are many types of JDRV, such as:

    1) [**Multinomial**](https://en.wikipedia.org/wiki/Multinomial_distribution): is a generalization of the binomial distribution. For example, it models the probability of counts of each side for rolling a k-sided die n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.
    
    2) [**Bivariate Normal**](https://en.wikipedia.org/wiki/Joint_probability_distribution#Bivariate_normal_distribution): When there are specifically two random variables, this is the bivariate normal distribution, with the possible values of the two variables plotted in two of the dimensions and the value of the density function for any pair of such values plotted in the third dimension. The probability that the two variables together fall in any region of their two dimensions is given by the volume under the density function above that region.

- [**Independent Variables**](https://en.wikipedia.org/wiki/Joint_probability_distribution#Bivariate_normal_distribution): In general two random variables X and Y are independent if the joint cumulative distribution function satisfies: $F_{X,Y}$(x,y) = $F_X(x)\circ F_Y(y)$.

- [**Independent and Identically Distributed Random Variables**](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables): each random variable has the same probability distribution as the others and all are mutually independent. So, for the variables to be called *iid*, they must:

    1) Each $X_i$ be identically distributed, i.e., have the same CDF.
    
    2) The set of $X_i$'s be independent.
    
##### Conditional Probability

- [**Conditional Probability**](https://en.wikipedia.org/wiki/Conditional_probability): is a measure of the probability of an event occurring given that another event has occurred. If the event of interest is A and the event B is known or assumed to have occurred, "the conditional probability of A given B", or "the probability of A under the condition B", is usually written as P(A | B), or sometimes PB(A) or P(A / B). So, define the following new probability measure $\mathbb{P}$(B|A) $\equiv$ $\frac{\mathbb{P}(B\cap A)}{\mathbb{P}(A)}$. So, we are replacing $\Omega$ with the subset of $\Omega$ where E holds, and replacing the measure $\mathbb{P}$ by the new measure defined above.

- [**Conditional Probability Distribution**](https://en.wikipedia.org/wiki/Conditional_probability_distribution): given two jointly distributed random variables X and Y, the conditional probability distribution of Y given X is the probability distribution of Y when X is known to be a particular value; in some cases the conditional probabilities may be expressed as functions containing the unspecified value x of X as a parameter. When both X and Y are categorical variables, a conditional probability table is typically used to represent the conditional probability. The conditional distribution contrasts with the marginal distribution of a random variable, which is its distribution without reference to the value of the other variable.

- Properties of Conditional Probability: If X,Y are integrable on $(\Omega, \mathcal{F},\mathbb{P})$, and $\mathcal{G} \subseteq \mathcal{F}$ is a field of events, then

    1) $\mathbb{E}[\mathbb{E}[X|\mathcal{G}]]$= $\mathbb{E}$[X]
    
    2) If X is $\mathcal{G}$-measurable, then $\mathbb{E}[X|\mathcal{G}]$ = X a.s.
    
    3) If X is independent of $\mathcal{G}$, then $\mathbb{E}[X|\mathcal{G}]$ = $\mathbb{E}[X]$ a.s. 
    
    4) If X ≥ 0 a.s., then $\mathbb{E}[X|$\mathcal{G}$] ≥ 0 a.s.
    
    5) For any $\alpha$, ß $\in$ $\mathbb{R}$, we have $\mathbb{E}$[$\alpha$X + ßY| $\mathcal{G}$] = $\alpha \mathbb{E}$[Y|$\mathcal{G}$] + ß$\mathbb{E}$[Y|$\mathcal{G}$] a.s.
    
    6) If another event field $\mathcal{H} \subset \mathcal{G}$, then $\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}]$ = $\mathbb{E}[X|\mathcal{H}$] a.s.
    
    7) If Y is bounded and $\mathcal{G}$-measurable, $\mathbb{E}[XY|\mathcal{G}]$ = Y$\mathbb{E}$[X|$\mathcal{G}$]
    
- [**Law of Iterated Expectation**](https://brilliant.org/wiki/law-of-iterated-expectation/): states that the expected value of a random variable is equal to the sum of the expected values of that random variable conditioned on a second random variable. Intuitively speaking, the law states that the expected outcome of an event can be calculated using casework on the possible outcomes of an event it depends on.

- [**Conditional Expectation**](https://en.wikipedia.org/wiki/Conditional_expectation): of a random variable is its expected value – the value it would take “on average” over an arbitrarily large number of occurrences – given that a certain set of "conditions" is known to occur. If the random variable can take on only a finite number of values, the “conditions” are that the variable can only take on a subset of those values. More formally, in the case when the random variable is defined over a discrete probability space, the "conditions" are a partition of this probability space.

- [**Bayes' Theorem**](https://en.wikipedia.org/wiki/Bayes%27_theorem): describes the probability of an event, based on prior knowledge of conditions that might be related to the event. One of the many applications of Bayes’ theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in Bayes’ theorem may have different probability interpretations. With the Bayesian probability interpretation the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for availability of related evidence. It is stated as: P(A|B) = $\frac{P(B|A)P(A)}{P(B)}$, where A and B are events and P(B) ≠ 0.

    1) P(A|B) is a conditional probability: the likelihood of event A occurring given that B is true.
    
    2) P(B|A) is also a conditional probability: the likelihood of event B occurring given that A is true.
    
    3) P(A) and P(B) are the probabilities of observing A and B independently of each other: this is known as the marginal probability.
    
##### Random Sequences

- [**Modes of Convergence**](https://en.wikipedia.org/wiki/Modes_of_convergence): there are many senses in which a sequence or a series is said to be convergent. 

    1) A sequence (X$_n$)$_{n≥0}$ **converges in distribution** to X if $\mathbb{P}$($X_n$ ≤ k) -> $\mathbb{P}$(X ≤ k) at every k at which $\mathbb{P}$(X ≤ k) is continuous. Write $X_n \implies$ X. ($\equiv$ "converges weakly"; "converges in law"). 
    
    2) A sequence (X$_n$)$_{n≥0}$ **converges in probability** to X if $\mathbb{P}$(d($X_n,X$) > $\varepsilon$) -> 0 $\forall \varepsilon > 0$ ($X_n ->^p$ X).
    
    3) A sequence (X$_n$)$_{n≥0}$ **converges almost surely** if d($X_n,X$) -> 0 with probability 1. ($X_n$ ->$^{a.s.}$ X)
    
- [**Slutsky's Theorem**](https://en.wikipedia.org/wiki/Slutsky%27s_theorem): if $X_n$, X and $Y_n$ are random variables such that $X_n$ $\implies$ X and $Y_n$ $\implies$ c for some constant c, then:

    1) $X_n$ + $Y_n$ $\implies$ X + c
    
    2) $Y_nX_n$ $\implies$ cX
    
    3) $Y_n^{-1}X_n$ $\implies$ c$^{-1}$X if c ≠ 0
    
- [**Central Limit Theorem**](https://en.wikipedia.org/wiki/Central_limit_theorem): establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a "bell curve") even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. 

##### Stochastic Processes

- [**Markov Process**](https://en.wikipedia.org/wiki/Markov_chain): is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Roughly speaking, a process satisfies the Markov property if one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process's full history, hence independently from such history, that is, conditional on the present state of the system, its future and past states are independent. So, a finite-state **Markov process** ($X_t$)$_{t≥0}$ is an R-valued stochastic process ($X_t$)$_{t≥0}$ initialized at a (possibly random) $X_0$ that satisfies $\mathbb{P}$($X_{t+1}$ = i|$X_t,X_{t-1},...,X_0$}) = $\mathbb{P}(X_{t+1} = i|X_t)$ for any i $\in$ R and each t ≥ 0.

- [**Filtration**](https://en.wikipedia.org/wiki/Filtration_(probability_theory)): filtrations are used to model the information that is available at a given point and therefore play an important role in the formalization of random processes. Let ($\Omega,\mathcal{A},\mathcal{P}$) be a probability space and let I be an index set with a total order ≤ (often $\mathbb{N},\mathbb{R}^+$, or a subset of $\mathbb{R}^+$). For every i $\in$ I, let $\mathcal{F}_i$ be a Sub $\sigma$-algebra of $\mathcal{A}$. Then $\mathbb{F}$: = ($\mathcal{F}_i$)$_{i \in I}$ is called a filtration if $\mathcal{F}_k \subseteq \mathcal{F}_\ell \subseteq \mathcal{A} for all k ≤ \ell$. So filtrations are families of $\sigma$-algebras that are ordered non decreasingly. If $\mathbb{F}$ is a filtration, then ($\Omega,\mathcal{A},\mathbb{F},\mathcal{P}$) is called a **filtered probability space**. 

- [**Martingale**](https://en.wikipedia.org/wiki/Martingale_(probability_theory)): is a sequence of random variables (i.e., a stochastic process) for which, at a particular time, the conditional expectation of the next value in the sequence, given all prior values, is equal to the present value. So, an adapted integrable stochastic process is **martingale** if $\mathbb{E}[X_t|\mathcal{F}_s]$ = $X_s$; almost surely for all s ≤ t.

- [**Brownian Motion**](https://en.wikipedia.org/wiki/Brownian_motion): is the random motion of particles suspended in a fluid (a liquid or a gas) resulting from their collision with the fast-moving molecules in the fluid. The Brownian motion can be modeled by a random walk. Random walks in porous media or fractals are anomalous. In the general case, Brownian motion is a non-Markov random process and described by stochastic integral equations. So, a real-valued stochastic process ($X_t$)$_{t≥0}$ is standard Brownian motion if:

    1) $X_0$ = 0 a.s.
    
    2) $X_t$ - $X_s$ ~ $\mathcal{N}$(0, t - s) for all s < t.
    
    3) X has independent increments: for any 0 ≤ $t_0$ < $t_1$ < ... < $t_n$, the random variables $W_{t_0}$ - $W_0$, $W_{t_1}$ - $W_0$, etc, are independent.
    
    4) X is continuous, in the sense that $\mathbb{P}$($\omega \in \Omega$ : t $\mapsto$ $X_t$($\omega$) is continuous) = 1.
 